{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#contact","title":"Contact","text":"<p>For all enquiries please email rtis.support@otago.ac.nz</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#access","title":"Access","text":"<p>Getting an account</p><p>Information about getting access</p> <p>OnDemand (Web)</p><p>Logging into the Research Cluster through the OnDemand web portal</p> <p>SSH</p><p>Logging into the Research Cluster using SSH</p>"},{"location":"#storage","title":"Storage","text":"<p>Storage locations</p><p>Where to put your data</p> <p>Moving data on and off the Research Cluster</p><p>How to transfer data onto and off of the cluster</p> <p>File Permissions</p><p>How to manage file permissions on the cluster</p>"},{"location":"#ondemand","title":"OnDemand","text":"<p>OnDemand (Web)</p><p>Logging into the Research Cluster through the OnDemand web portal</p> <p>Available Web Apps</p><p>Pre-installed apps available within Open OnDemand</p> <p>Using HPC Desktop</p><p>How to use the virtual desktop environment</p> <p>Managing files with OnDemand</p><p>How to manage files through the Open OnDemand web interface</p>"},{"location":"#using-the-cluster","title":"Using the Cluster","text":"<p>Available Web Apps</p><p>Pre-installed apps available within Open OnDemand</p> <p>Using HPC Desktop</p><p>How to use the virtual desktop environment</p> <p>Installing and managing software</p><p>How to install and configure software for yourself</p> <p>Interactive jobs</p><p>Options for interactive jobs</p> <p>Using the scheduler</p><p>Submitting and managing jobs through the SLURM scheduler</p>"},{"location":"data_storage/","title":"Data Management","text":"High Capacity Storage RTIS Storage Mircosoft Teams or SharePoint OneDrive Ideal Use Research data you are working on Research data you are actively working on Shared research administration files but not research data Personal work related files but not research data Suitable for sensitive research data Located in Aotearoa New Zealand"},{"location":"training/","title":"Training","text":""},{"location":"training/#workshops","title":"Workshops","text":""},{"location":"training/#carpentries-workshops","title":"Carpentries Workshops","text":"<p>The Carpentries (https://carpentries.org) is a global organistation that teaches foundational  coding and data science skils to researchers world-wide. University of Otago is a member organistation  of the Carpentries and uses the Software and Data Carpentry curriculums for workshops. Each year we also train new instructors through instructor training workshops.</p>"},{"location":"training/#software-carpentry","title":"Software Carpentry","text":"<p>A Software Carpentry (https://software-carpentry.org) workshop is a hands-on training that covers the core skills needed to be productive in a small research team.  Short tutorials alternate with practical exercises, and all instruction is done via live coding. </p> <p>A Software Carpentry workshop is taught by at least one trained and badged instructor. Over the course of the workshop,  instructors teach our three core topics: the Unix shell, version control with Git, and a programming language (Python or R).</p>"},{"location":"training/#data-carpentry","title":"Data Carpentry","text":"<p>Data Carpentry's (https://datacarpentry.org) focus is on the introductory computational skills needed for data management and analysis  in all domains of research. Our lessons are domain-specific, and build on the existing knowledge of learners  to enable them to quickly apply skills learned to their own research. Our initial target audience is learners  who have little to no prior computational experience. We create a friendly environment for learning to empower  researchers and enable data driven discovery.</p>"},{"location":"child/ood_apps/blender/","title":"Blender","text":""},{"location":"child/ood_apps/blender/#blender","title":"Blender","text":"<p>Blender is a free and open source 3D creation suite.</p> <p>The Blender GUI can be accessed via the Open OnDemand Applications. This requires 3D acceleration for visualisation and rendering so needs to be run on a GPU partition.</p>"},{"location":"child/ood_apps/chimerax/","title":"Chimerax","text":""},{"location":"child/ood_apps/chimerax/#chimerax","title":"ChimeraX","text":"<p>The ChimeraX GUI can be accessed via the Open OnDemand Applications. This requires 3D acceleration for visualisation so needs to be run on a GPU VirtualGL partition.</p>"},{"location":"child/ood_apps/clc_wb/","title":"Clc wb","text":""},{"location":"child/ood_apps/clc_wb/#clc-genomics-workbench","title":"CLC genomics Workbench","text":"<p>QIAGEN CLC genomics Workbench is a user-friendly sequence  analysis platform for genomics, epigenomics and metagenomics.</p> <p>Warning</p> <p>CLC Genomics Workbench is licensed software. The Research Cluster currently has a small number of floating licenses available for trial purposes, facilitated by <code>Dr Sunali Mehta &lt;mailto:sunali.mehta@otago.ac.nz&gt;_</code>, Pathology dept. Please ensure you are authorised to consume a license.</p> <p>The CLC genomics Workbench GUI can be accessed via the Open OnDemand Applications.</p> <p>Optionally tick the '3D hardware-accelerated rendering option and select a GPU partition if using the 3D viewers.</p>"},{"location":"child/ood_apps/ecoassist/","title":"Ecoassist","text":""},{"location":"child/ood_apps/ecoassist/#ecoassist","title":"EcoAssist","text":"<p>The EcoAssist GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"child/ood_apps/epi2me/","title":"Epi2me","text":""},{"location":"child/ood_apps/epi2me/#epi2me-desktop","title":"EPI2ME Desktop","text":"<p>Oxford Nanopore's EPI2ME Desktop is a data analysis platform providing a user-friendly graphical interface to running various bioinformatics pipelines.</p> <p>Note</p> <p>Apart from the list of ONT workflows prepopulated in the <code>Available Workflows</code> tab, EPI2ME Desktop allows for the importation of other generic Nextflow workflows,  such as the 100+ curated pipelines of []nf-core](https://nf-co.re/pipelines).</p> <p>Under Workflows, click <code>Import workflow</code> and copy-paste the workflow's git repository URL. (i.e. <code>https://github.com/nf-core/&lt;wf&gt;</code>)</p> <p>The EPI2ME GUI can be accessed via the Open OnDemand Applications.</p> <ul> <li> <p>Individual pipeline tasks will be sent to the Slurm cluster scheduler and scheduled as separate jobs with their own allocated resources (as per the workflow defaults, or as specified in the <code>Nextflow configuration</code> tab).</p> </li> <li> <p>For pipelines requiring GPU compute, there is no need to run the EPI2ME OOD app on a GPU partition -In fact, the OOD app form doesn't give you that option-; Individual tasks requiring GPUs will be automatically scheduled on GPU-capable cluster nodes.</p> </li> <li> <p>In the EPI2ME Launch Wizard, there is also no need to change the Profile setting (in <code>Nextflow configuration</code>); This instance has been patched to default to the <code>singularity</code> profile (which uses apptainer).</p> </li> </ul> <p>To accomplish this, running the OnDemand app for the first time will automatically create the global nextflow config <code>$HOME/.nextflow/config</code> to support running the tasks via the cluster scheduler and make use of the GPU partition when appropriate. However if this file already exists, you may need to manually add this functionality:</p> <p>Terminal</p> <pre><code>  process {\n    executor = 'slurm'\n    time = 6.h\n    withLabel: 'gpu' {\n      queue = 'aoraki_gpu'\n    }\n  }\n</code></pre> <p>Warning</p> <p>EPI2ME Desktop isn't designed with HPC cluster use in mind. As such we have had to devise a number of workarounds to make this work and integrate with the cluster scheduler.  While initial testing has been promising, the added complexity of running EPI2ME Desktop this way, as well as the application's general lack of (Nextflow) configurability may introduce  hard-to-troubleshoot issues.</p> <p>Consider running the workflows directly from the commandline using Nextflow instead.</p>"},{"location":"child/ood_apps/fiji/","title":"Fiji","text":""},{"location":"child/ood_apps/fiji/#fiji","title":"Fiji","text":"<p>Fiji is an image processing package \u2014 a \"batteries-included\" distribution of ImageJ, bundling many plugins which facilitate scientific image analysis. .</p> <p>The Fiji GUI is available as an Open OnDemand app.</p>"},{"location":"child/ood_apps/flexpde/","title":"Flexpde","text":""},{"location":"child/ood_apps/flexpde/#flexpde","title":"FlexPDE","text":"<p>The FlexPDE GUI can be accessed via the Open OnDemand Applications.</p> <p>The FlexPDE Lite (evaluation) configuration is free to use but wil be restricted in the number of simultaneous equations and mesh cells.</p> <p>FlexPDE Professional can be Internet-activated with an appropriate serial number. Do note that the license activation is machine-based so will need to be deactivated and reactivated on subsequent runs unless the same node is selected.</p>"},{"location":"child/ood_apps/grass/","title":"Grass","text":""},{"location":"child/ood_apps/grass/#grass-gis","title":"GRASS GIS","text":"<p>The GRASS GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"child/ood_apps/ilastik/","title":"Ilastik","text":""},{"location":"child/ood_apps/ilastik/#ilastik","title":"ilastik","text":"<p>ilastik is a user-friendly open-source tool for interactive image classification, segmentation and analysis leveraging machine learning algorithms.</p>"},{"location":"child/ood_apps/ilastik/#via-ondemand","title":"via OnDemand","text":"<p>The ilastik GUI is available as an Open OnDemand app.</p>"},{"location":"child/ood_apps/jupyter/","title":"Jupyter","text":""},{"location":"child/ood_apps/jupyter/#jupyter-lab","title":"Jupyter Lab","text":"<p>Jupyter Lab can be accessed via the Open OnDemand applications.</p> <p>This app features a number of domain-focused or application-specific variants. These containerised software stacks are immutable which promotes consistency and reproducibility.</p> <p>The different environments are based on the Jupyter Docker Stacks images, maintained by the Jupyter team.</p> <p>Contact us if you require a customised environment.</p>"},{"location":"child/ood_apps/kilosort/","title":"Kilosort","text":""},{"location":"child/ood_apps/kilosort/#kilosort4","title":"Kilosort4","text":"<p>Kilosort4 is a tool for clustering spikes from multi-channel electrophysiological recordings.</p> <p>https://kilosort.readthedocs.io</p>"},{"location":"child/ood_apps/kilosort/#via-ondemand","title":"via OnDemand","text":"<p>The Kilosort4 GUI is available as an Open OnDemand app.</p>"},{"location":"child/ood_apps/melts/","title":"Melts","text":""},{"location":"child/ood_apps/melts/#melts","title":"MELTS","text":"<p>Legacy software for thermodynamic modeling of phase equilibria in magmatic systems experimental</p> <p>\ud83d\udd17 https://melts.ofm-research.org</p> <p>The MELTS GUI can be accessed via the Open OnDemand Applications.</p> <p>The launcher allows for the selection of the different versions/models;</p> <ul> <li>rhyolite-MELTS v. 1.0.2 (original version, with corrections) - old H2O model, no mixed fluids.</li> <li>rhyolite-MELTS v, 1.1.0 (mixed fluid version that perserves the ternary minimum) - old H2O model.</li> <li>rhyolite-MELTS v. 1.2.0 (mixed fluid version optimal for mafic and alkalic melts) - new H2O model.</li> <li>pMELTS v. 5.6.1 (original version, with corrections) - - old H2O model, no mixed fluids.</li> </ul>"},{"location":"child/ood_apps/netlogo/","title":"Netlogo","text":""},{"location":"child/ood_apps/netlogo/#netlogo","title":"NetLogo","text":"<p>The NetLogo GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"child/ood_apps/qgis/","title":"Qgis","text":""},{"location":"child/ood_apps/qgis/#qgis","title":"QGIS","text":"<p>The QGIS GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"child/ood_apps/relion/","title":"Relion","text":""},{"location":"child/ood_apps/relion/#relion","title":"RELION","text":"<p>The RELION GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"child/ood_apps/rstudio/","title":"Rstudio","text":""},{"location":"child/ood_apps/rstudio/#rstudio-server","title":"RStudio Server","text":"<p>The RStudio Server web UI can be accessed via the Open OnDemand Applications.</p>"},{"location":"child/ood_apps/saga/","title":"Saga","text":""},{"location":"child/ood_apps/saga/#saga-gis","title":"SAGA GIS","text":"<p>The SAGA GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"child/ood_apps/satscan/","title":"Satscan","text":""},{"location":"child/ood_apps/satscan/#satscan","title":"SaTScan","text":"<p>The SaTScan GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"child/ood_apps/specify/","title":"Specify","text":""},{"location":"child/ood_apps/specify/#specify","title":"Specify","text":"<p>The Spcify 6 GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"child/ood_apps/stata/","title":"Stata","text":""},{"location":"child/ood_apps/stata/#stata","title":"Stata","text":"<p>The Stata GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"child/ood_apps/ugene/","title":"Ugene","text":""},{"location":"child/ood_apps/ugene/#ugene","title":"UGENE","text":"<p>https://ugene.net</p> <p>UGENE is a GUI for DNA and protein sequence visualization, alignment, assembly and annotation. It integrates dozens of well-known biological tools,  algorithms, and original tools in the context of genomics, evolutionary biology, virology, and other branches of life science.</p>"},{"location":"child/ood_apps/ugene/#gui","title":"GUI","text":"<p>The UGENE GUI can be accessed via the Open OnDemand Applications. https://ondemand.otago.ac.nz/pun/sys/dashboard/batch_connect/sys/ood_ugene_apptainer</p> <ul> <li>hardware-accelerated 3D visualisation can be enabled to improve 3D visualisation (e.g. the 3D viewer)</li> <li>OpenCL can be enabled to improve performance of a few select algorithms (Smith-Waterman, UGENE Genome Aligner)</li> </ul>"},{"location":"child/ood_apps/vscodium/","title":"Vscodium","text":""},{"location":"child/ood_apps/vscodium/#vscodium","title":"VSCodium","text":"<p>VSCodium is a community-driven, freely-licensed distribution of Microsoft's editor VSCode.</p> <p>The VSCodium GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"general/overview/","title":"Research HPC Cluster (Aoraki)","text":"<p>Shared computing resources available to Otago researchers include high performance computing, fast storage, GPUs and virtual servers.</p>"},{"location":"general/overview/#otago-resources","title":"Otago Resources","text":"<p>The RTIS Research cluster provides researchers with access to shared resources, such as CPUs, GPUs, and high-speed storage.  Also available are specialised software and libraries optimised for scientific and datascience computing. </p> <p>If you need special software or configurations please ask the RTIS team at rtis.solutions@otago.ac.nz</p>"},{"location":"general/overview/#cluster-overview","title":"Cluster Overview","text":"<p>We offer a variety of SLURM partitions based on different resource needs. The default partition provides balanced compute and memory capabilities. Additional partitions include those optimized for GPU usage and those with expanded memory capacity. On every cluser node there are 2 cores reseved for the OS (weka storage), reducing the available compute cores by 2.  </p> PARTITION TIMELIMIT [DD-HH:MM:SS] NODES NODELIST MAX_CORES PER NODE MAX_MEMORY [MB] PER NODE GRES PER NODE aoraki* 7-00:00:00 9 aoraki[01-09] 126 1030000 nan aoraki_bigcpu 14-00:00:00 5 aoraki[15,20-23] 254 1500000 nan aoraki_bigmem 14-00:00:00 5 aoraki[14,17,24-26] 126 2000000 nan aoraki_long 30-00:00:00 7 aoraki[20-26] 126 or 254 2000000 or 1500000 nan aoraki_short 1-00:00:00 3 aoraki[11-12,16] 80 600000 nan aoraki_small 7-00:00:00 4 aoraki[18-19,27-28] 40 800000 nan aoraki_gpu 7-00:00:00 2 aoraki[11-12] 126 1030000 gpu:A100:2 aoraki_gpu_H100 7-00:00:00 1 aoraki16 110 1030000 gpu:H100:2 aoraki_gpu_L40 7-00:00:00 2 aoraki[18-19] 62 1030000 gpu:L40:3 aoraki_gpu_A100_80GB 7-00:00:00 2 aoraki[11-12] 126 1030000 gpu:A100:2 aoraki_gpu_A100_40GB 7-00:00:00 2 aoraki[27-28] 62 1030000 gpu:A100:2 <ul> <li>Partition: Name of the partition, with an asterisk (*) denoting the default partition. Aoraki_small and aoraki_short are specialized partitions that utilize typically idle CPU cores on GPU nodes, designed to handle small or short-duration jobs efficiently. </li> <li>Time Limit: Maximum time a job can run in that partition. The time limit for running jobs can be extended upon request. In such cases, the extended time limit may exceed the partition's standard wall time.</li> <li>Nodes: Number of nodes available in the partition.</li> <li>Nodelist: The specific nodes allocated to that partition.</li> <li>Max Cores per Node: Maximum number of CPU cores available on a node.</li> <li>Max Memory (MB) per Node: The maximum amount of memory (in MB) available on each node in partition.</li> <li>GRES per Node: Generic Resources (e.g., GPUs) available in the partition.</li> </ul> Number Node type CPU RAM Extra 1 aoraki-login 2x 64 cores AMD EPYC 7763 1TB DDR4 3200 MT/s nan 9 aoraki[01-9] 2x 64 cores AMD EPYC 7763 1TB DDR4 3200 MT/s nan 2 aoraki[11,12] 2x 64 cores AMD EPYC 7763 1TB DDR4 3200 MT/s 2x A100 80GB PCIe GPU per node cuda12.5 2 aoraki[27,28] 2x 32 cores AMD EPYC 7543 1TB DDR4 3200 MT/s 2x A100 40GB PCIe GPU per node cuda12.5 5 aoraki[14,17,24-26] 2x 64 cores AMD EPYC 7763 2TB DDR4 2933 MT/s nan 5 aoraki[15,20-23] 2x 128 cores AMD EPYC 9754 1.5TB DDR5 4800 MT/s nan 1 aoraki16 2x 56 cores Intel Xeon 8480+ 1TB DDR5 4800 MT/s 4x H100 80GB HBM3 GPU per node cuda12.4 2 aoraki[18,19] 2x 32 cores AMD EPYC 7543 1TB DDR4 3200 MT/s 3x L40 48GB PCIe GPU per node cuda12.5 2 standalone 32 cores AMD Ryzen Threadripper PRO 3975WX 128 GB DDR4 3200 MT/s 1x RTX 3090 24GB cuda12.5 3 standalone 16 cores AMD Ryzen 9 5950X 64 GB DDR4 3200 MT/s 1x RTX 3090 24GB cuda12.5 2 standalone 2x 6 cores Intel Xeon CPU E5-2620 v3 256GB DDR4 3200 MT/s 2x RTX A6000 48GB GPU per node"},{"location":"general/support/","title":"Support","text":""},{"location":"general/support/#support","title":"Support","text":""},{"location":"general/support/#support-channels","title":"Support channels","text":"<p>Email the RTIS team at rtis.support@otago.ac.nz  or find support on the Research Computing Teams Channel</p>"},{"location":"general/support/#technical-support","title":"Technical support","text":"<p>While our team is dedicated to helping you resolve technical issues, most of us are not scientific or domain experts in your field of research. To ensure that we can assist you effectively, please follow the guidelines below when requesting technical support;</p> <p>Before requesting support, please check the documentation. Many common issues are addressed in these resources.</p> <p>To request technical support, please provide the following information:</p> <ul> <li>Clear error messages: Copy and paste the exact error messages you encounter. This will help us identify the issue quickly.<ul> <li>Screenshots may be useful for visual errors in GUI applications, but for text-based output, copy-pasted text or log files are highly preferable. </li> </ul> </li> <li>Command lines or code snippets: Provide the exact commands or code snippets that led to the error. This will enable us to reproduce the issue.</li> <li>Example test dataset: If possible, provide a minimal, self-contained example dataset that demonstrates the issue. This will allow us to reproduce the problem and test potential solutions.</li> <li>System information: Include information about the system/resources you are using, such as:<ul> <li>Node name/type (e.g., login node, compute node, partition, etc.)</li> <li>Software versions (e.g. application, library, compiler, etc.)</li> <li>Steps to reproduce: Describe the steps you took leading up to the error. This will help us understand the context and reproduce the issue.</li> </ul> </li> </ul>"},{"location":"general/support/#best-practices-for-providing-an-example-test-dataset","title":"Best Practices for Providing an Example Test Dataset","text":"<p>When providing an example test dataset, please follow these guidelines:</p> <ul> <li>Minimize the dataset: Use the smallest possible dataset that still reproduces the issue. This will make it easier for us (and yourself) to diagnose the problem.</li> <li>Make it self-contained: Ensure that the dataset is complete and self-contained, including all necessary input files and dependencies.</li> <li>Use a standard format: Use a standard format for your dataset, such as a text file or a widely-supported binary format.</li> <li>Avoid sensitive data: Remove any sensitive or confidential data from the dataset.</li> </ul> <p>By following these guidelines, you will help us provide you with efficient and effective technical support.  If you have any questions or concerns, please don't hesitate to reach out to us.</p>"},{"location":"general/announcements/2024-11-09-maintenance/","title":"November 2024 Maintenance","text":""},{"location":"general/announcements/2024-11-09-maintenance/#aoraki-research-cluster-november-2024-maintenance-complete","title":"Aoraki Research Cluster November 2024 Maintenance Complete","text":"<p>The scheduled maintenance for the Aoraki Research Cluster was successfully completed on November 10, 2024. This upgrade includes several significant improvements aimed at enhancing performance, reliability, and resource management for all users.</p>"},{"location":"general/announcements/2024-11-09-maintenance/#key-enhancements","title":"Key Enhancements:","text":""},{"location":"general/announcements/2024-11-09-maintenance/#fairshare-scheduling-with-slurm-accounting","title":"Fairshare Scheduling with SLURM Accounting","text":"<p>SLURM Accounting is now enabled to monitor resource usage effectively, supporting fairshare scheduling for a balanced distribution of workloads.</p>"},{"location":"general/announcements/2024-11-09-maintenance/#enhanced-slurm-clustered-database-and-secondary-controller","title":"Enhanced SLURM Clustered Database and Secondary Controller","text":"<p>A new SLURM clustered database and secondary controller have been implemented to improve resource management and redundancy. This addition strengthens failover capabilities, ensuring uninterrupted SLURM operations in the event of a single-point failure.</p>"},{"location":"general/announcements/2024-11-09-maintenance/#gpu-visibility-with-cgroups-device-constraints","title":"GPU Visibility with Cgroups Device Constraints","text":"<p>Cgroups device constraints have been added to GPU nodes, allowing users to access only the GPUs they have requested. This change minimizes conflicts and promotes fair GPU sharing across workloads.</p>"},{"location":"general/announcements/2024-11-09-maintenance/#auks-slurm-plugin-for-kerberos-authentication","title":"AUKS SLURM Plugin for Kerberos Authentication","text":"<p>The AUKS SLURM plugin has been integrated, enabling jobs to access Kerberos-protected network resources for up to 7 days without user intervention, simplifying authentication needs during job execution.</p>"},{"location":"general/announcements/2024-11-09-maintenance/#9-10-november-2024-aoraki-research-cluster-maintenance-updates","title":"9-10 November 2024 - Aoraki Research Cluster Maintenance Updates","text":"<p>As part of the scheduled maintenance for the Aoraki Research Cluster over the weekend of 9-10 November 2024, we will be implementing several updates to enhance system performance, resource allocation, and overall reliability. Below are the key changes that will be applied during this outage:</p> <ol> <li>Implementation of New SLURM Clustered Database and Secondary Controller<ul> <li>To improve resource management and increase redundancy, we will be introducing a new SLURM clustered database along with a secondary controller. This upgrade is critical to ensure better failover capabilities and maintain SLURM operations in the event of any single-point failure.</li> </ul> </li> <li>Enabling SLURM Accounting for Fairshare Scheduling<ul> <li>SLURM Accounting will be enabled to track resource usage and ensure that fairshare scheduling is properly implemented. This will help balance workload distribution across users, giving fair access to cluster resources based on usage history.</li> </ul> </li> <li>CPU Reservation for Weka (Recommended for 100G)<ul> <li>Each node will now have 2 CPUs reserved specifically for Weka filesystem network services. This is in line with the recommended configuration for 100G networking and will help to optimize I/O performance and system responsiveness, particularly for jobs involving large data.</li> </ul> </li> <li>Cgroup Device Constraints for GPU Visibility<ul> <li>Cgroup device constraints will be added to all GPU nodes. This change is essential for improving GPU allocation and visibility, ensuring that users can only see and use the GPUs they have specifically requested. Currently, users can see or attempt to use all GPUs on a node, even if those GPUs are allocated to other jobs. This update will prevent such conflicts and ensure fair resource sharing.</li> </ul> </li> <li>Implementing GPU Sharding on L40 Nodes<ul> <li>To further optimize GPU resource management, we will be introducing GPU sharding on the L40 nodes. This will allow better partitioning of GPU resources, giving users more flexibility and control over their workloads while minimizing GPU contention between jobs.</li> </ul> </li> <li>Implementing AUKS SLURM Plugin<ul> <li>The AUKS SLURM plugin extends the Kerberos authentication system in HPC environments. By integrating AUKS with SLURM, jobs can access network resources (HCS) that require Kerberos authentication without user intervention during job execution. After the initial user's Kerberos ticket is obtained on the login node and added to the SLURM AUKS repository, network resources are accessible on all cluster nodes for 7 days.</li> </ul> </li> </ol>"},{"location":"general/announcements/2025-05-25-maintenance/","title":"April 2025 Maintenance","text":""},{"location":"general/announcements/2025-05-25-maintenance/#aoraki-research-cluster-april-maintenance-complete","title":"Aoraki Research Cluster April Maintenance Complete","text":"<p>The following was implemented as part of this maintenance period:</p> <ul> <li>Expansion of the cluster with additional nodes</li> <li>Storage software upgrade</li> <li>Network migration</li> <li>Enabling cgroups on all GPUs for better resource control</li> <li>Storage configuration updates, including:<ul> <li>Unmounting of the current/scratch directory to finalise deprecation.</li> </ul> </li> </ul>"},{"location":"general/changelogs/2024-05-30-changelog/","title":"May 2024","text":"<p>CHANGES AND FIXES IMPLEMENTED:</p> <p>Breaking changes:</p> <ul> <li>Memory usage is now limited to the amount specified in your Slurm job script.</li> <li>Partitions now have time limits. All jobs must now include a specified time variable.</li> </ul> <p>Other changes/fixes:</p> <ul> <li>Storage has been updated to allow permissions to be applied more effectively.</li> <li>A more robust login node has been introduced, featuring the same CPU architecture as the cluster nodes, which is ideal for compiling software.<ul> <li>If prompted during SSH login regarding host key change use <code>ssh-keygen -f \"~/.ssh/known_hosts\" -R \"aoraki-login.otago.ac.nz\"</code> to remove the old key and reattempt to SSH.</li> </ul> </li> <li>New compute and GPU nodes have been added, including 5 high-memory (2TB) nodes, 4 high-CPU nodes, 4 A100 GPU nodes, and 1 H100 GPU node, in addition to our existing compute nodes.</li> <li>New Slurm partitions have been created.</li> </ul>"},{"location":"general/changelogs/2024-11-09-changelog/","title":"November 2024","text":"<ul> <li> <p>SLURM Controller Daemon Upgrade   Upgraded <code>slurmctld</code> daemon to version 23.02.0-json, enabling both Lua and JSON capabilities.</p> </li> <li> <p>Secondary SLURM Control Node   Added a secondary SLURM control node.</p> </li> <li> <p>SLURM Database Migration   Migrated the SLURM database to a Galera cluster.</p> </li> <li> <p>Enhanced SLURM Database Backup and Monitoring   Added new backup and monitoring features for the SLURM database.</p> </li> <li> <p>GPU Cgroups Constraints   Implemented Cgroups constraints on GPU resources to ensure fair resource distribution.</p> </li> <li> <p>Weka Storage Upgrade   Upgraded Weka storage to version 4.2.17.</p> </li> <li> <p>Weka Container Configuration Update   Configured Weka containers to use 2 CPUs on each node.</p> </li> <li> <p>CPU Reservation for Weka in SLURM Configuration   Updated SLURM configuration to reserve CPU resources specifically for Weka operations.</p> </li> <li> <p>SLURM Accounting User and Group Import   Imported all users and groups into SLURM accounting for enhanced tracking and management.</p> </li> <li> <p>AUKS Installation and Configuration   Installed and configured AUKS to enable Kerberos authentication across cluster nodes.</p> </li> <li> <p>Node Renaming for Active Directory Compliance   Renamed all nodes to comply with the Active Directory naming scheme and rejoined the domain.</p> </li> <li> <p>OnDemand Reconfiguration   Reconfigured OnDemand to utilise new node names.</p> </li> <li> <p>L40 GPU Node Activation for OnDemand Desktop Use   Enabled the L40 GPU Nodes for use in OnDemand desktop environments.</p> </li> </ul>"},{"location":"general/changelogs/2025-04-25-changelog/","title":"April 2025","text":"<ul> <li>Expansion of the cluster with additional nodes</li> <li>Storage software upgrade</li> <li>Network migration</li> <li>Enabling cgroups on all GPUs for better resource control</li> <li>Storage configuration updates, including:<ul> <li>Unmounting of the current/scratch directory to finalise deprecation.</li> </ul> </li> </ul>"},{"location":"general/faq/disk_usage/","title":"How much disk storage am I using?","text":"<p>For your home directory this will give you an indication of how much space each directory is currently occupying </p> <p>Terminal</p> <pre><code>du -h -d 1 -c ~/\n</code></pre>"},{"location":"general/faq/job_start_time/","title":"How long do I have to wait for my job to start?","text":"<p>There is no guaranteed way to know the exact wait time for a SLURM job, but you can get a good estimate using SLURM commands and by checking cluster utilisation.</p>"},{"location":"general/faq/job_start_time/#check-your-job-in-the-queue","title":"Check your job in the queue:","text":"<p>Use the following command to see your jobs and their status:</p> <p>Terminal</p> <pre><code>squeue -u $USER\n</code></pre> <p>Look for the <code>ST</code> (state) and <code>START_TIME</code> columns. <code>PD</code> means \"pending,\" and <code>R</code> means \"running.\"</p>"},{"location":"general/faq/job_start_time/#estimated-start-time","title":"Estimated start time:","text":"<p>After submitting a job, you can check its estimated start time (if available):</p> <p>Terminal</p> <pre><code> scontrol show job &lt;jobid&gt; | grep StartTime\n</code></pre> <p>This shows SLURM's estimated start time, but be aware that this can change as other jobs are submitted or finish.</p>"},{"location":"general/faq/job_start_time/#tip","title":"Tip:","text":"<p>If you need your jobs to start quickly, consider requesting fewer resources (such as fewer CPUs, less memory, or a shorter time limit) or being flexible with which partition you use.</p> <ul> <li>This command can be useful to list configured and allocated resources for all nodes:</li> </ul> <p>Terminal</p> <pre><code>for i in {01..09} {11..12} {14..43}; do echo aoraki$i; scontrol show node aoraki$i | grep TRES; done\n</code></pre>"},{"location":"general/faq/slurm_job_failures/","title":"Jobs Failing","text":""},{"location":"general/faq/slurm_job_failures/#why-did-my-job-fail","title":"Why did my job fail?","text":"<p>Check the job's output and error logs (e.g., <code>slurm-&lt;jobid&gt;.out</code>) for error messages.</p> <p>You can also view a summary of job exit codes and status with:</p> <p>Terminal</p> <pre><code>sacct -j &lt;jobid&gt; --format=JobID,State,ExitCode\n</code></pre>"},{"location":"general/faq/slurm_job_failures/#how-do-i-change-or-cancel-a-job-after-it-starts-running","title":"How do I change or cancel a job after it starts running?","text":"<ul> <li>To cancel a job, use:</li> </ul> <p>Terminal</p> <pre><code>scancel &lt;jobid&gt;\n</code></pre> <ul> <li>To change resources, you must cancel the job and resubmit it with new resource requests.</li> </ul>"},{"location":"general/guidelines/login_node_usage/","title":"Login Node Usage","text":"<p>The main purpose of the login node is to provide a mechanism for interacting with the scheduler to submit jobs.</p> <p>The login node is a shared resource and is not intended to have computational jobs run on it. There is a limit of 8 CPUS and 60GB of memory per user, exceeding this will trigger warning emails and the possiblity of your tasks being cancelled in order to maintain the stability and accessibilty of the node for everyone. Tasks involving data copying/moving would ideally be kept to less than 30 minutes.</p> <p>Examples of tasks that the login node is suitable for:</p> <ul> <li>Small file transfers through <code>scp</code> or <code>rsync</code></li> <li>editing code/scripts</li> <li>moving or copying data on the file system (durations &lt; 30min)</li> <li>compressing/tarring data small amounts of data (durations &lt; 30min)</li> </ul> <p>For the more intensive tasks we ask that you make use of either the OnDemand HPC Desktop or create an interactive allocation through the scheduler so that resources can be allocated and dedicated to you.</p>"},{"location":"general/guidelines/reasonable_usage/","title":"Reasonable Useage","text":"<p>TODO</p>"},{"location":"general/guidelines/storage_guidelines/","title":"Storage","text":"<p>TODO</p>"},{"location":"getting_started/access/access_overview/","title":"Access Overview","text":"<p>In order to access the resources of the Aoraki Reseach Cluster:</p> <ol> <li>Have a current University of Otago login credentials</li> <li>If your require more than 40 GB of space (default home directory quota), have your PI request a group project space</li> <li>Log in either via:</li> </ol> <p>OnDemand (Web)</p><p>Logging into the Research Cluster through the OnDemand web portal</p> <p>SSH</p><p>Logging into the Research Cluster using SSH</p>"},{"location":"getting_started/access/coldfront/","title":"ColdFont (Allocation Management)","text":"<p>TODO</p>"},{"location":"getting_started/access/login_ssh/","title":"Accessing the login node (ssh)","text":"<p>Use of the login node should be limited to 'lightweight' tasks such as file browsing/copying/moving or submitting jobs into the SLURM scheduler.</p> <p>You can access the Research Cluster login node remotely through the two below mechanisms.</p>"},{"location":"getting_started/access/login_ssh/#ssh-through-a-terminal","title":"SSH through a terminal","text":"<p>To SSH to the login node from your local computer, first open a terminal/commandline and then use the <code>ssh</code> command with username being your Otago username and the address for the remote computer being <code>aoraki-login.otago.ac.nz</code> which will look like this: <code>ssh lasfi12p@aoraki-login.otago.ac.nz</code></p> <p>Terminal</p> Staff NetworkStudent Wifi/VPN <pre><code>ssh &lt;otago-username&gt;@aoraki-login.otago.ac.nz\n</code></pre> <pre><code>&lt;otago-username&gt;@aoraki-login.otago.ac.nz's password:\n</code></pre> <pre><code>ssh &lt;otago-username&gt;@aoraki-login-stu.uod.otago.ac.nz\n</code></pre> <pre><code>&lt;otago-username&gt;@aoraki-login-stu.uod.otago.ac.nz's password:\n</code></pre> <p>Info</p> <p>If you are using Student Wi-Fi or VPN, you will need to use the alternate address aoraki-login-stu: <code>ssh &lt;otago-username&gt;@aoraki-login-stu.uod.otago.ac.nz</code></p> <p>First time connecting</p> <p>The first time you <code>ssh</code> from a computer you will likely see output extremely similar to:</p> <p>Terminal</p> <pre><code>The authenticity of host 'aoraki-login.otago.ac.nz (X.X.X.X)' can't be established.\nED25519 key fingerprint is SHA256:WXAGQmlR5C7rvCOiSSL8PtiuxytA368rjozXXO0NckE.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\n</code></pre> <p>Warning</p> <p>Double check the fingerprint matches one of the following: </p><pre><code>(RSA) SHA256:pzUx1abRS35sbM9n/3OqMpGiF9zegvlG4jQrr78cGFg\n(ECDSA) SHA256:g/BKtsqcMchX7l7YRqDJ98azAumTxzQT0hCCCgVIKxc\n(ED25519) SHA256:WXAGQmlR5C7rvCOiSSL8PtiuxytA368rjozXXO0NckE\n</code></pre> <p>Type 'yes' and Enter, where you will then be prompted for your password (or need to re-<code>ssh</code>) depending on your <code>ssh</code> client.</p> <p>Password not showing</p> <p>When typing in your password when prompted there will be nothing outputed on the screen. Once you have typed your password press Enter to submit and continue. If you mistyped your password you will be prompted to re-enter it.</p>"},{"location":"getting_started/access/login_ssh/#ssh-within-ondemand","title":"SSH within OnDemand","text":"<p>To use the cluster shell access within OnDemand, first connect to the Otago OnDemand web portal and then from the top menu bar select the menu <code>Clusters</code> &gt; <code>Aoraki Cluster Shell Access</code>.</p> <p>You will then be prompted to input your password, similar to SSH through the terminal</p> <p>Figure 1: Open OnDemand Shell</p>"},{"location":"getting_started/access/ondemand_web/","title":"OnDemand Web portal","text":""},{"location":"getting_started/access/ondemand_web/#overview","title":"Overview","text":"<p>Open Ondemand is a direct connection to datacentre HPC resources. It provides a web interface for interactive desktop and computing applications, as well as shell access and the ability to orchestrate batch jobs.</p> <p>Apps/services include:</p> <ul> <li>Interactive Desktop (that includes access to Matlab, Jupyter Notebook, shell and research file systems)</li> <li>MATLAB</li> <li>Jupyter Notebook</li> <li>RStudio </li> <li>Slurm job scheduling</li> <li>File system browsing</li> <li>Terminal/shell access to login nodes</li> </ul>"},{"location":"getting_started/access/ondemand_web/#logging-in","title":"Logging In","text":"<p>Visit https://ondemand.otago.ac.nz in your web browser. You must be on the university network, either on campus or via the University VPN</p> <p>Use your Otago username</p> <ul> <li>Login using your university email address and password You will be required to use 2fa if your university account has it enabled.</li> </ul> <p>After you login you should see the OOD home page. </p> <p>Figure 1: Open onDemand Home Page</p>"},{"location":"getting_started/access/signup/","title":"Getting an Account","text":"<p>Access to the Otago compute cluster is by request. Submit the form below to request an account on the Otago compute cluster. You will be emailed when your account is available for use.</p> <p>Please contact the RTIS team (rtis.support@otago.ac.nz) if you have any questions.</p> <p>Account Creation Form</p> <p>What Next?</p> <p>Log in to the cluster through either the Open OnDemand Web portal or commandline access with ssh</p>"},{"location":"getting_started/data_transfer/data_transfer_overview/","title":"Data Transfer Overview","text":"<p>In order to make the most of the compute platform, chances are you will have data you want to work with but in order to make use of it, you will need to transfer it to a place that the cluster can access.</p> <p>The two locations that the cluster can access data are:</p> <ul> <li>Otago High Capacity Storage (HCS)</li> <li>The cluster research storage</li> </ul> <p>For more details about the storage please see the Storage overview</p> <p>There are several methods for moving data onto and off of the research storage.</p>"},{"location":"getting_started/data_transfer/data_transfer_overview/#transfer-singlefew-files-of-small-size","title":"Transfer Single/Few Files of Small Size","text":"<p>The file browser within Open OnDemand can be used to upload files directly into either your home or a project directory. It can't be used for accessing data on the Otago HCS.</p>"},{"location":"getting_started/data_transfer/data_transfer_overview/#transfer-multiple-or-large-files","title":"Transfer Multiple or Large Files","text":"<p>When it comes to transferring many files, or files that are large this is best done through either:</p> <ul> <li><code>scp</code> or <code>rsync</code> to transfer through an <code>ssh</code> connection</li> <li>Using globus</li> </ul>"},{"location":"getting_started/data_transfer/globus/","title":"Globus","text":""},{"location":"getting_started/data_transfer/globus/#initial-set-up","title":"Initial Set up","text":""},{"location":"getting_started/data_transfer/globus/#transferring-data-between-research-storage-ohau-and-hcs-with-globus","title":"Transferring data between Research Storage (Ohau) and HCS with Globus","text":"<p>Globus is a high-speed, secure data transfer platform that is available to all University of Otago researchers.  It is a great way to transfer large amounts of data between Otago HCS and the Research Storage, as well as to external institutes.</p> <p>Note</p> <p>If you do not yet have a Globus-connected HCS share, please fill out the HCS High Speed Data Transfer Service Access Form and wait for ITS to contact you. You will also need to contact RTIS Solutions to have your Research Storage set up with Globus. For assistance, please email rtis.support@otago.ac.nz with your university username, HCS share names you would like to connect, and whether you are already a Globus user.</p>"},{"location":"getting_started/data_transfer/globus/#how-to-log-in-to-globus-and-transfer-data","title":"How To Log in to Globus and transfer data","text":"<p>If you have used Globus before, this process will look a little bit different than you are used to, so please read ahead.</p> <p>You will need to ensure you are logging in to Globus via your University of Otago login.</p> <p>Go to app.globus.org and search for \"University of Otago\" in the \"Use your existing organizational login\" section of the page.</p> <p>This will redirect you to a University of Otago login page. Please sign in with your Otago username and password.</p> Choose \"University of Otago\" from the organisation drop down. <p>Note</p> <p>You may be prompted to link any existing Globus identities. Do this if you would like to link a GlobusID account with your Otago account.</p>"},{"location":"getting_started/data_transfer/globus/#once-you-are-logged-in","title":"Once you are logged in","text":"<ol> <li>Select the File Manager tab on the left side of the page, and search for \"University of Otago - RTIS\" in the Collection search box.</li> <li>Use the RTIS endpoint to connect to Research Storage AND HCS. </li> <li>Navigate to /fs in the path to find the RTIS-Storage and HCS collections (see image below). University of Otago - HCS endpoint is for off campus transfers and NOT for transfers between Ohau and HCS </li> <li>Click on the collection to connect to it, one on each side of the File Manager.</li> <li>You can now transfer files between the two collections by selecting the Directories or files you want to transfer and clicking the Start arrow in the middle of the page pointing in the direction you want to transfer.</li> </ol> <p>You should then see the RTIS-Storage collection as well as the HCS-Storage collection (with dtn_). Click on the collection to connect to it, one on each side of the File Manager.</p>  Mapping of key locations between Aoraki and Globus (University of Otago - RTIS endpoint) Directory Aoraki Path Globus Path Home /home/&lt;account_name&gt; (~/) /home-dtn/&lt;account_name&gt; (~/) Projects /projects/ /fs/RTIS-Storage/RTIS-Projects HCS /mnt/auto-hcs/&lt;hcs sharename&gt; /fs/HCS-Storage/&lt;dtn-sharename&gt; <p>Info</p> <p>the dtn-sharename may or may not match exactly to the hcs sharename but should be able to be interpretable.</p> <p>You can now transfer files between the two collections by selecting the Directories or files you want to transfer and clicking the Start arrow in the middle of the page pointing in the direction you want to transfer.</p> <p>Warning</p> <p>Ensure that both end points are using \"University of Otago - RTIS\"</p> <p>If you have any issues with Globus, please contact us at rtis.support@otago.ac.nz.</p>"},{"location":"getting_started/data_transfer/globus/#transferring-data-between-research-storage-and-your-desktop","title":"Transferring Data between Research Storage and your Desktop","text":"<p>The RTIS Globus endpoint works with Globus Connect Personal and will transfer data to and from your desktop or lab computer.</p> <p>Note - the HCS endpoint does not work with Globus Connect Personal on campus, but does allow you to share and receive data from other Globus users off campus</p> <p>To transfer data between your desktop and Research Storage, you will need to install the Globus Connect Personal application on your desktop. Follow the instructions on the Globus Connect Personal page to install it.</p> <p>Once you have installed Globus Connect Personal, you can connect your desktop endpoint to the RTIS Globus endpoint and transfer data between your desktop and Research Storage.</p>"},{"location":"getting_started/data_transfer/rsync/","title":"scp or rsync","text":"<p>The <code>scp</code> or <code>rsync</code> commands can be used to transfer data on or off the Research Cluster using a local terminal. For transferring data through a web interface the OnDemand File Manager can be used.</p> <p>This method is recommended for small numbers/sizes of files. For transferring large amounts of data Globus is the recommended solution.</p>"},{"location":"getting_started/data_transfer/rsync/#scp","title":"scp","text":"<p><code>scp</code> is a commandline program that uses transfers data via <code>ssh</code>. It is most useful for copying small files or directories between computers where you are not concerned about resuming transfers in the event of disconnection.</p>"},{"location":"getting_started/data_transfer/rsync/#transferring-onto-the-cluster","title":"Transferring onto the cluster","text":"<p>Terminal</p> <pre><code>scp /local/path/to/file &lt;otago-username&gt;@aoraki-login.otago.ac.nz:/destination/path/on/aoraki\n</code></pre>"},{"location":"getting_started/data_transfer/rsync/#copying-data-from-the-cluster","title":"Copying data from the cluster","text":"<p>Terminal</p> <pre><code>scp  &lt;otago-username&gt;@aoraki-login.otago.ac.nz:/path/on/aoraki/to/file /destination/local/path\n</code></pre>"},{"location":"getting_started/data_transfer/rsync/#rsync","title":"rsync","text":"<p>Rsync has additional functionalies from <code>scp</code>' such as the ability to resume transfers if the connection is interrupted.</p>"},{"location":"getting_started/data_transfer/rsync/#transferring-to-the-cluster","title":"Transferring to the cluster","text":"<p>Terminal</p> <pre><code>rsync /local/path/to/file &lt;otago-username&gt;@aoraki-login.otago.ac.nz:/destination/path/on/aoraki\n</code></pre>"},{"location":"getting_started/data_transfer/rsync/#copying-from-the-cluster","title":"Copying from the cluster","text":"<p>Terminal</p> <pre><code>rsync  &lt;otago-username&gt;@aoraki-login.otago.ac.nz:/path/on/aoraki/to/file /destination/local/path\n</code></pre>"},{"location":"getting_started/running/current_utilisation/","title":"Current utilisation","text":""},{"location":"getting_started/running/current_utilisation/#cluster-traffic-utilisation","title":"Cluster Traffic (Utilisation)","text":"<p>Note</p> <p>This data is only visible if you are on the University of Otago network or connected via VPN.</p>"},{"location":"getting_started/running/current_utilisation/#cpu-allocation-over-the-last-7-days","title":"CPU allocation over the last 7 days","text":""},{"location":"getting_started/running/current_utilisation/#gpu-allocation-over-the-last-7-days","title":"GPU allocation over the last 7 days","text":""},{"location":"getting_started/running/current_utilisation/#aoraki11-2x-a100-80gb","title":"aoraki11 - 2x A100 80GB","text":""},{"location":"getting_started/running/current_utilisation/#aoraki12-2x-a100-80gb","title":"aoraki12 - 2x A100 80GB","text":""},{"location":"getting_started/running/current_utilisation/#aoraki16-4x-h100-80gb","title":"aoraki16 - 4x H100 80GB","text":""},{"location":"getting_started/running/current_utilisation/#aoraki30-4x-h100-96gb","title":"aoraki30 - 4x H100 96GB","text":""},{"location":"getting_started/running/current_utilisation/#aoraki18-3x-l40-48gb","title":"aoraki18 - 3x L40 48GB","text":""},{"location":"getting_started/running/current_utilisation/#aoraki19-3x-l40-48gb","title":"aoraki19 - 3x L40 48GB","text":""},{"location":"getting_started/running/current_utilisation/#aoraki27-2x-a100-40gb","title":"aoraki27 - 2x A100 40GB","text":""},{"location":"getting_started/running/current_utilisation/#aoraki28-2x-a100-40gb","title":"aoraki28 - 2x A100 40GB","text":""},{"location":"getting_started/running/current_utilisation/#aoraki29-7x-l4-24gb","title":"aoraki29 - 7x L4 24GB","text":""},{"location":"getting_started/running/efficiency/","title":"Job Efficiency","text":"<p>Efficient use of cluster resources helps you get results faster and reduces wait times for everyone. Here are the steps you can follow to determine how efficient your SLURM job was:</p> <p>To determine the efficiency of your SLURM job, you can follow these steps:</p>"},{"location":"getting_started/running/efficiency/#1-submit-your-job","title":"1. Submit Your Job","text":"<p>Submit your job to the SLURM scheduler using the <code>sbatch</code> command. Jobs run through OnDemand are done for you and the job number can be found on the job card under the \"My Interactive Sessions\" page.</p> <p>Terminal</p> <pre><code>sbatch my_job_script.sh\n</code></pre> <p>What to look for: The command will return a job ID. Make a note of this ID, as you will use it to check your job's status and efficiency.</p>"},{"location":"getting_started/running/efficiency/#2-monitor-job-progress","title":"2. Monitor Job Progress","text":"<p>Check the status of your job while it is running.</p> <p>Terminal</p> <pre><code>squeue -u your_username\n</code></pre>"},{"location":"getting_started/running/efficiency/#3-check-job-completion","title":"3. Check Job Completion","text":"<p>After your job finishes, view its details using the <code>sacct</code> command.</p> <p>Terminal</p> <pre><code>sacct -j job_id --format=JobID,JobName,Partition,Account,AllocCPUS,State,ExitCode\n</code></pre> <p>What to look for: Confirm that the job state is COMPLETED. If the state is FAILED, OUT_OF_MEMORY, or CANCELLED, investigate the reason (e.g., insufficient resources, errors in your script).</p>"},{"location":"getting_started/running/efficiency/#4-analyze-job-efficiency","title":"4. Analyze Job Efficiency","text":"<p>Use the <code>seff</code> command to see how well your job used the allocated resources.</p> <p>Terminal</p> <pre><code>seff job_id\n</code></pre> <p>The output includes: - Job ID - Job Name - Partition - User - State - Nodes - Cores per node - CPU Utilized - CPU Efficiency - Memory Utilized - Memory Efficiency</p> <p>Example output:</p> <pre><code>Job ID: 123456\nJob Name: my_job\nPartition: compute\nUser: your_username\nState: COMPLETED\nNodes: 1\nCores per node: 4\nCPU Utilized: 01:30:00\nCPU Efficiency: 75.00% of 02:00:00 core-walltime\nMemory Utilized: 2.00 GB\nMemory Efficiency: 50.00% of 4.00 GB\n</code></pre> <p>What to look for: Focus on CPU Efficiency and Memory Efficiency. These show how much of your allocated resources were actually used.</p>"},{"location":"getting_started/running/efficiency/#5-interpret-efficiency-metrics","title":"5. Interpret Efficiency Metrics","text":"<ul> <li>CPU Efficiency: Shows how much of the allocated CPU time was actually used. Low values may mean your job was waiting or underutilized CPUs.</li> <li>Memory Efficiency: Shows how much of the allocated memory was used. Low values may mean you requested more memory than needed.</li> </ul> <p>What to look for: High percentages (close to 100%) mean you used resources efficiently. Low percentages suggest you may be over-requesting resources.</p>"},{"location":"getting_started/running/efficiency/#6-improve-efficiency","title":"6. Improve Efficiency","text":"<ul> <li>If CPU or memory efficiency is low, consider reducing your resource requests in future jobs.</li> <li>If your job was killed for exceeding memory or time, request more resources next time.</li> <li>Use efficiency data to balance resource requests and job reliability. What to look for: Adjust your job scripts based on the efficiency metrics to optimize future runs.</li> </ul> <p>By following these steps and checking the suggested outputs, you can assess and improve the efficiency of your SLURM jobs.</p>"},{"location":"getting_started/running/interactive/","title":"Interactive Jobs","text":"<p>Interactive jobs are best run within Open Ondemand</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/","title":"Slurm Overview","text":"<p>The Otago cluster uses Simple Linux Utility for Resource Management (SLURM) for job management. Slurm is an open-source resource manager and job scheduling system for HPC (High-Performance Computing) which manages jobs, job steps, nodes, partitions (groups of nodes), and other entities on the cluster. Its main purpose is to allocate and manage resources on a cluster, so that jobs can be run in a way that maximizes utilization of the available resources. In order for SLURM to effectively manage resources,  jobs are submitted to a queue and based on the requested resources the scheduler  will run them to make best utilisation of the cluster. </p> <p>In contrast to the usual interactive mode of running commands on your computer,  the main way of interacting with slurm is to 'batch' your jobs up and submit them.  This 'batching' is usually in the form of a bash script which also includes meta information about  the resource requirements such as the amount of time it's expected to take, along with the number of CPUs and RAM needed. At a minimum, a time-limit needs to be specified for your job at submission.</p> <p>The following is a summary of how to submit jobs and interact with the scheduler.  Full documentation for slurm is available at https://slurm.schedmd.com/documentation.html</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#slurm-workflow","title":"Slurm Workflow","text":"<p>Below is a high-level overview of how Slurm schedules and runs your jobs:</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#1-define-job-and-submit","title":"1. Define Job and Submit","text":"<p>You submit your job to Slurm using the <code>sbatch</code> command. Your job is typically a script that specifies what program(s) to run and what resources are needed (e.g., CPUs, memory, time limit).</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#2-resource-request-and-queueing","title":"2. Resource Request and Queueing","text":"<p>Slurm places your job in a queue and waits until the requested resources (CPU cores, memory, GPUs, etc.) become available. The scheduler considers all jobs in the queue and allocates resources based on availability and job priority.</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#3-resource-allocation","title":"3. Resource Allocation","text":"<p>Once resources are available, Slurm allocates them to your job. This includes assigning nodes, CPUs, memory, and any other requested resources.</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#4-job-execution","title":"4. Job Execution","text":"<p>Slurm starts your job on the assigned resources. If your job has multiple tasks or steps, Slurm manages these as job steps, which may run in parallel or sequence depending on your script.</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#5-monitoring-and-management","title":"5. Monitoring and Management","text":"<p>While your job is running, Slurm monitors its progress, manages input/output, and handles any errors. You can check the status of your job at any time using commands like <code>squeue</code> or <code>sacct</code>.</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#6-job-completion-and-output","title":"6. Job Completion and Output","text":"<p>When your job finishes, Slurm collects the output and error messages and writes them to files (by default, <code>slurm-&lt;jobid&gt;.out</code>). You are notified of completion (if enabled), and you can review the results and resource usage.</p> <p>Slurm's workflow ensures that jobs are run efficiently and fairly, making the best use of available cluster resources. </p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#interacting-with-the-slurm-scheduler","title":"Interacting with the SLURM scheduler","text":"<p>The following are commands that are used to find out information about the status of the scheduler and jobs that have been submitted</p> <ul> <li><code>sinfo</code>     View the status of the cluster's compute nodes.     The output includes how many nodes and of what types are currently      available for running jobs</li> <li><code>squeue</code>     Check the current jobs in the batch queue system.      Use <code>squeue --me</code> to view your own jobs.</li> <li><code>scancel</code>     Cancel a job based on its job ID.      <code>scancel 123</code> would cancel the job with ID <code>123</code>. It is only possible to cancel your own jobs.     <code>scancel --me</code> will cancel all of your jobs.</li> <li><code>sacct</code>     Display the job usage metrics after a job has run. This is useful to see resource usage of a job, or determine if it failed.     <code>sacct -j &lt;jobid&gt;</code></li> </ul> <p>Hint</p> <p><code>sinfo</code> will quickly tell you the state of the cluster and <code>squeue</code> will show you all of the jobs running and in the queue. </p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#defining-jobs","title":"Defining Jobs","text":"<p>In order to submit a job to the scheduler using <code>sbatch</code> you first need to define the job through a script. </p> <p>A job script specifies where and how you want to run your job on the cluster and ends with the actual command(s) needed to run your job. The job script file looks much like a standard shell script (<code>.sh</code>) file, but at the top also includes one or more lines that specify options for the SLURM scheduler. These lines take the form of</p> <p>Terminal</p> <pre><code>#SBATCH --some-option-here\n</code></pre> <p>Although these lines start with hash signs (<code>#</code>), and thus are regarded as comments by the shell, they are nonetheless read and interpreted by the SLURM scheduler.</p> <p>It is through these <code>#SBATCH</code> lines that the system resources are requested for the allocation that will run your job.  These parameters can also the supplied as part of calling <code>sbatch</code> at job submission. parameters supplied in this way will override the values in your job script.</p> <p>Common parameters include:</p> <p>Meta</p> <ul> <li><code>--time=</code>     (required) Time limit to be applied to the job. Supplied in format hh:mm:ss.</li> <li><code>--job-name=</code> / <code>-J</code>     Custom job name</li> <li><code>--partition=</code>     aoraki (default) or aoraki_gpu</li> <li><code>--output=</code> / <code>-o</code>     File to save output from stdout</li> <li><code>--error=</code>/ <code>-e</code>     File to save output from stderr</li> <li><code>--dependency=</code>/ <code>-d</code>     Depends on a specified jobid finishing. Can be modifed by completion status. See documentation.</li> <li><code>--chdir=</code> / <code>-D</code>     Directory to change into before running the job</li> </ul> <p>Memory - Only need to supply one of these.</p> <ul> <li><code>--mem=</code>      (default 8GB) Total memory for the job per node. Specify with units (MB, GB)</li> <li><code>--mem-per-cpu=</code>      amount of memory for each cpu (slurm will total this). Specify with units (MB, GB)</li> </ul> <p>Parallelism</p> <ul> <li><code>--cpus-per-task=</code> / <code>-c</code>     Number of cores being requested (default = 1)</li> <li><code>--ntasks=</code>     Number of tasks (default = 1)</li> <li><code>--array=</code>     defines an array task</li> <li><code>--nodes=</code>/ <code>-N</code>     (default = 1). Number of nodes to run jobs across.</li> </ul> <p>The full list of parameters and their descriptions is available at https://slurm.schedmd.com/sbatch.html</p> <p>Here is an example slurm script that would request a single cpu with an allocation of 4 GB of memory, and run for a maximum of 1 minute:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=my_job # define the job name\n#SBATCH --mem=4GB # request an allocation with 4GB of ram\n#SBATCH --time=00:01:00 # job time limit of 1 minute (hh:mm:ss)\n#SBATCH --partition=aoraki # 'aoraki' or 'aoraki_gpu' (for gpu access)\n\n# usual bash commands go below here:\necho \"my example script will now start\"\nsleep 10 # pretend to do something\necho \"my example script has finished\"\n</code></pre> <p>Finding Output</p> <p>Output from running a SLURM batch job is, by default, placed in a file named <code>slurm-%j.out</code>, where the job's ID is substituted for <code>%j</code>; e.g. <code>slurm-478012.out</code>. This file will be created in your current directory; i.e. the directory from within which you entered the <code>sbatch</code> command. Also by default, both command output and error output (to stdout and stderr, respectively) are combined in this file.</p> <p>To specify alternate files for command and error output use:</p> <p><code>--output</code>   destination file for stdout <code>--error</code>   destination file for stderr</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#slurm-scheduler-example","title":"Slurm Scheduler Example","text":"<p>Here is a minimal example of a job script file.  It will run unattended for up to 30 seconds on one of the compute nodes in the <code>aoraki</code> partition, and will simply print out the text <code>hello world</code>.</p> <p>Terminal</p> <pre><code>#!/bin/bash\n# Job name:\n#SBATCH --job-name=test\n#\n# Partition:\n#SBATCH --partition=aoraki\n#\n# Request one node:\n#SBATCH --nodes=1\n#\n# Specify one task:\n#SBATCH --ntasks-per-node=1\n#\n# Number of processors for single task needed for use case (example):\n#SBATCH --cpus-per-task=4\n#\n# Wall clock limit:\n#SBATCH --time=00:00:30\n#\necho \"hello world\"  \n</code></pre> <p>If the text of this file is stored in <code>hello_world.sh</code> you could run and retrieve the result at the Linux prompt as follows</p> <p>Terminal</p> <pre><code>$ sbatch hello_world.sh\nSubmitted batch job 716\n$ cat slurm-716.out\nhello world\n</code></pre> <p>Note</p> <p>By default the output will be stored in a file called <code>slurm-&lt;number&gt;.out</code> where <code>&lt;number&gt;</code> is the job ID assigned by Slurm</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#submitting-jobs","title":"Submitting Jobs","text":"<p>To run your work on the cluster, you need to submit a job script to Slurm. Here\u2019s how:</p> <ul> <li>Write a job script: Create a text file (e.g., <code>myjob.sh</code>) with your commands and resource requests using <code>#SBATCH</code> lines at the top.</li> <li> <p>Submit your job: Use the <code>sbatch</code> command to send your script to the scheduler.</p> <p>Terminal</p> <pre><code>sbatch myjob.sh\n</code></pre> <p>Slurm will respond with a job ID. Make a note of this number.</p> </li> <li> <p>Override script options at submission: You can provide or override Slurm parameters on the command line. For example:</p> <p>Terminal</p> <pre><code>sbatch --job-name=my_job myjob.sh\n</code></pre> <p>Command-line options take precedence over those in your script.</p> </li> <li> <p>Cancel a job: If you need to stop a job, use the <code>scancel</code> command with your job ID.</p> <p>Terminal</p> <pre><code>scancel 123\n</code></pre> <p>You can only cancel your own jobs. To cancel all your jobs:</p> <p>Terminal</p> <pre><code>scancel --me\n</code></pre> </li> </ul> <p>If you do not specify a time limit, your job will not run.</p> <p>At a minimum, you must specify a time limit for your job using <code>--time=hh:mm:ss</code>. This can be set in your script or on the command line. </p> <p>Here we give details on job submission for various kinds of jobs in both batch (i.e., non-interactive or background) mode and interactive mode.</p> <p>In addition to the key options of account, partition, and time limit (see below), your job script files can also contain options to request various numbers of cores, nodes, and/or computational tasks.  There are also a variety of additional options you can specify in your batch files, if desired, such as email notification options when a job has completed. These are all described further below.</p> <p>At a minimum, a time limit must be provided when submitting a job with <code>--time=hh:mm:ss</code> (replacing hh,mm, and ss with number values). This can be provided either be as part of your jobscript or as a commandline parameter.</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#memory-available","title":"Memory Available","text":"<p>Also note that in all partitions except for GPU and HTC partitions, by default the full memory on the node(s) will be available to your job. </p> <p>You should specify the amount using either the total memory required with <code>--mem</code> (which is the same as <code>--mem-per-node</code>), or the amount of ram required per task with <code>--mem-per-task</code>.</p> <p>On the GPU and HTC partitions you get an amount of memory proportional to the number of cores your job requests relative to the number of cores on the node. For example, if the node has 64 GB and 8 cores, and you request 2 cores, you'll have access to 16 GB memory.</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#parallelization","title":"Parallelization","text":"<p>When submitting parallel code, you usually need to specify the number of tasks, nodes, and CPUs to be used by your job in various ways. For any given use case, there are generally multiple ways to set the options to achieve the same effect; these examples try to illustrate what we consider to be best practices.</p> <p>The key options for parallelization are:</p> <ul> <li><code>--nodes</code> (or <code>-N</code>)   indicates the number of nodes to use</li> <li><code>--ntasks-per-node</code>   indicates the number of tasks (i.e., processes one wants to run on each   node)</li> <li><code>--cpus-per-task</code> (or <code>-c</code>)   indicates the number of cpus to be used for each task</li> </ul> <p>In addition, in some cases it can make sense to use the <code>--ntasks</code> (or <code>-n</code>) option to indicate the total number of tasks and let the scheduler determine how many nodes and tasks per node are needed. In general <code>--cpus-per-task</code> will be <code>1</code> except when running threaded code.  </p> <p>Note that if the various options are not set SLURM will in some cases infer what the value of the option needs to be given other options that are set and in other cases will treat the value as being <code>1</code>.  So some of the options set in the example below are not strictly necessary, but we give them explicitly to be clear.</p> <p>Here's an example script that requests an entire Otago node and specifies 20 cores per task.</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --account=account_name\n#SBATCH --partition=aoraki\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=20\n#SBATCH --time=00:00:30\n## Command(s) to run:\necho \"hello world\" \n</code></pre> <p>Only the partition, time, and account flags are required.</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#gpu-jobs","title":"GPU Jobs","text":"<p>Requesting a GPU for a SLURM job requires that your job specifies both</p> <ul> <li>a GPU partition</li> <li>includes the <code>--gres</code> flag</li> </ul> <p>The partition is used to specify a specific GPU, or how much GPU memory is needed</p> <ul> <li><code>aoraki_gpu</code> will get you any free GPU</li> <li><code>aoraki_gpu_H100</code> will get you an entire H100 with 80 GB of GPU memory</li> <li><code>aoraki_gpu_L40</code> will get you an entire L40 with 48GB of GPU memory</li> <li><code>aoraki_gpu_A100_80GB</code> will get you an A100 with 80GB of GPU memory to use</li> <li><code>aoraki_gpu_A100_40GB</code> will get you an A100 with 40GB of GPU memory to use</li> </ul> <p>Make sure to request at least two CPUs for each GPU requested, using <code>--cpus-per-task</code></p> <p>You can request multiple GPUs with syntax like this (in this case for two    GPUs): <code>--gpus-per-node=2</code></p> <p>Please see the SLURM GPU examples page for examples of how to submit SLURM jobs that require a GPU.</p>"},{"location":"getting_started/running/using_slurm/slurm_quickstart/#job-accounting-efficency","title":"Job Accounting / Efficency","text":"<p>To view your job information you can use the <code>sacct</code> command. </p> <p>To view detailed job information:</p> <p>Terminal</p> <pre><code>sacct --format=\"JobID,JobName,Elapsed,AveCPU,MinCPU,TotalCPU,Alloc,NTask,MaxRSS,State\" -j &lt;job_id_number&gt;\n</code></pre> <pre><code>sacct --format=\"JobID,JobName,Elapsed,AveCPU,MinCPU,TotalCPU,Alloc,NTask,MaxRSS,State\" -j 321746\nJobID           JobName    Elapsed     AveCPU     MinCPU   TotalCPU  AllocCPUS   NTasks     MaxRSS      State\n------------ ---------- ---------- ---------- ---------- ---------- ---------- -------- ---------- ----------\n321746       ondemand/+   23:11:07                        00:05.337          4                     CANCELLED+\n321746.batch      batch   23:11:08   00:00:00   00:02:56  00:05.337          4        1    683648K  CANCELLED\n</code></pre>"},{"location":"getting_started/running/using_slurm/slurm_examples/array-slurm/","title":"Array Jobs","text":"<p>Slurm array jobs allow you to submit many similar jobs at once, each with a unique task ID. This is useful for running the same script with different input parameters, files, or configurations. Each array task runs independently and can access its own task ID using the environment variable <code>SLURM_ARRAY_TASK_ID</code>.</p>"},{"location":"getting_started/running/using_slurm/slurm_examples/array-slurm/#how-array-jobs-work","title":"How Array Jobs Work","text":"<p>When you submit an array job, Slurm schedules multiple tasks, each with a different value of <code>SLURM_ARRAY_TASK_ID</code>. You specify the range of task IDs when submitting the job. For example, <code>--array=1-10</code> will run 10 tasks with IDs from 1 to 10.</p> <p>Inside your job script, you can use the task ID to select input files, set parameters, or control the behavior of each task.</p>"},{"location":"getting_started/running/using_slurm/slurm_examples/array-slurm/#example-array-job-script","title":"Example Array Job Script","text":"<p>Below is an example Slurm batch script that demonstrates how to use the array task ID to process different input files:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --partition=aoraki\n#SBATCH --job-name=array_example\n#SBATCH --time=00:01:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=512MB\n#SBATCH --output=array_example_%A_%a.out\n#SBATCH --array=1-5\n\necho \"This is task ID $SLURM_ARRAY_TASK_ID\"\n</code></pre> <p>To submit this array job, use:</p> <p>Terminal</p> <pre><code>sbatch array_example.sh\n</code></pre> <p>Alternatively, you can define the array size directly on the command line when submitting the job, overriding the value in the script:</p> <p>Terminal</p> <pre><code>sbatch --array=1-10 array_example.sh\n</code></pre> <p>This will run tasks with IDs from 1 to 10, regardless of the value set in the script.</p> <p>Each task will process a different input file based on its task ID.</p>"},{"location":"getting_started/running/using_slurm/slurm_examples/array-slurm/#example-using-task-id-to-select-a-filename-from-a-list","title":"Example: Using Task ID to Select a Filename from a List","text":"<p>Suppose you have a file called <code>file_list.txt</code> containing a list of filenames, one per line:</p> <pre><code>input_a.txt\ninput_b.txt\ninput_c.txt\ninput_d.txt\ninput_e.txt\n</code></pre> <p>You can use the array task ID to select the corresponding line from this file in your Slurm script:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --partition=aoraki\n#SBATCH --job-name=array_example\n#SBATCH --time=00:01:00\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=512MB\n#SBATCH --output=array_example_%A_%a.out\n#SBATCH --array=1-5\n\n# Get the filename for this task by matching the task id to the line number\nFILENAME=$(sed -n \"${SLURM_ARRAY_TASK_ID}p\" file_list.txt)\n\necho \"Task ID $SLURM_ARRAY_TASK_ID will process $FILENAME\"\n# Replace the following line with your actual processing command\ncat \"$FILENAME\"\n</code></pre> <p>This approach allows you to flexibly assign files to tasks based on the order in <code>file_list.txt</code>.</p>"},{"location":"getting_started/running/using_slurm/slurm_examples/array-slurm/#controlling-the-number-of-simultaneous-tasks","title":"Controlling the Number of Simultaneous Tasks","text":"<p>By default, Slurm may run all array tasks at once, depending on available resources. You can limit the number of tasks running simultaneously by adding a percent sign (<code>%</code>) and a number to the array specification. For example, <code>--array=1-10%3</code> will run at most 3 tasks at the same time.</p> <p>Terminal</p> <pre><code># Submit an array job with 10 tasks, but only 3 running at once\nsbatch --array=1-10%3 array_example.sh\n</code></pre> <p>You can also set this in your batch script:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --array=1-10%3\n# ...other SBATCH options...\n\necho \"This is task ID $SLURM_ARRAY_TASK_ID\"\n</code></pre> <p>This is useful for controlling resource usage and avoiding overloading the cluster.</p>"},{"location":"getting_started/running/using_slurm/slurm_examples/dependent_jobs/","title":"Dependent Jobs","text":"<p>Slurm allows you to specify dependencies between jobs, so that a job will only start after another job (or jobs) have finished, failed, or met other conditions. This is useful for workflows where later steps depend on the output of earlier steps.</p>"},{"location":"getting_started/running/using_slurm/slurm_examples/dependent_jobs/#how-dependencies-work","title":"How Dependencies Work","text":"<p>You can use the <code>--dependency</code> option with <code>sbatch</code> to set dependencies. The most common type is <code>afterok</code>, which means the dependent job will start only if the specified job(s) complete successfully.</p> <p>Example syntax:</p> <p>Terminal</p> <pre><code>sbatch --dependency=afterok:&lt;jobid&gt; second_job.sh\n</code></pre>"},{"location":"getting_started/running/using_slurm/slurm_examples/dependent_jobs/#example-simple-dependency","title":"Example: Simple Dependency","text":"<p>Submit a first job and capture its job ID:</p> <p>Terminal</p> <pre><code>jobid=$(sbatch first_job.sh | awk '{print $4}')\n</code></pre> <p>Then submit a second job that depends on the first:</p> <p>Terminal</p> <pre><code>sbatch --dependency=afterok:$jobid second_job.sh\n</code></pre>"},{"location":"getting_started/running/using_slurm/slurm_examples/dependent_jobs/#example-linking-array-jobs-by-task-id","title":"Example: Linking Array Jobs by Task ID","text":"<p>Suppose you have two array jobs (see array examples for more about array jobs) and you want each task in the second array to depend on the corresponding task in the first array.</p> <p>first_array.sh</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --array=1-5\necho \"First array job, task $SLURM_ARRAY_TASK_ID\"\n</code></pre> <p>second_array.sh</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --array=1-5\necho \"Second array job, task $SLURM_ARRAY_TASK_ID\"\n</code></pre> <p>Submit the first array job and capture its job ID:</p> <p>Terminal</p> <pre><code>first_jobid=$(sbatch --array=1-5 first_array.sh | awk '{print $4}')\n</code></pre> <p>Then submit the second array job, linking each task by its array index:</p> <p>Terminal</p> <pre><code>sbatch --array=1-5 --dependency=afterok:${first_jobid}_%a second_array.sh\n</code></pre> <p>Here, <code>%a</code> is replaced by the array index, so task 1 in <code>second_array.sh</code> depends on task 1 in <code>first_array.sh</code>, and so on.</p> <p>This setup ensures that each corresponding task in the second array only starts after its counterpart in the first array completes successfully</p>"},{"location":"getting_started/running/using_slurm/slurm_examples/gpu-slurm/","title":"Using a GPU with Slurm","text":"<p>These examples can be found at https://appsgit.otago.ac.nz/projects/RTIS-SP/repos/slurm-code-examples/browse</p> <p>Or downloaded and browsed on the cluster by:</p> <p>Terminal</p> <pre><code>git clone https://appsgit.otago.ac.nz/scm/rtis-sp/slurm-code-examples.git\n</code></pre> <p>The key things to remember are:</p> <ul> <li>Submit to a partition with nodes with GPUs</li> <li>Include the <code>--gres</code> flag.</li> <li>Request at least two CPUs for each GPU requested, using <code>--cpus-per-task</code></li> <li>You can request multiple GPUs with syntax like this (in this case for two    GPUs): <code>--gpus-per-node=2</code></li> <li>The partition is used to specify a specific GPU, or how much GPU memory is needed<ul> <li>aoraki_gpu will get you any free GPU</li> <li>aoraki_gpu_H100 will get you an entire H100 with 80 GB of GPU memory</li> <li>aoraki_gpu_L40 will get you an entire L40 with 48GB of GPU memory</li> <li>aoraki_gpu_A100_80GB will get you an A100 with 80GB of GPU memory to use</li> <li>aoraki_gpu_A100_40GB will get you an A100 with 40GB of GPU memory to use</li> </ul> </li> </ul> <p>Note</p> <p>You may see some scripts use a command line <code>--gres=gpu:2</code> to specify two GPUS. This way of specifying the number of GPUs to use is in the process of being deprecated.</p> <p>Running a GPU job on Slurm involves specifying the required resources and submitting the job to the scheduler. Here are the basic steps to run a GPU job on Slurm:</p> <ol> <li> <p>Request the required resources.     In order to run a GPU job on Slurm, you need to specify the number of GPUs     and the amount of memory required.     For example, to request a single GPU with 16GB of CPU memory, you would add the     following line to your Slurm job script:</p> <p>Terminal</p> <pre><code>#SBATCH --gpus-per-node=1\n#SBATCH --mem=16GB # 16 GB CPU Memory\n</code></pre> </li> <li> <p>Load the necessary modules.     Depending on the software and libraries you are using you may need to load     additional modules to access the GPU resources.      This can usually be done using the module load command.      For example, to load the CUDA toolkit:</p> <p>Terminal</p> <pre><code>lua\nmodule load cuda\n</code></pre> </li> <li> <p>Write the job script.     Create a job script that specifies the commands and arguments needed to run     your GPU job.     This can include running a CUDA program, a TensorFlow script, or any other     GPU-accelerated code.</p> </li> <li> <p>Submit the job.     Use the sbatch command to submit the job script to the Slurm scheduler.     For example:</p> <p>Terminal</p> <pre><code>sbatch my_gpu_job.sh\n</code></pre> <p>Once your job is submitted, Slurm will allocate the requested resources and schedule the job to run on a node with the appropriate GPU. You can monitor the status of your job using the squeue command and view the output using the sacct command once the job completes.</p> </li> </ol> <p>Here's an example script that will return the information on the GPU available on <code>aoraki_gpu</code>:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --account=account_name\n#SBATCH --partition=aoraki_gpu\n#SBATCH --gpus-per-node=1\n#SBATCH --mem=4GB\n#SBATCH --time=00:00:30\nnvidia-smi\n</code></pre> <p>Hint</p> <p>If you want to run a GPU job interactively you can create slurm session on a gpu node (Partition aoraki_gpu_L40 in this example) using the following command which simply adds the <code>--gres=gpu:1</code> flag to the <code>srun</code> command:</p> <p><code>srun --ntasks=1 --partition=aoraki_gpu_L40 --gres=gpu:1  --cpus-per-task=4 --time=0-03:00 --mem=50G --pty  /bin/bash</code></p> <p>For a slightly more involved example consider the following C code.</p> <p>Terminal</p> <pre><code>#include&lt;stdio.h&gt;\n\n#define BLOCKS 2\n#define WIDTH 16\n\n__global__ void whereami() {\n  printf(\"I'm thread %d in block %d\\n\", threadIdx.x, blockIdx.x);\n}\n\nint main() {\n  whereami&lt;&lt;&lt;BLOCKS, WIDTH&gt;&gt;&gt;();\n  cudaDeviceSynchronize();\n  return 0;\n}\n</code></pre> <p>If this is stored in the file <code>whereami.cu</code> and compiled with  <code>nvcc whereami.cu -o whereami</code> we can use the Slurm job script</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --account=account_name\n#SBATCH --partition=aoraki_gpu\n#SBATCH --gpus-per-node=1\n#SBATCH --mem=4GB\n#SBATCH --time=00:00:30\nwhereami\n</code></pre> <p>to obtain output such as the following (ordering of lines may differ):</p> <p>Terminal</p> <pre><code>I'm thread 0 in block 1 \nI'm thread 1 in block 1 \nI'm thread 2 in block 1 \nI'm thread 3 in block 1 \nI'm thread 4 in block 1 \nI'm thread 5 in block 1 \nI'm thread 6 in block 1 \nI'm thread 7 in block 1 \nI'm thread 8 in block 1 \nI'm thread 9 in block 1 \nI'm thread 10 in block 1 \nI'm thread 11 in block 1 \nI'm thread 12 in block 1 \nI'm thread 13 in block 1 \nI'm thread 14 in block 1 \nI'm thread 15 in block 1 \nI'm thread 0 in block 0 \nI'm thread 1 in block 0 \nI'm thread 2 in block 0 \nI'm thread 3 in block 0 \nI'm thread 4 in block 0 \nI'm thread 5 in block 0 \nI'm thread 6 in block 0 \nI'm thread 7 in block 0 \nI'm thread 8 in block 0 \nI'm thread 9 in block 0 \nI'm thread 10 in block 0 \nI'm thread 11 in block 0 \nI'm thread 12 in block 0 \nI'm thread 13 in block 0 \nI'm thread 14 in block 0 \nI'm thread 15 in block 0\n</code></pre>"},{"location":"getting_started/running/using_slurm/slurm_examples/heterogeneous_jobs/","title":"Heterogeneous Jobs","text":"<p>Heterogeneous jobs in Slurm allow you to submit a single job that requests different resources for different job components (job steps). This is useful when your workflow requires, for example, both CPU-only and GPU resources, or different memory or core counts for different tasks, all within the same job allocation.</p>"},{"location":"getting_started/running/using_slurm/slurm_examples/heterogeneous_jobs/#how-heterogeneous-jobs-work","title":"How Heterogeneous Jobs Work","text":"<p>A heterogeneous job is composed of two or more job components, each with its own resource requirements. Slurm schedules all components together, and each component runs as a separate job step.</p>"},{"location":"getting_started/running/using_slurm/slurm_examples/heterogeneous_jobs/#example-cpu-and-gpu-components","title":"Example: CPU and GPU Components","text":"<p>Suppose you have a workflow where the first part runs on CPUs and the second part requires a GPU. You can submit a heterogeneous job by specifying the resources for each component directly in your script using <code>#SBATCH</code> lines and separating components with <code>#SBATCH hetjob</code>.</p> <p>Below is an example heterogeneous job script for a cluster where GPU jobs must use a separate partition (e.g., <code>aoraki_gpu</code>):</p> <p>Terminal</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=hetero_example\n\n# Component 1: CPU-only\n#SBATCH --partition=aoraki\n#SBATCH --ntasks=2\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=8G\n\n#SBATCH hetjob\n\n# Component 2: GPU\n#SBATCH --partition=aoraki_gpu\n#SBATCH --gres=gpu:1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=16G\n\necho \"This is component $SLURM_HET_GROUP_ID of the heterogeneous job\"\n\nif [ \"$SLURM_HET_GROUP_ID\" -eq 0 ]; then\n    echo \"Running CPU-only component\"\n    # Place CPU-only commands here\nelse\n    echo \"Running GPU component\"\n    # Place GPU commands here\nfi\n</code></pre> <p>To submit this job, simply run:</p> <p>Terminal</p> <pre><code>sbatch hetero_script.sh\n</code></pre> <p>Each component will execute the script, and you can branch logic based on <code>SLURM_HET_GROUP_ID</code>.</p> <p>Note</p> <p>If your cluster uses a separate partition for GPU jobs (e.g., <code>aoraki_gpu</code>), you must specify the partition for each component using <code>#SBATCH --partition=...</code> in the relevant section. For example:</p> <pre><code>#SBATCH --partition=aoraki           # For CPU component\n#SBATCH hetjob\n#SBATCH --partition=aoraki_gpu       # For GPU component\n</code></pre> <p>Each component can have its own partition and resource requirements. Make sure to match the partition to the resources (CPU or GPU) needed for each component.</p> <p>For more details, see the Slurm documentation on heterogeneous jobs.</p>"},{"location":"getting_started/running/using_slurm/slurm_examples/interactive_jobs/","title":"Interactive Jobs","text":"<p>In some instances, you may need to use software that requires user interaction rather than running programs or scripts in batch mode.  To do so, you must first start an instance of an interactive shell on a Otago login node, within which you can then run your software on that node. To run such an interactive job on a compute node, you'll use <code>srun</code>.  Here is a basic example that launches an interactive bash shell on that node and includes the required account and partition options:</p> <p>Terminal</p> <pre><code>[user@aoraki-login ~]$ srun --pty --partition=aoraki --time=00:05:30 -N 1 -n 4 /bin/bash\n</code></pre> <p>Once your job starts, the prompt will change and indicate you are on a compute node rather than a login node:</p> <p>Terminal</p> <pre><code>srun: job 669120 queued and waiting for resources  \nsrun: job 669120 has been allocated resources  \n[user@aoraki13 ~]\n</code></pre>"},{"location":"getting_started/running/using_slurm/slurm_examples/python-slurm/","title":"Using Python with Slurm","text":"<p>These examples can be found at https://appsgit.otago.ac.nz/projects/RTIS-SP/repos/slurm-code-examples/browse</p> <p>Or downloaded and browsed on the cluster by:</p> <p>Terminal</p> <pre><code>git clone https://appsgit.otago.ac.nz/scm/rtis-sp/slurm-code-examples.git\n</code></pre> <p>TODO</p>"},{"location":"getting_started/running/using_slurm/slurm_examples/r-slurm/","title":"Using R with Slurm","text":"<p>These examples can be found at https://appsgit.otago.ac.nz/projects/RTIS-SP/repos/slurm-code-examples/browse (need to be on the campus network to access)</p> <p>Or downloaded and browsed on the cluster by:</p> <p>Terminal</p> <pre><code>git clone https://appsgit.otago.ac.nz/scm/rtis-sp/slurm-code-examples.git\n</code></pre> <p>If you have downloaded the repository, the following code examples are in the <code>r_examples</code> subdirectory: </p> <p>Terminal</p> <pre><code>cd slurm-code-examples/r_examples/\n</code></pre>"},{"location":"getting_started/running/using_slurm/slurm_examples/r-slurm/#slurm-job-calling-an-r-script","title":"SLURM job calling an R script","text":"<p>This pair of scripts represents how an Rscript can be run using the SLURM scheduler.</p> <p>Create the R script <code>hello_rscript.R</code> with the following contents:</p> <p>R</p> <pre><code>print(\"hello\")\n</code></pre> <p>Create the slurm script <code>run_hello_rscript.sh</code> with the following contents:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --mem=512MB\n#SBATCH --time=00:01:00\n#SBATCH --cpus-per-task=1\n#SBATCH --ntasks=1\n\n# load R v4.4.3 through the modules\nmodule load r/4.4.3\n\n# run the rscript\nRscript hello_rscript.R\n</code></pre> <p>The job can now be submitted to the scheduler with:</p> <p>Terminal</p> <pre><code>[user@aoraki-login r_examples]$ sbatch run_hello_rscript.sh\n</code></pre>"},{"location":"getting_started/running/using_slurm/slurm_examples/r-slurm/#example-of-using-multiple-cores-within-a-single-rscript","title":"Example of using multiple cores within a single Rscript","text":"<p>The following are two examples creating a job that uses multiple cores on a single node with R managing the the parallelism. In R, either the <code>parallel</code>  package or the <code>future</code> package are (mainly) used to do this. The <code>parallelly</code> package provides some bonus functionality to both.</p> <p>The key difference to the previous example is that we need to define how to parallelise the code. This is done through <code>parallel</code> or <code>future</code> and we need to define the number of cores/cpus to use for the parallelism within the SLURM script through <code>--cpus-per-task</code> which gets stored in a BASH environment variable <code>$SLURM_CPUS_PER_TASK</code>. There are multiple approaches to how this can be done but for this example two have been demonstrated</p> <p>The overall task demonstrated is to calculate the means for sub-groups of a dataset in parallel. </p> <p>The first approach uses the packages <code>parallel</code> and <code>doParallel</code> to create and manage the parallelism within R. It will create and register a 'cluster' of 'workers' within R and then pass each sub-task to a worker.</p> <p><code>r_multicore_example-parallel.R</code>:</p> <p>R</p> <pre><code>library(parallelly)\nlibrary(parallel)\nlibrary(doParallel)\n\n# automatically detect number of available cores\nncpu &lt;- parallelly::availableCores()\n\n\ndoParallel::registerDoParallel(cores = ncpu)\n\n# calculate the mean sepal length for each iris species in parallel\nmean_petal_lengths &lt;- foreach(species = unique(iris$Species), .combine = 'c') %dopar% {\n    m &lt;- mean(iris[iris$Species == species, \"Sepal.Length\"])\n    names(m) &lt;- species\n    return(m)\n}\n\nprint(mean_petal_lengths)\n</code></pre> <p><code>run_multicore_r_example-parallel.sh:</code></p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --mem=512MB\n#SBATCH --time=00:01:00\n#SBATCH --cpus-per-task=4\n#SBATCH --ntasks=1\n\n# load R v4.4.3 through the modules\nmodule load r/4.4.3\n\n# run the rscript\nRscript r_multicore_example-parallel.R\n</code></pre> <p>Submitting the job:</p> <p>Terminal</p> <pre><code>[user@aoraki-login r_examples]$ sbatch run_multicore_r_example-parallel.sh\n</code></pre> <p>The following is same example as above but insetad implemented using the <code>furrr</code> package to parallelise <code>purrr</code> using <code>future</code>.</p> <p><code>r_multicore_example-furrr.R</code>:</p> <p>R</p> <pre><code>library(parallelly)\nlibrary(furrr)\n\n\nncpus &lt;- parallelly::availableCores()\nplan(\"multisession\", workers = ncpus)\n\nmean_species &lt;- function(x){\n    mean(iris[iris$Species == x, \"Sepal.Length\"])\n}\n\nspecies &lt;- unique(iris$Species)\nmean_petal_lengths &lt;- furrr::future_map_dbl(species, mean_species)\nnames(mean_petal_lengths)  &lt;- species           \n\nprint(mean_petal_lengths)\n</code></pre> <p><code>run_multicore_r_example-furrr.sh</code></p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --mem=512MB\n#SBATCH --time=00:01:00\n#SBATCH --cpus-per-task=4\n#SBATCH --ntasks=1\n\n# load R v4.4.3 through the modules\nmodule load r/4.4.3\n\n# run the rscript\nRscript r_multicore_example-furrr.R \n</code></pre> <p>Submitting the job:</p> <p>Terminal</p> <pre><code>[user@aoraki-login r_examples]$ sbatch run_multicore_r_example-furrr.sh\n</code></pre>"},{"location":"getting_started/running/using_slurm/slurm_examples/r-slurm/#slurm-array-job-with-r","title":"SLURM array job with R","text":"<p>An array job allows you to define the resources for a single job but run many instances of it.  Instead of submitting many individual jobs, it is best to use an array job as it is more effcient for the scheduler.  A common use case would be to define a job for a sample, and then run the job on all samples in parallel. SLURM facilitates this through the array job type. For an array job there are two bash environment variables that you can  make use of: <code>SLURM_JOBID</code> which is the overall job, and <code>SLURM_ARRAY_TASK_ID</code> which is the id assigned to  that specific \"subjob\". A common use for the task id would be to use it as an index on a sample sheet.</p> <p>A second benefit of using an array over individually submitting many of the same job is that if you want to rerun a job, you can specify specific array indexes, instead of many differnt job ids.</p> <p>To make use of these variables inside your R task, you can either pass them through as a commandline argument or you can access them from within R with <code>option()</code></p> <p>The scenario for the example will be again calculating the mean for each species in the <code>iris</code> data set but this time instead of using a single job with multiple cores, we'll use a single job per species utilising the SLURM array.</p> <p>Here is an example of running 3 jobs in parallel using a slurm array passing the array index as a commandline argument to R</p> <p><code>r_array_job_args.R</code>:</p> <p>R</p> <pre><code>args &lt;- commandArgs(trailingOnly = TRUE)\n# args come in as strings so need to convert to numeric type\nid &lt;- as.numeric(args[1]) \nmessage(paste(\"SLURM_ARRAY_TASK_ID was: \", id))\n\n# determine which of the species is the target for the job\njob_species &lt;- unique(iris$Species)[id]\nspecies_mean_sepal_length &lt;- mean(iris[iris$Species == job_species, \"Sepal.Length\"])\nresults &lt;- data.frame(species = job_species, mean_sepal_length = species_mean_sepal_length)\nwrite.csv(results, paste0(job_species,\".csv\"), row.names = FALSE)\n</code></pre> <p><code>run_array_job_example-args.sh</code>:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --mem=512MB\n#SBATCH --time=00:01:00\n#SBATCH --cpus-per-task=1\n#SBATCH --ntasks=1\n#SBATCH --array=1-3 # run three of this job with the indexes 1,2,3\n\n# load R v4.4.3 through the modules\nmodule load r/4.4.3\n\n# run the rscript\nRscript r_array_job_example-args.R ${SLURM_ARRAY_TASK_ID}\n</code></pre> <p>Submitting the job:</p> <p>Terminal</p> <pre><code>[user@aoraki-login r_examples]$ sbatch run_array_job_example-args.sh\n</code></pre> <p>And here is an example of the same job but instead accessing the system environment variable from within R:</p> <p><code>r_array_job_example-env.R</code>:</p> <p>R</p> <pre><code># read the environment varible SLURM_ARRAY_TASK_ID and convert to numeric data type\nid &lt;- as.numeric(Sys.getenv(x = \"SLURM_ARRAY_TASK_ID\"))\nmessage(paste(\"SLURM_ARRAY_TASK_ID was: \", id))\n\n# determine which of the species is the target for the job\njob_species &lt;- unique(iris$Species)[id]\nspecies_mean_sepal_length &lt;- mean(iris[iris$Species == job_species, \"Sepal.Length\"])\nresults &lt;- data.frame(species = job_species, mean_sepal_length = species_mean_sepal_length)\nwrite.csv(results, paste0(job_species,\".csv\"), row.names =  FALSE)\n</code></pre> <p><code>run_array_job_example-env.sh</code>:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --mem=512MB\n#SBATCH --time=00:01:00\n#SBATCH --cpus-per-task=1\n#SBATCH --ntasks=1\n#SBATCH --array=1-3 # run three of this job with the indexes 1,2,3\n\n# load R v4.4.3 through the modules\nmodule load r/4.4.3\n\n# run the rscript\nRscript r_array_job_example-env.R \n</code></pre> <p>Submitting the job:</p> <p>Terminal</p> <pre><code>[user@aoraki-login r_examples]$ sbatch run_array_job_example-env.sh\n</code></pre> <p>One disadvantage of this approach is it can be harder to create and debug the R code as it relies on environmental variables being set prior to execution,  rather than passing the values at runtime on the commandline through arguments.</p>"},{"location":"getting_started/running/using_slurm/slurm_examples/r-slurm/#slurm-job-dependencies-with-r","title":"SLURM job dependencies with R","text":"<p>SLURM job dependencies allow you to link jobs together and subsequent jobs will only run depending on  the running status of pre-requisite jobs. This allows you to create workflows with different job requirements for the different stages.</p> <p>When submitting the <code>-d</code> (or <code>--dependency=</code>) option is supplied to <code>sbatch</code> with the job id to make the job dependent on e.g. <code>sbatch -d 1234</code>.  The main advantage of using dependencies is that SLURM will cordinate the running (or cancelling if failed) of downstream dependent jobs.  There are extra options for the dependencies such as <code>-d afterok:&lt;jobid&gt;</code>, <code>-d afterany:&lt;job_id&gt;</code>. See https://slurm.schedmd.com/sbatch.html for more infomation regarding the <code>-d</code> option.</p> <p>In the previous example we outputted the mean for each group but it would be good to have a single output that summarised the results. We could  create a second job that was dependant on the previous job compeleting before it ran to take the results and combine them together. This type of  work flow is often referred to as a scatter-gather as there is a scattering phase to calculate results per group and a gathering phase to combine them back together.</p> <p><code>r_combine_results.R</code>:</p> <p>R</p> <pre><code>results_list &lt;-list()\nfor(species in unique(iris$Species)){\n    results_list[[species]] &lt;- read.csv(paste0(species,\".csv\"), header=TRUE)\n}\ncombined_results &lt;- do.call(rbind, results_list)\nwrite.csv(combined_results, \"combined_results.csv\", row.names=FALSE)\n</code></pre> <p><code>run_combine_example.sh</code></p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --mem=512MB\n#SBATCH --time=00:01:00\n#SBATCH --cpus-per-task=1\n#SBATCH --ntasks=1\n\n\n# load R v4.4.3 through the modules\nmodule load r/4.4.3\n\n# run the rscript\nRscript r_combine_results.R\n</code></pre> <p>Now in order to run this as a dependency we supply <code>-d &lt;jobid&gt;</code> to sbatch with the jobid of the job upon which we are depending to run first when submitting.</p> <p>e.g.</p> <p>Terminal</p> <pre><code>[user@aoraki-login r_examples]$ sbatch run_array_job_example-args.sh \nSubmitted batch job 323173\n[user@aoraki-login r_examples]$ sbatch -d 323173 run_combine_example.sh \nSubmitted batch job 323174\n</code></pre> <p>This requires you to pay attention the job number however, so instead we can wrap the submission in a bash script to automatically grab the jobid:</p> <p><code>run_scatter_gather.sh</code></p> <p>Terminal</p> <pre><code># store the output from sbatch into a variable\nFIRST_JOB=$(sbatch run_array_job_example-args.sh)\n\n# extract out the job id from the variable\n# use space as a delimiter and take the 4th field\nFIRST_JOB_ID=$(echo \"${FIRST_JOB}\" | cut -d\" \" -f 4)\n\n# submit the second job setting the dependency on the first\nsbatch -d ${FIRST_JOB_ID} run_combine_example.sh\n</code></pre> <p>run the script to do the submission:</p> <p>Terminal</p> <pre><code>bash run_scatter_gather.sh\n</code></pre>"},{"location":"getting_started/software/software_overview/","title":"Software Overview","text":""},{"location":"getting_started/software/software_overview/#available-software","title":"Available Software","text":""},{"location":"getting_started/software/software_overview/#managing-your-own-software","title":"Managing your own Software","text":"Feature / Tool Lmod / Modules conda / mamba Apptainer Spack SBGrid venv renv Language Support Language-agnostic Multi-language (Python, R, etc.) Language-agnostic (container-based) Multi-language (C, Fortran, Python) Scientific tools (bio/chem/structural) Python only R only Scope Manages access to software Packages + native/system dependencies Full OS + environment Source builds + dependency trees Curated software suite Python packages R packages Package Manager N/A (env modules only) <code>conda</code>, <code>mamba</code> None (uses definition files) <code>spack</code> SBGrid client <code>pip</code> <code>renv</code> Reproducibility Medium (depends on loaded modules) High (<code>environment.yml</code>, lockfiles) Very high (frozen image) High (via concretization + locks) High (versioned environment) Medium (<code>requirements.txt</code>) High (<code>renv.lock</code>) Binary Packages N/A Yes (conda-forge, etc.) Optional (user-defined or prebuilt) Mostly built from source Yes (precompiled) From PyPI (wheels) CRAN, Bioconductor Non-language Deps Managed by system/site admin Built-in (e.g., OpenCV, HDF5, BLAS) Fully included in image Fully supported Bundled per environment Manual (via OS tools) Manual or external Custom Builds No Some Yes (via definition file) Yes (fully configurable) No No No Cross-Platform Depends on cluster/system Excellent (Windows/Linux/macOS) Excellent (Linux-native) Builds per target system Linux/macOS Good, minor issues Excellent HPC-Friendly Yes (standard on clusters) Yes Yes (designed for HPC) Yes (made for HPC) Yes Yes Yes Best Use Case Shared software on HPC systems Data science, ML, research computing Portable workflows, pipelines on HPC Custom scientific toolchains Structural biology labs Simple Python apps R reproducibility &amp; isolation Complexity Low for users, high for admins Medium (lower with <code>mamba</code>) Medium (def files + CLI) High (very flexible, steep learning curve) Very low (plug-and-play) Low Low Use Case Recommended Tool(s) Shared HPC environments <code>Lmod / modules</code> Python/R + system deps, easy setup <code>conda / mamba</code> Portable containers on HPC <code>Apptainer</code> Custom scientific software stacks <code>Spack</code> Ready-made scientific software (biology) <code>SBGrid</code> Lightweight, local Python projects <code>venv</code> Reproducible R projects <code>renv</code> <p>Lmod/Modules: Exposes pre-installed tools with module load; used on HPCs for easy switching between compilers and software versions.</p> <p>conda/mamba: General-purpose, language-aware environment + dependency manager; mamba is a faster drop-in.</p> <p>Apptainer: Build-and-run container system for non-root, secure app packaging on HPC.</p> <p>Spack: HPC-focused package manager for custom builds, compiler/toolchain control, and dependency isolation.</p> <p>SBGrid: Plug-and-play scientific software stack (mainly structural biology); not customizable.</p> <p>venv: Built-in tool for isolating Python projects.</p> <p>renv: Lightweight and reproducible environment manager for R workflows.</p> <pre><code>graph TD;\n    A[\"Using pre-installed software via HPC?\"] --&gt;|Yes| Lmod[\"Lmod / Environment Modules\"]\n    A --&gt;|No| B[\"Need your own Python, R, or native stack?\"]\n\n    B --&gt;|Yes| C[Use conda / mamba]\n    C --&gt; D[Need portable, reproducible OS-level environment?]\n    D --&gt;|Yes| Apptainer[\"Use Apptainer (containers)\"]\n    D --&gt;|No| E[Need custom toolchains or builds?]\n    E --&gt;|Yes| Spack[\"Use Spack (custom builds)\"]\n    E --&gt;|No| F[Working in Python or R?]\n\n    F --&gt;|R| Renv[\"Use renv (R projects)\"]\n    F --&gt;|Python| G[Pure Python project?]\n    G --&gt;|Yes| Venv[Use venv]\n    G --&gt;|No| C\n\n    Lmod --&gt; Done1[\u2714 Done]\n    C --&gt; Done2[\u2714 Done]\n    Apptainer --&gt; Done3[\u2714 Done]\n    Spack --&gt; Done4[\u2714 Done]\n    Renv --&gt; Done5[\u2714 Done]\n    Venv --&gt; Done6[\u2714 Done]\n</code></pre>"},{"location":"getting_started/software/software_overview/#software-environments","title":"Software Environments","text":""},{"location":"getting_started/software/software_overview/#containers","title":"Containers","text":"<p>Containerization is a technology that allows you to package an application together with everything it needs to run \u2014 code, libraries, dependencies, and environment settings \u2014 into a single, portable unit called a container.</p> <p>Think of a container like a lightweight, standalone mini-computer that runs consistently anywhere \u2014 on your laptop, on a server, or on a supercomputer \u2014 regardless of the underlying system.</p>"},{"location":"getting_started/software/software_overview/#apptainer","title":"Apptainer","text":"<p>What is Apptainer?</p> <p>Apptainer (previously called Singularity) is a container platform designed specifically for High-Performance Computing (HPC) environments.</p> <p>Why Apptainer over Docker in HPC? - Docker needs root/admin access \u2192 Not allowed on most HPC clusters. - Apptainer is designed to run containers as non-root \u2192 Safe for shared systems.</p> <p>Apptainer integrates better with HPC tools (like MPI, GPUs, file systems).</p> <p>What is Apptainer Good For? - Packaging scientific software &amp; dependencies. - Running complex workflows reproducibly. - Sharing pre-configured environments. - Moving workloads between different systems easily (laptop \u2192 HPC \u2192 cloud).</p> <p>For more information see the Apptainer page</p>"},{"location":"getting_started/software/software_overview/#environments","title":"Environments","text":""},{"location":"getting_started/software/software_overview/#lmodmodules","title":"LMOD/Modules","text":"<p>Lmod (Lua Modules) is an environment module system commonly used on High-Performance Computing (HPC) systems, clusters, and supercomputers. It helps users easily manage and switch between different software environments.</p> <p>For more information see the Modules (LMOD) page</p>"},{"location":"getting_started/software/software_overview/#condamamba","title":"Conda/Mamba","text":"<p>Conda/mamba is an open-source package management and environment management system. It was developed to simplify installing, running, and managing software packages and their dependencies, especially for data science, machine learning, and scientific computing.</p> <ul> <li>Environments can be created locally and recreated on the research cluster</li> <li>Environments can be created per project</li> <li>Conda/Mamba manage software dependencies for reproducible research.</li> <li>Install non-Python libraries (like TensorFlow, OpenCV, etc.) easily.<ul> <li>bioconda is a great source of bioinformatic tools</li> </ul> </li> </ul> <p>For more information see the Conda/Mamba page</p>"},{"location":"getting_started/software/software_overview/#spack","title":"Spack","text":"<p>Spack is a flexible, open-source package manager designed specifically for supercomputers, HPC clusters, and scientific computing. It helps users build, install, and manage multiple versions of scientific software and their complex dependencies.</p> <p>For more information see the Spack page</p>"},{"location":"getting_started/software/software_overview/#sbgrid","title":"SBGrid","text":"<p>SBGrid is a specialized software distribution and management system designed primarily for the structural biology community. It provides a curated collection of scientific software used in fields like:</p> <ul> <li>X-ray crystallography</li> <li>Cryo-electron microscopy (Cryo-EM)</li> <li>NMR spectroscopy</li> <li>Molecular modeling and visualization</li> </ul> <p>For more information see the SBGrid page</p>"},{"location":"getting_started/software/software_overview/#venv","title":"venv","text":"<p>venv is a built-in Python tool (since Python 3.3) used to create virtual environments. A virtual environment is like a self-contained Python workspace \u2014 it has its own Python interpreter and its own set of installed packages, completely separate from the system Python.</p> <p>Why is venv Useful?</p> <p>When working on multiple Python projects:</p> <ul> <li>Different projects might need different versions of the same package.</li> <li>Installing everything globally could lead to version conflicts.</li> </ul> <p>venv solves this by isolating environments.</p> <p>What is venv Good For?</p> <ul> <li>Keeping project dependencies separate.</li> <li>Avoiding conflicts between package versions.</li> <li>Preventing changes to system-wide Python packages.</li> <li>Making projects easier to share &amp; reproduce.</li> </ul> <p>For more information see the venv page</p>"},{"location":"getting_started/software/software_overview/#renv","title":"renv","text":"<p><code>{renv}</code> is an R library that is used to create virtual environments of R packages. These environments can be at the user or project level and help to isolate the packages used for a project from the versions installed at a system level.</p> <p>renv is useful for when you want to ensure your package versions are controlled by you, or if you want to ensure the same package versions are used across multiple devices or collaborators.</p> <p>What is renv Good For?</p> <ul> <li>Keeping project dependencies separate.</li> <li>Preventing changes to system-wide R packages.</li> <li>Making projects easier to share &amp; reproduce.</li> </ul> <p>For more information see the renv page</p>"},{"location":"getting_started/software/onDemand/available_apps/","title":"Available Apps","text":""},{"location":"getting_started/software/onDemand/available_apps/#blender","title":"Blender","text":"<p>Blender is a free and open source 3D creation suite.</p> <p>The Blender GUI can be accessed via the Open OnDemand Applications. This requires 3D acceleration for visualisation and rendering so needs to be run on a GPU partition.</p>"},{"location":"getting_started/software/onDemand/available_apps/#chimerax","title":"ChimeraX","text":"<p>The ChimeraX GUI can be accessed via the Open OnDemand Applications. This requires 3D acceleration for visualisation so needs to be run on a GPU VirtualGL partition.</p>"},{"location":"getting_started/software/onDemand/available_apps/#clc-genomics-workbench","title":"CLC genomics Workbench","text":"<p>QIAGEN CLC genomics Workbench is a user-friendly sequence  analysis platform for genomics, epigenomics and metagenomics.</p> <p>Warning</p> <p>CLC Genomics Workbench is licensed software. The Research Cluster currently has a small number of floating licenses available for trial purposes, facilitated by <code>Dr Sunali Mehta &lt;mailto:sunali.mehta@otago.ac.nz&gt;_</code>, Pathology dept. Please ensure you are authorised to consume a license.</p> <p>The CLC genomics Workbench GUI can be accessed via the Open OnDemand Applications.</p> <p>Optionally tick the '3D hardware-accelerated rendering option and select a GPU partition if using the 3D viewers.</p>"},{"location":"getting_started/software/onDemand/available_apps/#ecoassist","title":"EcoAssist","text":"<p>The EcoAssist GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"getting_started/software/onDemand/available_apps/#epi2me-desktop","title":"EPI2ME Desktop","text":"<p>Oxford Nanopore's EPI2ME Desktop is a data analysis platform providing a user-friendly graphical interface to running various bioinformatics pipelines.</p> <p>Note</p> <p>Apart from the list of ONT workflows prepopulated in the <code>Available Workflows</code> tab, EPI2ME Desktop allows for the importation of other generic Nextflow workflows,  such as the 100+ curated pipelines of []nf-core](https://nf-co.re/pipelines).</p> <p>Under Workflows, click <code>Import workflow</code> and copy-paste the workflow's git repository URL. (i.e. <code>https://github.com/nf-core/&lt;wf&gt;</code>)</p> <p>The EPI2ME GUI can be accessed via the Open OnDemand Applications.</p> <ul> <li> <p>Individual pipeline tasks will be sent to the Slurm cluster scheduler and scheduled as separate jobs with their own allocated resources (as per the workflow defaults, or as specified in the <code>Nextflow configuration</code> tab).</p> </li> <li> <p>For pipelines requiring GPU compute, there is no need to run the EPI2ME OOD app on a GPU partition -In fact, the OOD app form doesn't give you that option-; Individual tasks requiring GPUs will be automatically scheduled on GPU-capable cluster nodes.</p> </li> <li> <p>In the EPI2ME Launch Wizard, there is also no need to change the Profile setting (in <code>Nextflow configuration</code>); This instance has been patched to default to the <code>singularity</code> profile (which uses apptainer).</p> </li> </ul> <p>To accomplish this, running the OnDemand app for the first time will automatically create the global nextflow config <code>$HOME/.nextflow/config</code> to support running the tasks via the cluster scheduler and make use of the GPU partition when appropriate. However if this file already exists, you may need to manually add this functionality:</p> <p>Terminal</p> <pre><code>  process {\n    executor = 'slurm'\n    time = 6.h\n    withLabel: 'gpu' {\n      queue = 'aoraki_gpu'\n    }\n  }\n</code></pre> <p>Warning</p> <p>EPI2ME Desktop isn't designed with HPC cluster use in mind. As such we have had to devise a number of workarounds to make this work and integrate with the cluster scheduler.  While initial testing has been promising, the added complexity of running EPI2ME Desktop this way, as well as the application's general lack of (Nextflow) configurability may introduce  hard-to-troubleshoot issues.</p> <p>Consider running the workflows directly from the commandline using Nextflow instead.</p>"},{"location":"getting_started/software/onDemand/available_apps/#fiji","title":"Fiji","text":"<p>Fiji is an image processing package \u2014 a \"batteries-included\" distribution of ImageJ, bundling many plugins which facilitate scientific image analysis. .</p> <p>The Fiji GUI is available as an Open OnDemand app.</p>"},{"location":"getting_started/software/onDemand/available_apps/#flexpde","title":"FlexPDE","text":"<p>The FlexPDE GUI can be accessed via the Open OnDemand Applications.</p> <p>The FlexPDE Lite (evaluation) configuration is free to use but wil be restricted in the number of simultaneous equations and mesh cells.</p> <p>FlexPDE Professional can be Internet-activated with an appropriate serial number. Do note that the license activation is machine-based so will need to be deactivated and reactivated on subsequent runs unless the same node is selected.## GRASS GIS</p> <p>The GRASS GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"getting_started/software/onDemand/available_apps/#ilastik","title":"ilastik","text":"<p>ilastik is a user-friendly open-source tool for interactive image classification, segmentation and analysis leveraging machine learning algorithms.</p>"},{"location":"getting_started/software/onDemand/available_apps/#via-ondemand","title":"via OnDemand","text":"<p>The ilastik GUI is available as an Open OnDemand app.</p>"},{"location":"getting_started/software/onDemand/available_apps/#jupyter-lab","title":"Jupyter Lab","text":"<p>Jupyter Lab can be accessed via the Open OnDemand applications.</p> <p>This app features a number of domain-focused or application-specific variants. These containerised software stacks are immutable which promotes consistency and reproducibility.</p> <p>The different environments are based on the Jupyter Docker Stacks images, maintained by the Jupyter team.</p> <p>Contact us if you require a customised environment.</p>"},{"location":"getting_started/software/onDemand/available_apps/#kilosort4","title":"Kilosort4","text":"<p>Kilosort4 is a tool for clustering spikes from multi-channel electrophysiological recordings.</p> <p>https://kilosort.readthedocs.io</p>"},{"location":"getting_started/software/onDemand/available_apps/#via-ondemand_1","title":"via OnDemand","text":"<p>The Kilosort4 GUI is available as an Open OnDemand app.</p>"},{"location":"getting_started/software/onDemand/available_apps/#melts","title":"MELTS","text":"<p>Legacy software for thermodynamic modeling of phase equilibria in magmatic systems experimental</p> <p>\ud83d\udd17 https://melts.ofm-research.org</p> <p>The MELTS GUI can be accessed via the Open OnDemand Applications.</p> <p>The launcher allows for the selection of the different versions/models;</p> <ul> <li>rhyolite-MELTS v. 1.0.2 (original version, with corrections) - old H2O model, no mixed fluids.</li> <li>rhyolite-MELTS v, 1.1.0 (mixed fluid version that perserves the ternary minimum) - old H2O model.</li> <li>rhyolite-MELTS v. 1.2.0 (mixed fluid version optimal for mafic and alkalic melts) - new H2O model.</li> <li>pMELTS v. 5.6.1 (original version, with corrections) - - old H2O model, no mixed fluids.</li> </ul>"},{"location":"getting_started/software/onDemand/available_apps/#netlogo","title":"NetLogo","text":"<p>The NetLogo GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"getting_started/software/onDemand/available_apps/#qgis","title":"QGIS","text":"<p>The QGIS GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"getting_started/software/onDemand/available_apps/#relion","title":"RELION","text":"<p>The RELION GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"getting_started/software/onDemand/available_apps/#rstudio-server","title":"RStudio Server","text":"<p>The RStudio Server web UI can be accessed via the Open OnDemand Applications.</p>"},{"location":"getting_started/software/onDemand/available_apps/#saga-gis","title":"SAGA GIS","text":"<p>The SAGA GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"getting_started/software/onDemand/available_apps/#satscan","title":"SaTScan","text":"<p>The SaTScan GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"getting_started/software/onDemand/available_apps/#specify","title":"Specify","text":"<p>The Spcify 6 GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"getting_started/software/onDemand/available_apps/#stata","title":"Stata","text":"<p>The Stata GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"getting_started/software/onDemand/available_apps/#ugene","title":"UGENE","text":"<p>https://ugene.net</p> <p>UGENE is a GUI for DNA and protein sequence visualization, alignment, assembly and annotation. It integrates dozens of well-known biological tools,  algorithms, and original tools in the context of genomics, evolutionary biology, virology, and other branches of life science.</p>"},{"location":"getting_started/software/onDemand/available_apps/#gui","title":"GUI","text":"<p>The UGENE GUI can be accessed via the Open OnDemand Applications. https://ondemand.otago.ac.nz/pun/sys/dashboard/batch_connect/sys/ood_ugene_apptainer</p> <ul> <li>hardware-accelerated 3D visualisation can be enabled to improve 3D visualisation (e.g. the 3D viewer)</li> <li>OpenCL can be enabled to improve performance of a few select algorithms (Smith-Waterman, UGENE Genome Aligner)</li> </ul>"},{"location":"getting_started/software/onDemand/available_apps/#vscodium","title":"VSCodium","text":"<p>VSCodium is a community-driven, freely-licensed distribution of Microsoft's editor VSCode.</p> <p>The VSCodium GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"getting_started/software/onDemand/hpc_desktop/","title":"OnDemand HPC Desktop","text":"<p>SMB Local Mount from HPC Desktop GUI </p> <ol> <li>Start an Otago HPC Desktop </li> <li>Open a Terminal window on the desktop</li> <li>Type \"kdestroy\" to remove invalid older tickets</li> <li>Type \"kinit\" and Enter your password </li> <li>Open File browser window  </li> <li>Connect and login to HCS by entering the smb://username@storage.hcs-p01.otago.ac.nz/share-name address  </li> <li>When the authentication window appears type in the domain \"registry\" if staff or \"student\" if you are using a student account, and your password  </li> <li>Press connect and wait a few seconds for authentication and your HCS files to appear in the window.  </li> </ol> <p>Figure 1: Connecting to HCS - create kerberos ticket with kinit</p> <p>Figure 2: Connect to HCS - authenticating using samba in the file browser</p> <p>Figure 3: Connect to HCS - browsing hcs files in file browser</p>"},{"location":"getting_started/software/onDemand/ondemand/","title":"Open OnDemand","text":"<p>Open OnDemand is a web portal that lets you launch applications, access files, and interact with the cluster\u2014all from your browser, with no software installation needed. When you start an application or session, Open OnDemand submits a Slurm job for you. These are called \"interactive sessions.\" You can also use the portal to view files, check job status, and open a shell.</p> <p>Key points:</p> <ul> <li>Interactive sessions are Slurm jobs managed for you by Open OnDemand.</li> <li>Sessions will stay running even if you close your browser; you can see and manage them under \"My Interactive Sessions.\"</li> <li>To end a session, click \"Delete\" next to it.</li> </ul> <p>Ways to monitor usage:</p> <ul> <li>Use the Open OnDemand dashboard to see your running and recent sessions.</li> <li>Check Jobs &gt; Active Jobs for all jobs (including those started by sbatch, srun, or Open OnDemand).</li> <li>Use standard Slurm commands (like <code>squeue</code>, <code>sacct</code>) in a shell session for more details.</li> </ul>"},{"location":"getting_started/software/onDemand/ondemand/#using-open-ondemand","title":"Using Open OnDemand","text":"<p>Open OnDemand provides several services:</p>"},{"location":"getting_started/software/onDemand/ondemand/#files-app","title":"Files App","text":"<p>Access your files from the top menu: Files &gt; Home Directory. You can:</p> <ul> <li>Create, delete, and move files and folders.</li> <li>Upload and download files.</li> <li>(Section on HCS and Globus to be added.)</li> </ul> <p>After logging in, you'll see the Open OnDemand home page.</p> <p>Figure 1: Open OnDemand Files App</p>"},{"location":"getting_started/software/onDemand/ondemand/#view-active-jobs","title":"View Active Jobs","text":"<p>See and cancel your Slurm jobs from Jobs &gt; Active Jobs. This includes jobs started via <code>sbatch</code>, <code>srun</code>, and Open OnDemand.</p> <p>Figure 2: Active Jobs</p>"},{"location":"getting_started/software/onDemand/ondemand/#shell-access","title":"Shell Access","text":"<p>Get command-line access to the cluster from the top menu bar options: Clusters &gt; Aoraki Cluster Shell Access.</p> <p>Figure 3: Open OnDemand Shell</p>"},{"location":"getting_started/software/onDemand/ondemand/#interactive-apps","title":"Interactive Apps","text":"<p>Launch interactive applications from the Interactive Apps menu:</p> <ul> <li>Desktop App: For GUI-based programs.</li> <li>Jupyter Server: For Jupyter notebooks.</li> <li>RStudio Server: For RStudio sessions.</li> </ul> <p>Figure 4: Open OnDemand Files App</p>"},{"location":"getting_started/software/onDemand/ondemand/#desktop-app","title":"Desktop App","text":"<p>The OOD Desktop App allows you to run programs that require graphical user interfaces (GUIs) on the Research Cluster</p> <p>Intended Usage</p> <p>When possible, you should carry out your computation via the traditional command line plus SLURM functionality. OOD Desktop is intended for use for programs that require GUIs. Furthermore, if you need to use Jupyter notebooks, RStudio, or the MATLAB GUI, we provide specialized interactive apps that you should use instead of the OOD Desktop App.</p> <p>Before getting started, make sure you have access to the Research Cluster (by contacting RTIS).</p> <p>Fill out the form presented to you and then press \"Launch\". (Note, as of this time, that the only partition that the Desktop app can be launched on when computing via Slurm is otago1, as we assume that most GUI usage would be for programs using one or a small number of cores). After a moment, the Desktop session will be initialized and allow you to specify the image compression and quality options. If you are unhappy with the default values, you can relaunch the session from this page with different choices. Then, press \"Launch Desktop\" and the Desktop will open in a new tab.</p>"},{"location":"getting_started/software/onDemand/ondemand/#interacting-with-files","title":"Interacting with Files","text":"<p>Your Desktop session is running directly on the Research Cluster, and can interact with your files either through the command line as usual or through Desktop the file manager.</p> <p>To open a command line terminal, right click anywhere on the Desktop and select \"Open Terminal Here\".</p>"},{"location":"getting_started/software/onDemand/ondemand/#using-otago-hcs-data","title":"Using Otago HCS Data","text":"<ol> <li>Connect to the HCS Share</li> <li>Copy your data to your projects directory</li> <li>Process your data with the cluster</li> <li>Copy your results back to the HCS Share </li> </ol> <p>Note: Connecting to Otago HCS is intended for copying data to the Research Clutster for processing. It is not intended for data processing as the speeds and accessibility are not suited to cluster computing.</p>"},{"location":"getting_started/software/onDemand/ondemand/#connecting-to-otago-hcs-shares","title":"Connecting to Otago HCS Shares","text":"<p>AutoFS from the commandline </p> <p>This will mount your HCS share on the local machine and allow you to access and sync files. Note that this is not high-speed access and handling large files may be slow. </p> <ol> <li>Take note of your HCS share directory name, the part after <code>//storage.hcs-p01.otago.ac.nz/ **&lt;yourshare&gt;**</code> </li> <li>Login to a commandline shell session </li> <li>Type \"kdestroy\" to remove invalid older tickets</li> <li>Type \"kinit\" and Enter your password  </li> <li>List or navigate to yoru directory <code>/mnt/auto-hcs/*&lt;yourshare&gt;*</code> </li> <li>sync your files to your projects directory eg. <code>rsync -avz /mnt/auto-hcs/its-rtis/testfile /projects/rtis/higje06p/</code> </li> <li>use your files to process on the compute cluster   </li> </ol>"},{"location":"getting_started/software/onDemand/ondemand/#copy-your-hcs-data-to-your-project-directory","title":"Copy your HCS data to your project directory","text":"<ol> <li> <p>Naviagate to your hcs data and copy it to your user projecy directory</p> <p>Figure 5: Connect to HCS</p> </li> <li> <p>When you have finished processing copy your data back to your HCS Share.</p> </li> </ol>"},{"location":"getting_started/software/onDemand/ood_file_manager/","title":"Using the Open OnDemand File Manager","text":"<p>The Open OnDemand File Manager provides a simple way to manage your files on the cluster directly from your web browser.</p>"},{"location":"getting_started/software/onDemand/ood_file_manager/#accessing-the-file-manager","title":"Accessing the File Manager","text":"<ol> <li>Log in to the Open OnDemand portal using your web browser.</li> <li>From the top menu, select Files &gt; Home Directory (or another available directory).</li> </ol>"},{"location":"getting_started/software/onDemand/ood_file_manager/#main-features","title":"Main Features","text":"<ul> <li>Browse Directories: Navigate through your home directory and other accessible storage locations.</li> <li>Upload/Download Files: Click the \"Upload\" button to add files from your local computer, or select files and click \"Download\" to save them to your computer.</li> <li>Create and Delete: Use the interface to create new folders or files, and to delete items you no longer need.</li> <li>Rename and Move: Right-click on files or folders to rename or move them.</li> <li>Edit Files: Click on a text file to open it in the built-in editor for quick changes.</li> </ul>"},{"location":"getting_started/software/onDemand/ood_file_manager/#tips","title":"Tips","text":"<ul> <li>Use the file manager to quickly transfer data between your computer and the cluster.</li> <li>For large files or bulk transfers, consider using command-line tools (like <code>rsync</code> or <code>scp</code>) for better performance. </li> <li>Always organize your files in your home or project directories for easier access and management.</li> </ul> <p>The Open OnDemand File Manager is a convenient tool for everyday file operations and is especially helpful for new users or those who prefer a graphical interface.</p>"},{"location":"getting_started/software/onDemand/ood_shell/","title":"Accessing the Shell through Open OnDemand","text":"<p>Open OnDemand provides a convenient way to access a shell on the cluster directly from your web browser, without needing to install or configure an SSH client.</p>"},{"location":"getting_started/software/onDemand/ood_shell/#how-to-open-a-shell-session","title":"How to Open a Shell Session","text":"<ol> <li>Log in to the Open OnDemand portal using your web browser.</li> <li>In the top menu, go to Clusters &gt; Aoraki Cluster Shell Access (or the relevant cluster name).</li> <li>A new tab or window will open with a terminal session connected to the cluster.</li> </ol>"},{"location":"getting_started/software/onDemand/ood_shell/#what-you-can-do","title":"What You Can Do","text":"<ul> <li>Run command-line programs and scripts.</li> <li>Submit and monitor Slurm jobs using commands like <code>sbatch</code>, <code>squeue</code>, and <code>sacct</code>.</li> <li>Navigate and manage your files using standard Linux commands.</li> <li>Use text editors such as <code>nano</code>, <code>vim</code>, or <code>emacs</code>.</li> </ul>"},{"location":"getting_started/software/onDemand/ood_shell/#tips","title":"Tips","text":"<ul> <li>The shell session runs on a login node, so avoid running heavy computations directly in this terminal. Start an OnDemand HPC Desktop session, or use Slurm to submit compute jobs.</li> <li>You can open multiple shell sessions if needed.</li> <li>If you are disconnected, simply reconnect through the Open OnDemand portal.</li> </ul> <p>The Open OnDemand shell is a quick and user-friendly way to interact with the cluster for everyday tasks.</p>"},{"location":"getting_started/software/software_environments/sbgrid/","title":"SBGrid","text":"<p>SBGrid offers a comprehensive package of scientific software that is regularly updated and available on the cluster.  It includes both module files and SBGrid's proprietary software loading utility.  Currently, the suite comprises 543 applications, with multiple versions of each to accommodate the diverse needs of users.  The SBGrid Consortium, which operates out of Harvard Medical School, is a research computing group funded by member research laboratories.  It provides essential computing support to the global structural biology community.</p> <p>Note: SBGrid users MUST be registered with the SBGrid Consortium to use this software package.</p> <ol> <li>To sign up for SBGrid access, simply go to the SBGrid Registration Page.</li> <li>Take note of the SBGrid License Agreement.</li> <li>Fill out the registration form.</li> <li>Take a screenshot of your completed registration acknowledgement.</li> <li>Send the screenshot to rtis.solutions.team@otago.ac.nz. </li> <li>This process must be done for each lab.</li> </ol> <p>SBGrid Information WIKI</p>"},{"location":"getting_started/software/software_environments/sbgrid/#sbgrid-on-the-research-cluster","title":"SBGrid on the Research Cluster","text":""},{"location":"getting_started/software/software_environments/sbgrid/#modules","title":"Modules","text":"<p>To use SBGrid via modules, use the following command <code>export MODULEPATH=/programs/share/modulefiles/x86_64-linux:\"$MODULEPATH\"</code> </p> <p>Note</p> <p>To make this permanent, add the above command to your <code>.bashrc</code></p> <p>Module Examples: Once the modulepath is loaded as above <code>module avail</code> will list all modules including SBGrid, Spack, and RTIS custom modules </p> <p><code>module spider alphafold</code> will search for all module packages include the SBGrid packages</p> <p>Terminal</p> <pre><code>module spider alphafold\n</code></pre> <pre><code>sbgrid/alphafold:\n Versions:\n    sbgrid/alphafold/2.1.2\n    sbgrid/alphafold/2.2.0\n    sbgrid/alphafold/2.2.2\n    sbgrid/alphafold/2.2.3\n    sbgrid/alphafold/2.2.4\n    sbgrid/alphafold/2.3.0\n    sbgrid/alphafold/2.3.1\n    sbgrid/alphafold/2.3.2_20241024\n    sbgrid/alphafold/2.3.2\n    sbgrid/alphafold/3.0.0_20241202_aa724ca\n    sbgrid/alphafold/3.0.0\n</code></pre> <p>To load the SBGrid software simply type <code>module load sbgrid/alphafold</code>. This will load the default version of the software.</p> <p>To load a specific version use <code>module load sbgrid/alphafold/2.1.2</code></p> <p>To show which versions are loaded <code>module list</code></p>"},{"location":"getting_started/software/software_environments/sbgrid/#sbgrid-tools","title":"SBgrid Tools","text":"<p>To use SBGrid Tools, execute the following command <code>source /programs/sbgrid.shrc</code> </p> <p>Note</p> <p>To make this permanent, add the above command to your <code>.bashrc</code></p> <p>To load SBGrid software using the SBGrid tools instead of modules see SBGrid Getting Started </p> <p><code>source /programs/sbgrid.shrc</code></p> <p>Once you are in the SBGrid environment you can use all SBGrid commands.</p> <p>https://sbgrid.org/wiki/examples</p> <p>SBGrid youtube</p>"},{"location":"getting_started/software/software_environments/containers/afni/","title":"AFNI (Analysis of Functional NeuroImages)","text":"<p>https://afni.nimh.nih.gov</p> <p>For customising the behaviour of AFNI tools with a <code>~/.afnirc</code> config or via environment variables, see https://afni.nimh.nih.gov/pub/dist/doc/program_help/README.environment.html</p>"},{"location":"getting_started/software/software_environments/containers/afni/#gui","title":"GUI","text":"<p>The AFNI GUI can be accessed via the Open OnDemand Applications - https://ondemand.otago.ac.nz/pun/sys/dashboard/batch_connect/sys/ood_afni_apptainer.</p>"},{"location":"getting_started/software/software_environments/containers/afni/#command-line","title":"Command line","text":"<p>The AFNI suite is made available on the cluster as a shared Apptainer container image.  Binaries and scripts are run within the context of the container.</p> <p>You can use the <code>apptainer/AFNI</code> module to add convenient aliases to running any of the AFNI binaries and scripts within the container:</p> <p>Terminnal</p> <p>module avail afni module load apptainer/AFNI</p> <p>Alternatively run with apptainer directly; i.e. to run binaries within the container, prefix any command with <code>apptainer run $APPTAINER_IMG/&lt;apptainer_image.sif&gt;</code>. </p> <p>To get an interactive shell into the container:</p> <p>Terminal</p> <p>apptainer run $APPTAINER_IMG/afni_24.2.03.sif /bin/bash</p> <p>Within the container, you'll find the AFNI binaries and scripts in <code>/opt/afni/install</code></p> <p>These can also be invoked directly within the context of the container, with:</p> <p>Terminal</p> <p>apptainer run $APPTAINER_IMG/afni_24.2.03.sif  </p>"},{"location":"getting_started/software/software_environments/containers/afni/#the-following-is-required-to-use-aliases-in-a-non-interactiveslurm-batch-script","title":"The following is required to use aliases in a non-interactive/SLURM batch script:","text":"<p>shopt -s expand_aliases afni......</p>"},{"location":"getting_started/software/software_environments/containers/alphafold/","title":"AlphaFold","text":"<p>AlphaFold is an AI system developed by DeepMind that makes  state-of-the-art accurate predictions of a protein's structure from its amino-acid sequence.</p> <p>AlphaFold is made available on the cluster as a shared Apptainer container image.  This should be run on a GPU Compute partition.</p> <p>You can use the <code>apptainer/alphafold2</code> module to add a convenient alias: <code>run_alphafold_apptainer</code>, which will run  the <code>run_alphafold.py</code> within the container; The alias will also bind-mount the AlphaFold database base path (<code>$AF2DB</code>, which is set to <code>/opt/alphafold_databases/</code>) into the container on <code>/db</code>.</p> <p>To use the <code>run_alphafold_apptainer</code> alias in a non-interactive/SLURM batch script, add the following in your script before using the alias:</p> <p>Terminal</p> <pre><code>shopt -s expand_aliases\n</code></pre> <p>See the AlphaFold documentation for usage information. Example:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH ....\n#SBATCH ....\nmodule load apptainer/alphafold2\nshopt -s expand_aliases\n\nINPUT=/home/doeja01p/alphafold_test/in\nOUTPUT=/home/doeja01p/alphafold_test/out\n\nrun_alphafold_apptainer \\\n--use_gpu_relax \\\n--fasta_paths=${INPUT}/T1050.fasta \\\n--output_dir=$OUTPUT \\\n--max_template_date=2020-05-14 \\\n--model_preset=monomer_casp14 \\\n--benchmark \\\n--data_dir=/db \\\n--uniref90_database_path=/db/uniref90/uniref90.fasta \\\n--mgnify_database_path=/db/mgnify/mgy_clusters_2022_05.fa \\\n--template_mmcif_dir=/db/pdb_mmcif/mmcif_files \\\n--obsolete_pdbs_path=/db/pdb_mmcif/obsolete.dat \\\n--bfd_database_path=/db/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--uniref30_database_path=/db/uniref30/UniRef30_2021_03 \\\n--pdb70_database_path=/db/pdb70/pdb70\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/ansys_fluent/","title":"Ansys Fluent","text":"<p>Ansys Fluent is proprietary fluid simulation software.</p> <p>These containers can be run on both GPU and CPU partitions. There is about a 20-30% speedup with using a GPU. They can be operated interactively (either through a text user interface or a graphical user interface) once you have logged into a node (with <code>srun</code>) or in \"batch mode\" where the container can be run through the job scheduler.</p> <p>The <code>ansys_fluent_2022r2.sif</code> container contains a full Linux installation of ANSYS Fluent 2022R2. It is designed to be operated from within the cluster.  It does not accept network commands from the <code>Parallel Settings</code> or <code>Remote</code> section of the Fluent Launcher running from the outside of the cluster.</p> <p>Versioning</p> <p>The Fluent 2022R2 container can only open Workbench and Fluent files that are created in version 2022R2 and earlier.</p>"},{"location":"getting_started/software/software_environments/containers/ansys_fluent/#interactive-mode","title":"Interactive mode","text":"<p>The container is run in <code>Interactive mode</code> by running the container with no arguments.</p> <p>The supported version (2022R2) can be run on the cluster with:</p> <p>Terminal</p> <pre><code>apptainer run /opt/apptainer_img/ansys_fluent_2022r2.sif\n</code></pre> <p>This will load into an Ubuntu Linux Bash shell environment where the user can start to issue commands:</p> <p>Terminal</p> <pre><code>[user@aorakiXX ~]$ apptainer run /opt/apptainer_img/ansys_fluent_2022r2.sif\n</code></pre> <pre><code>## ANSYS Fluent 2022R2 Container ##\n\nFor debugging information export the following before running ANSYS programs:\n   export ANSYS_FRAMEWORK_DEVELOPMENT=1\n   export WBTracing=true\n\nExample commands:\n  runwb2                        # Fluent workbench in graphical mode \n  runwb2 -B                     # Fluent workbench in text mode (enters into an IronPython shell)\n  runwb2 -B -R file_name.wbjn   # Fluent workbench in text mode and run the Python 2.7 commands in file_name.wbjn\n  fluent                        # Run Fluent in graphical mode\n\nPress [CTRL] + [C] to cancel a program which is currently running.\nPress [CTRL] + [D] to exit this container instance.\n\nANSFlu user@aorakiXX:~$\n</code></pre> <p>A convience script called <code>fluent-2022r2.sh</code> has been written that simplifies loading the container.  This and other files (SLURM examples) can be extracted from the container by running the following in an empty directory:</p> <p>Terminal</p> <pre><code>apptainer run /opt/apptainer_img/ansys_fluent_2022r2.sif --copy-execute-files\n</code></pre> <p>I recommend that you put <code>fluent-2022r2.sh</code> in a directory that has been added to your <code>PATH</code> so you can call it regardless of where you are in the directory tree. </p>"},{"location":"getting_started/software/software_environments/containers/ansys_fluent/#graphical-mode","title":"Graphical mode","text":"<p>In order to use Fluent through a graphical user interface you need to be able to display X11 programs (through WSL on Windows and XQuartz on Mac) on your local machine.  This requires a bit more setup to function correctly. If you are having issues with this please contact RTIS for support.</p> <p>Note that 3D software rendering is used which is alot slower than direct hardware acceleration that would be on somebodies local machine. This means that the graphical  interface is useful for initial setup and validation (confirming that it works) rather than for design and analysis.</p> <p>To start the Fluent workbench you have to enter into the container (as mentioned above) and then type:</p> <p>Terminal</p> <pre><code>runwb2\n</code></pre> <p>To start Fluent directly type:</p> <p>Terminal</p> <pre><code>fluent\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/ansys_fluent/#text-mode","title":"Text mode","text":"<p>The text mode is useful for running or developing journal files for automation purposes (traditionally ending in <code>.wbjn</code>). These are Python 2.7 scripts  that the ANSYS IronPython runtime uses to open, initialize, initiate, save and close a simulation.</p> <p>You are free to use the code developed for 'Batch mode' (see below) by having a look at the script found (within the container) at:</p> <p>Terminal</p> <pre><code>/run-files/runwb2-wbprj.py\n</code></pre> <p>The Python journal files can also be generated though the graphical ANSYS Workbench. This is done by enabling journalling that records your actions in the GUI. You can then play this back in text mode. To do this (in the Workbench) goto <code>File =&gt; Scripting =&gt; Record Journal...</code>. The workbench will then ask you for the journal file name and start recording your actions. This <code>.wbjn</code> file can be modified with a text editor. The workbench automatically records your actions when  you enable automatic journalling. To enable this goto <code>Tools =&gt; Options =&gt; Journals and Logs (side menu)</code> and check \"Record Journal File\". You can also set the  directory here as well.</p> <p>To run or 'replay' the script/journal you run the following within the container:</p> <p>Terminal</p> <pre><code># Journal file that was recorded through the GUI:\nrunwb2 -B -R file_name.wbjn\n\n# or:\nrunwb2 -B -R /run-files/runwb2-wbprj.py\n</code></pre> <p>In the above examples, the <code>-B</code> operates the workbench in batch mode (text mode), i.e it does not load up the GUI. The <code>-R</code> runs or replays the  script/journal file that is given (in this case: <code>file_name.wbjn</code> and <code>/run-files/runwb2-wbprj.py</code>).</p> <p>There is an open source project on github that also provides another example using design points (parameterization).  This can be found here: https://github.com/sikvelsigma/ANSYS-WB-Batch-Script.</p>"},{"location":"getting_started/software/software_environments/containers/ansys_fluent/#batch-mode","title":"Batch mode","text":"<p>The container can be run in automated <code>Batch mode</code> where it runs in text mode only (it does not display a GUI).  You give it the path to the workbench file <code>.wbpj</code> as a single parameter and it initalizes and  runs the solvers within the provided workbench file. It exits after it finishes its simulation(s).</p> <p>The steps that the container goes through are:</p> <ol> <li>Searches the directory that the <code>.wbpj</code> file is in and then opens it.</li> <li>Creates a list of Solution objects that have solvers.</li> <li>For each object with a solver it initalizes it with the chosen method (see below).</li> <li>Once the solver has been initalized any prior simulation data is deleted/cleaned and the simulation is started.</li> <li>Once the simulation has completed the simulation is saved and the container exits.</li> </ol>"},{"location":"getting_started/software/software_environments/containers/ansys_fluent/#environmental-variables","title":"Environmental variables","text":"<p>Environmental variables are used to control the initialization of the model before the simulation is run:</p> <p>ANSFLU_INITIALIZE_METHOD: The <code>ANSFLU_INITIALIZE_METHOD</code> is used to set the inititation method. The default is \"hybrid\".  There is: <code>none</code>, <code>hybrid</code>, <code>standard</code>, <code>custom</code>. The default is <code>hybrid</code>. If <code>custom</code> is set you have to define the  TUI initalization command in the <code>ANSFLU_INITIALIZE_METHOD_CUSTOM</code> environmental variable.</p> <p>For example: </p> <p>Terminal</p> <pre><code>export ANSFLU_INITIALIZE_METHOD=\"hybrid\"\nfluent-2022r2.sh projectdir/model1.wbpj\n\n# or:\nexport ANSFLU_INITIALIZE_METHOD=\"custom\"\nexport ANSFLU_INITIALIZE_METHOD_CUSTOM=\"/solve/initialize/init-flow-statistics\"\n</code></pre> <p>An example of a workbench based simulation run:</p> <p>Terminal</p> <pre><code>[user@aorakiXX ~]$ fluent-2022r2.sh projectdir/model1.wbpj\n</code></pre> <pre><code>WARN: CUDA not found. NVIDIA container will operate without GPU acceleration.\n\n*******************************************\n&gt;&gt; OPENING: model1.wbpj @ 2024-09-05 16:12:54\n\n*******************************************\n&gt;&gt; ENUMERATING 'model1.wbpj'\n\n   SYSTEM OBJECT:\n      UserId:  Geom 3\n      Caption: MX1 Geometry\n      Solver:  []\n\n   SYSTEM OBJECT:\n      UserId:  FLTG 8\n      Caption: 400k cells\n      Solver:  ['FLUENT']\n\n*******************************************\n&gt;&gt; SOLVING SOLUTIONS\n\n   * SYSTEM: FLTG 8\n     &gt; INITIALIZATION\n     &gt; INITIALIZE: Hybrid method...\n     &gt; RUNNING\n\n*******************************************\n&gt;&gt; COMPLETED\n\n   Started:   2024-09-05 16:12:54\n   Completed: 2024-09-05 16:21:10\n   Duration:  0:08:16\n\n*******************************************\n&gt;&gt; SAVING\n\n[user@aorakiXX ~]$\n</code></pre> <p>There can be only a single <code>.wbpj</code> workbench file present in that directory at a time.  No job completion information is given while the simulation(s) are underway.</p> <p>Example SLURM files are given (see above) to submit jobs to the SLURM job manager.</p>"},{"location":"getting_started/software/software_environments/containers/ansys_fluent/#home-directory-quota-constraints","title":"Home directory quota constraints","text":"<p>A quota system is in place on Aoraki that limits the data in a users home directory to 15GB. This can be easy exceeded with CFD simulations with large datasets and multiple runs.  The possible solutions involve moving your data onto another storage medium (Ohau or HCS) and running the container from that directory. </p>"},{"location":"getting_started/software/software_environments/containers/ansys_fluent/#resources","title":"Resources","text":"<p>Ansys Workbench Scripting Guide: https://dl.cfdexperts.net/cfd_resources/Ansys_Documentation/Workbench/Workbench_Scripting_Guide.pdf</p> <p>Fluent TUI commands (text commands within Fluent): https://www.afs.enea.it/project/neptunius/docs/fluent/html/ug/node48.htm</p> <p>Journaling and Scripting within Fluent (basics): https://www.afs.enea.it/project/neptunius/docs/fluent/html/wbug/node45.htm</p> <p>Running Fluent on a different HPC cluster using SLURM: https://www.hkhlr.de/sites/default/files/field_download_file/HKHLR-HowTo-Ansys_Fluent.pdf</p> <p>Starting Parallel Ansys Fluent on a Linux System:  https://ansyshelp.ansys.com/public/account/secured?returnurl=//Views/Secured/corp/v242/en/flu_ug/flu_ug_parallel_start_linux_unix.html</p>"},{"location":"getting_started/software/software_environments/containers/bohra/","title":"Bohra","text":"<p>Bohra is a microbial genomics pipeline.</p> <p>Bohra is made available on the cluster as a shared Apptainer container image. </p> <p>You can use the <code>apptainer/bohra</code> module which will add convenient aliases to the <code>bohra</code> binary, which will run within the container.</p> <p>See the Bohra wiki for usage information.</p> <p>Terminal</p> <pre><code>module load apptainer/bohra\n# The following is required to use aliases in a non-interactive/SLURM batch script:\nshopt -s expand_aliases\nbohra test\nbohra run ......\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/ccp4/","title":"CCP4","text":"<p>The CCP4 (Collaborative Computational Project Number 4) program suite provides an integrated suite of programs for determination of macromolecular structures by X-ray crystallography.</p> <p>The ccp4i2 GUI can be accessed via the Open OnDemand Applications.</p> <p>CCP4 is made available on the cluster as a shared Apptainer container image. To run CCP4 binaries within the container, prefix any command with the name of the apptainer image, <code>ccp4.sif</code>.</p> <p>Alternatively you can enter an interactive shell in the container with:</p> <p>Terminal</p> <pre><code>ccp4.sif /bin/bash\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/ccp4/#via-sbgrid","title":"via SBGrid","text":"<p>See SBGrid</p>"},{"location":"getting_started/software/software_environments/containers/circulator/","title":"Circlator","text":"<p>Circlator is made available as an Apptainer container. </p> <p>You can direcly run 'circlator' from the container with:</p> <p>Terminal</p> <pre><code>circlator_1.5.5.sif circlator test outdir\n</code></pre> <p>Alternatively you can enter an interactive shell in the container with:</p> <p>Terminal</p> <pre><code>circlator-1.5.5.sif /bin/bash\n</code></pre> <p>and run circlator from there.</p> <p>This assumes all input and output files are located in your home directory, which gets automatically made available in the container by Apptainer.  Other paths will have to be explicitely mapped, in which case we need to use the apptainer run command excplictely pointing to the location of the container image.  e.g. to map the <code>/scratch/foo/data</code> directory on <code>/data</code> in the container:</p> <p>Terminal</p> <pre><code>apptainer run --bind /tmp/test $APPTAINER_IMAGES/circlator_1.5.5.sif circlator test /tmp/test/outdir\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/claritas/","title":"GLOBEClaritas","text":"<p>GLOBEClaritas is a proprietary software package for 2D and 3D land and marine seismic data processing. </p> <p>Warning</p> <p>This application and the OnDemand app is currently being tested and may not fully work as expected yet.</p>"},{"location":"getting_started/software/software_environments/containers/claritas/#license","title":"License","text":"<p>GLOBEClaritas is licensed software. Running it on the Research Cluster will require you to bring your own network license server configuration file.</p> <p>By default, both the GUI and the commandline tools will expect your <code>serverLicense.lic</code> to exist as <code>$HOME/.claritas/serverLicense.lic</code>.  This can be overridden in the OnDemand app form, or for commandline usage by setting the <code>$CLARITAS_LICENSE</code> environment variable.</p>"},{"location":"getting_started/software/software_environments/containers/claritas/#shared-projects","title":"Shared projects","text":"<p>By default, the projects registry (<code>projects</code>) will be set to <code>$HOME/.claritas/projects</code>. The OnDemand app will create a blank file for you when starting  the app the first time; If using the commandline tools exclusively, you will need to create this file yourself (e.g. <code>mkdir ~/.claritas; touch ~/claritas/.projects</code>).  This project registry file in your <code>$HOME</code> directory will be accessible to your user account only. </p> <p>For shared projects, a shared space with write access for all participants should be set up under <code>/projects</code>. All users should then be pointing their GLOBEClaritas  sessions to the same :<code>projects</code> file, and any new projects should be created under the shared <code>/projects</code> space. </p> <p>In the GUI OnDemand app launch form, the path to the shared <code>projects</code> file can be specified. The OOD app will create a blank projects file at the given location on first run. </p> <p>For commandline usage, set the <code>$CLARITAS_PROJECTS</code> environment variable to the <code>projects</code> file path. This file will need to be manually created if it doesn't exist yet.</p>"},{"location":"getting_started/software/software_environments/containers/claritas/#gui","title":"GUI","text":"<p>The GUI can be accessed via the Open OnDemand Applications.</p>"},{"location":"getting_started/software/software_environments/containers/claritas/#commandline-tools","title":"Commandline tools","text":"<p>GLOBEClaritas is made available on the cluster as a shared Apptainer container image. Commandline tools have to be run within the context of the container.</p> <p>You can use the <code>apptainer/GLOBEClaritas</code> module to add convenient wrapper aliases to any of the Claritas binaries. i.e.</p> <p>Terminal</p> <pre><code>module load apptainer/GLOBEClaritas\nclaritas_info\n</code></pre> <p>The aliases will also bind-mount the GLOBEClaritas projects registry path (<code>$CLARITAS_PROJECTS</code>, which is set to <code>$HOME/.claritas/projects</code> by default) as well as the  license server file (<code>$CLARITAS_LICENSE</code>, set to <code>$HOME/.claritas/serverReference.lic</code> by default) into the container. Both of these files need to exist in order to run any commands in the container.</p> <p>To use aliases in a non-interactive/SLURM batch script, add the following in your script before using the alias:</p> <p>Terminal</p> <pre><code># The following is required to use aliases in a non-interactive/SLURM batch script:\nshopt -s expand_aliases\nmodule load apptainer/GLOBEClaritas\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/claritas/#slurm","title":"Slurm","text":"<p>TODO</p>"},{"location":"getting_started/software/software_environments/containers/colabfold/","title":"ColabFold","text":"<p>Easy to use protein structure and complex prediction using AlphaFold2 and Alphafold2-multimer.</p> <p>Local ColabFold is made available on the cluster as a shared Apptainer container image.  This should be run on a GPU Compute partition.</p> <p>You can use the <code>apptainer/colabfold</code> module which will add convenient aliases for: <code>colabfold_batch</code>,  <code>colabfold_search</code>, <code>colabfold_split_msas</code>, which will run within the container; The alias will also bind-mount the AlphaFold2 weights cache path (<code>/opt/colabfold/alpha2_weights_cache</code> on the nodes) into  the container on <code>/cache</code>.</p> <p>See the LocalColabFold documentation for usage information. Example:</p> <p>Terminal</p> <pre><code>module load apptainer/colabfold\n# The following is required to use aliases in a non-interactive/SLURM batch script:\nshopt -s expand_aliases\ncolabfold_batch ./input.fasta ./out/\n</code></pre> <p>An example Slurm script to run ColabFold on the cluster is provided below:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=colabfold    # Job name\n#SBATCH --partition=aoraki_gpu  # Partition (queue) name\n#SBATCH --nodes=1               # Number of nodes\n#SBATCH --ntasks-per-node=1     # Number of tasks (1 task per node)\n#SBATCH --cpus-per-task=12      # Number of CPU cores per task\n#SBATCH --gres=gpu:1            # Number of GPUs required\n#SBATCH --mem=96G               # Job memory request\n#SBATCH --time=10:00:00         # Time limit hrs:min:sec\n#SBATCH --mail-user=USERNAME@otago.ac.nz\n#SBATCH --output=colabfold%j.log # Standard output log\n\n# Set variables\nbase_name=\"$1\"\noutput_fasta=\"${base_name}_getorf.output.fa\"\n\n# Load the apptainer/colabfold module (assuming it's in your PATH)\nmodule load apptainer/colabfold\n\nshopt -s expand_aliases  # Enable alias expansion\n\n# Check loaded modules (for debugging)\nmodule list\n\n# Ensure PATH includes module binaries (for debugging)\necho \"Current PATH: $PATH\"\n\n# Run colabfold_batch command using alias\ncolabfold_batch \"./$output_fasta\" ./out/\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/connectome_workbench/","title":"Connectome Workbench","text":"<p>Connectome Workbench is an open-source visualization and discovery tool used to explore data generated by the Human Connectome Project.</p>"},{"location":"getting_started/software/software_environments/containers/connectome_workbench/#via-ondemand","title":"via OnDemand","text":"<p>The Connectome Workbench wb_view GUI is available as an Open OnDemand app.</p>"},{"location":"getting_started/software/software_environments/containers/connectome_workbench/#via-commandlineslurm","title":"via commandline/Slurm","text":"<p>Connectome Workbench is made available on the cluster as a shared Apptainer container image. You can use the <code>apptainer/connectome_workbench</code> module to add convenient aliases to running any of the FSL binaries within the container:</p> <p>Terminal</p> <pre><code>module load apptainer/connectome_workbench\n# The following is required to use aliases in a non-interactive/SLURM batch script:\nshopt -s expand_aliases\nwb_commnd ....\n</code></pre> <p>Alternatively run with apptainer directly; i.e. to run binaries within the container, prefix any command with <code>apptainer run $APPTAINER_IMG/&lt;apptainer_image.sif&gt;</code>. </p> <p>To get an interactive shell into the container:</p> <p>Terminal</p> <pre><code>apptainer run $APPTAINER_IMG/connectome_workbench-&lt;version&gt;.sif /bin/bash\n</code></pre> <p>Within the container, you'll find the binaries under `/opt/workbench/``</p> <p>These can also be invoked directly within the context of the container, with:</p> <p>Terminal</p> <pre><code>apptainer run $APPTAINER_IMG/connectome_workbench-&lt;version&gt;.sif wb_command ...\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/deeplabcut/","title":"DeepLabCut","text":""},{"location":"getting_started/software/software_environments/containers/deeplabcut/#gui","title":"GUI","text":"<p>The DeepLabCut GUI can be accessed via the Open OnDemand Applications. It is highly recommended to run this on a GPU compute/CUDA partition.</p>"},{"location":"getting_started/software/software_environments/containers/deeplabcut/#command-line-interface","title":"Command line interface","text":"<p>DeepLabCut is made available on the cluster as a shared Apptainer container image. Scripted/commandline access requires Python to be run within the context of the container.</p> <ul> <li> <p>Open a terminal on one of the GPU nodes.</p> </li> <li> <p>Start ipython in the container</p> </li> </ul> <p>Terminal</p> <pre><code>apptainer run --nv /opt/apptainer_img/deeplabcut-2.3.9-cuda11.8.sif ipython\n</code></pre> <ul> <li> <p>Then type: <code>import deeplabcut</code></p> </li> <li> <p>See https://deeplabcut.github.io/DeepLabCut/docs/standardDeepLabCut_UserGuide.html#deeplabcut-in-the-terminal-command-line-interface</p> </li> </ul>"},{"location":"getting_started/software/software_environments/containers/deeplabcut/#slurm","title":"Slurm","text":"<p>DeepLabCut is made available on the cluster as a shared Apptainer container image. Scripted/commandline access requires Python to be run within the context of the container.</p>"},{"location":"getting_started/software/software_environments/containers/deepposekit/","title":"DeepPoseKit","text":"<p>DeepPoseKit is a Python toolkit with a high-level API for 2D pose estimation of user-defined keypoints using deep learning.</p> <p>While DeepPoseKit can be self-installed e.g. using conda as per the project's installation instructions, the legacy codebase requires a specific set of outdated  dependencies with a supported TensorFlow/CUDA combination. We have bundled a working environment in an Apptainer container which can be launched from the OOD JupyterLab app or used on the commandline.</p> <p>The project website links to a number of notebooks detailing usage and the general workflow.</p>"},{"location":"getting_started/software/software_environments/containers/deepposekit/#jupyter","title":"Jupyter","text":"<p>The DeepPoseKit variant can be selected in the OOD JupyterLab app.  For CUDA support, make sure to select a GPU/CUDA partition.</p> <p>Note that the annotation GUI unfortunately cannot be run from the web-based Jupyterlab and needs to be run from an interactive graphical environment; This can be as a Jupyter notebook on your local workstation, or a jupyter server or python session run from an OOD desktop session (see commandline use below).</p>"},{"location":"getting_started/software/software_environments/containers/deepposekit/#commandline","title":"Commandline","text":"<p>DeepPoseKit has an interactive graphical annotation window (cf. 'step 2') that cannot be run from the web-based OOD JupyterLab. For this purpose, you could instead start a notebook server or run the python code from a terminal in a graphical OOD desktop session:</p> <p>Terminal</p> <pre><code>module load apptainer/deepposekit\nstart-notebook.py\n</code></pre> <p>and copy-paste the full <code>127.0.0.1</code> URL displayed into the browser (Firefox) within the desktop session to access the notebook. (e.g. <code>http://127.0.0.1:8888/lab?token=2edd3d93b0411e75da1a14029d3c5b23a6214b5048d11e76</code> )</p> <p>DeepPoseKit is made available on the cluster as a shared Apptainer container image. </p> <p>You can use the <code>apptainer/deepposekit</code> module to add a convenient alias to <code>python</code> in the container:</p> <p>Terminal</p> <pre><code>#SBATCH &lt;slurm job script options&gt;\n\nmodule load apptainer/deepposekit\n# The following is required to use aliases in a non-interactive/SLURM batch script:\nshopt -s expand_aliases\npython -c 'import deepposekit; print(deepposekit.__version__)'\n</code></pre> <p>Alternatively run with apptainer directly; i.e. to run binaries within the container, prefix any command with <code>apptainer -s run --nv &lt;$APPTAINER_IMG/apptainer_image.sif&gt;</code>.</p> <p>For CUDA support, make sure to specify <code>--nv</code>. e.g. </p> <p>Terminal</p> <pre><code>apptainer -s run --nv $APPTAINER_IMG/jupyter_deepposekit_&lt;version&gt;.sif python\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/dorado/","title":"Dorado","text":""},{"location":"getting_started/software/software_environments/containers/dorado/#dorado","title":"Dorado","text":"<p>Dorado is a high-performance, easy-to-use, open source basecaller for Oxford Nanopore reads.  It needs to be run on a partition/node with GPU compute/CUDA support, and is heavily-optimised for Nvidia A100 and H100 GPUs.</p> <p>Dorado is made available on the cluster as a shared Apptainer container image.</p> <p>You can use the <code>apptainer/dorado</code> module to add a convenient alias to running <code>dorado</code> within the container:</p> <p>Terminal</p> <pre><code>module avail dorado\nmodule load apptainer/dorado/0.7.1\n# The following is required to use aliases in a non-interactive/SLURM batch script:\nshopt -s expand_aliases\ndorado ....\n</code></pre> <p>Alternatively run with apptainer directly; i.e. to run binaries within the container, prefix any  command with <code>apptainer -s run --nv &lt;$APPTAINER_IMG/apptainer_image.sif&gt;</code>. </p> <p>Make sure to specify <code>--nv</code> to enable NVIDIA GPU support. e.g. </p> <p>Terminal</p> <pre><code>apptainer -s run --nv $APPTAINER_IMG/dorado-&lt;version&gt;.sif dorado basecaller /models/dna_r10.4.1_e8.2_400bps_hac@v4.1.0 ~/pod5s/\n</code></pre> <p>Also note that <code>-s / --silent</code> is required here to suppress the verbose container output that may otherwise contaminate standard output.</p> <ul> <li>To list the available basecaller models, run</li> </ul> <p>Terminal</p> <pre><code>$APPTAINER_IMG/dorado-&lt;version&gt;.sif ls -1 /models\n</code></pre> <p>As with all Apptainer containers, take care to qualify the files and paths in the context of the  container image, i.e. Models are located within the container in <code>/models/</code>, and any other files  and data stored outside the container needs to be in a folder that is bound by Apptainer into the  container (either by default, such as your <code>$HOME</code>, <code>/scratch</code> or <code>/projects</code>), or  by explicitely specifying a bind mount with Apptainer's <code>--bind</code> option). </p> <p>Please refer to the dorado GitHub page for more information regarding running dorado. https://github.com/nanoporetech/dorado</p>"},{"location":"getting_started/software/software_environments/containers/fsl/","title":"FSL","text":"<p>FSL is a library of analysis tools for FMRI, MRI and diffusion brain imaging data.</p>"},{"location":"getting_started/software/software_environments/containers/fsl/#via-ondemand","title":"via OnDemand","text":"<p>The FSL GUI tools (including FSLeyes) can be accessed via the Open OnDemand Applications. FSLeyes will be run with 3D hardware acceleration when started on a '3D-accelerated'/GPU partition (default); This should significantly improve rendering responsiveness. </p>"},{"location":"getting_started/software/software_environments/containers/fsl/#via-apptainer","title":"via Apptainer","text":"<p>FSL is made available on the cluster as a shared Apptainer container image. You can use the <code>apptainer/FSL</code> module to add convenient aliases to running any of the FSL binaries within the container:</p> <p>Terminal</p> <pre><code>module avail fsl\nmodule load apptainer/FSL/6.0.7.6\n# The following is required to use aliases in a non-interactive/SLURM batch script:\nshopt -s expand_aliases\nfsl......\n</code></pre> <p>Alternatively run with apptainer directly; i.e. to run binaries within the container, prefix any command with <code>apptainer run $APPTAINER_IMG/&lt;apptainer_image.sif&gt;</code>. </p> <p>To get an interactive shell into the container:</p> <p>Terminal</p> <pre><code>apptainer run $APPTAINER_IMG/fsl.sif /bin/bash\n</code></pre> <p>Within the container, you'll find the FSL binaries in <code>/home/fsl/fsl/bin</code></p> <p>These can also be invoked directly within the context of the container, with:</p> <p>Terminal</p> <pre><code>apptainer run $APPTAINER_IMG/fsl.sif &lt;fsl_binary&gt;\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/fsl/#via-spack","title":"via Spack","text":"<p>Alternatively FSL (<code>fsl</code>) can also be self-installed using Spack.</p>"},{"location":"getting_started/software/software_environments/containers/gubbins/","title":"Gubbins","text":"<p>Genealogies Unbiased By recomBinations In Nucleotide Sequences - Rapid phylogenetic analysis of large samples of recombinant bacterial whole genome sequences.</p> <p>Gubbins is made available on the cluster as a shared Apptainer container image. </p> <p>You can use the <code>apptainer/gubbins</code> module which will add convenient aliases to Gubbins scripts, such as  <code>run_gubbins.py</code>, which will run within the container.</p> <p>To use any of the aliases in a non-interactive/SLURM batch script, add the following in your script before using the alias: <code>shopt -s expand_aliases</code>.</p> <p>See the Gubbins manual for usage information. Example:</p> <p>Terminal</p> <pre><code>module load apptainer/gubbins\n# The following is required to use aliases in a non-interactive/SLURM batch script:\nshopt -s expand_aliases\nrun_gubbins.py &lt;FASTA alignment&gt;\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/guppy/","title":"Guppy (GPU)","text":"<p>Guppy is a data processing toolkit that contains the Oxford Nanopore Technologies' basecalling algorithms, and several bioinformatic post-processing features.</p> <p>The GPU version of guppy is significantly faster than the CPU version, and should be run on a partition/node with CUDA (GPU compute) support.</p> <p>The GPU-enabled guppy is made available on the cluster as a shared Apptainer container image. </p> <p>You can use the <code>apptainer/guppy</code> module to add a convenient alias to running <code>dorado</code> within the container.</p> <p>To use any of the aliases in a non-interactive/SLURM batch script, add the following in your script before using the alias:</p> <p>Terminal</p> <pre><code>shopt -s expand_aliases\n</code></pre> <p>Terminal</p> <pre><code>module avail guppy\nmodule load apptainer/guppy-gpu/6.4.6\n# The following is required to use aliases in a non-interactive/SLURM batch script:\nshopt -s expand_aliases\nguppy_basecaller ....\n</code></pre> <p>Alternatively run with apptainer directly; i.e. To run binaries within the container, prefix any command with <code>apptainer run --nv &lt;$APPTAINER_IMG/apptainer_image.sif&gt;</code>.  Make sure to specify <code>--nv</code> To enable GPU support.</p> <p>Terminal</p> <pre><code>apptainer run --nv $APPTAINER_IMG/guppy-gpu.sif guppy_basecaller &lt;guppy options&gt;\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/guppy/#spack","title":"Spack","text":"<p>Alternatively guppy (<code>ont-guppy</code>) can also be installed using Spack.</p>"},{"location":"getting_started/software/software_environments/containers/humann3/","title":"HUMAnN 3.0","text":"<p>While the requirements for the HUMAnN pipeline (MetaPhlAn etc.) could be satisfied using individual spack package loads, an Apptainer container packaging all required tools is available as a convenient alternative.</p> <p>To enter an interactive shell in the container (with your home directory automatically mounted):</p> <p>Terminal</p> <pre><code>humann3.sif /bin/bash\nhumann_test\n</code></pre> <p>For usage, refer the tutorials at https://github.com/biobakery/biobakery/wiki/humann</p>"},{"location":"getting_started/software/software_environments/containers/matlab/","title":"MATLAB","text":""},{"location":"getting_started/software/software_environments/containers/matlab/#matlab-gui","title":"MATLAB GUI","text":"<p>OnDemand features a MATLAB application using containerised builds of MATLAB including a number of popular toolboxes.</p>"},{"location":"getting_started/software/software_environments/containers/matlab/#toolboxes","title":"Toolboxes","text":"<p>Toolboxes and addons included:</p> <pre><code>* Image_Processing_Toolbox\n* Mapping_Toolbox Optimization_Toolbox\n* Parallel_Computing_Toolbox\n* Signal_Processing_Toolbox\n* Statistics_and_Machine_Learning_Toolbox\n* Wavelet_Toolbox\n* MATLAB_Compiler\n* MATLAB_Compiler_SDK\n* Deep_Learning_Toolbox\n* Computer_Vision_Toolbox\n* Simulink\n* DSP_System_Toolbox\n* Sensor_Fusion_and_Tracking_Toolbox\n* Image_Acquisition_Toolbox\n* Navigation_Toolbox\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/matlab/#hardware-accelerated-display","title":"Hardware-accelerated display","text":"<p>If your MATLAB work involves rendering/visualisation, you may benefit from using OpenGL 3D hardware acceleration for display.</p> <p>When launching the OOD app, make sure to tick the <code>Request GPU</code> and <code>3D hardware-accelerated display</code> checkbox.</p> <p>To verify from within MATLAB, enter: <code>rendererinfo()</code>. The Renderer should list an NVIDIA model; If this is 'llvmpipe', you are using software rendering instead.</p>"},{"location":"getting_started/software/software_environments/containers/matlab/#commandline-matlab","title":"Commandline MATLAB","text":"<p>Commandline <code>matlab</code> is available by default. Additional versions will be made available via <code>module</code> going forward.</p>"},{"location":"getting_started/software/software_environments/containers/matlab/#via-slurm","title":"via SLURM","text":"<p>Different containerised builds of MATLAB are available in /opt/apptainer_img/ folder:</p> <p>Terminal</p> <pre><code>/opt/apptainer_img/matlab-r2018a-GL.sif\n/opt/apptainer_img/matlab-r2023b-GL.sif\n/opt/apptainer_img/matlab-r2023b.sif\n/opt/apptainer_img/matlab-r2024a-GL.sif\n/opt/apptainer_img/matlab-r2024b-GL.1.sif\n/opt/apptainer_img/matlab-r2024b-GL.sif -&gt; matlab-r2024b-u4-GL.sif\n/opt/apptainer_img/matlab-r2024b-u4-GL.sif\n</code></pre> <p>Start a new slurm interactive job on the cpu node:</p> <p>Terminal</p> <pre><code>[userxyz@aoraki-login ~]$ srun --ntasks=1 --partition=aoraki --cpus-per-task=2 --time=1-01:00 --mem=20G --pty --x11=all /bin/bash\n[userxyz@rtis-hpc-r01 ~]$ apptainer shell --bind \"$(pwd)\":/opt/workspace,/weka/rtis/userxyz/tmp:/tmp --pwd /opt/workspace /opt/apptainer_img/matlab-r2024a-GL.sif\nApptainer&gt; export MLM_LICENSE_FILE=27001@slo-licence-svr.registry.otago.ac.nz\n</code></pre> <p>Start matlab in gui mode (ssh connection to aoraki-login node has to have \"<code>-X</code>\" or \"<code>-Y</code>\" to enable X11 forwarding, and <code>srun`` command has to have \"</code>--x11=all`\":</p> <p>Terminal</p> <pre><code>Apptainer&gt; matlab\n</code></pre> <p>Start matlab in non-gui mode:</p> <p>Terminal</p> <pre><code>Apptainer&gt; matlab -nodesktop\n\n&lt; M A T L A B (R) &gt;\nCopyright 1984-2024 The MathWorks, Inc.\nR2024a (24.1.0.2537033) 64-bit (glnxa64)\nFebruary 21, 2024\n\nTo get started, type doc.\nFor product information, visit www.mathworks.com.\n\n&gt;&gt;\n</code></pre> <p>To run a matlab script:</p> <p>Terminal</p> <pre><code>Apptainer&gt; matlab -nodisplay &lt; MATLAB_job.m\n</code></pre> <p>Start a new slurm interactive job on the gpu node:</p> <p>Terminal</p> <pre><code>[userxyz@aoraki-login ~]$ srun --ntasks=1 --partition=aoraki_gpu_H100 --nodelist=aoraki30 --cpus-per-task=1 --time=1-01:00 --gres=gpu:1 --mem=20G --pty --x11=all /bin/bash\n[userxyz@rtis-hpc-r30 ~]$ nvidia-smi\n</code></pre> <pre><code>Wed Mar  5 12:19:20 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 555.42.06              Driver Version: 555.42.06      CUDA Version: 12.5     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA H100 NVL                Off |   00000000:61:00.0 Off |                    0 |\n| N/A   25C    P0             60W /  400W |       1MiB /  95830MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n</code></pre> <pre><code>[userxyz@rtis-hpc-r30 ~]$ module load cuda/11.8\n[userxyz@rtis-hpc-r30 ~]$ apptainer shell --nv --bind \"$(pwd)\":/opt/workspace,/weka/rtis/userxyz/tmp:/tmp --pwd /opt/workspace /opt/apptainer_img/matlab-r2024a-GL.sif\nApptainer&gt; export MLM_LICENSE_FILE=27001@slo-licence-svr.registry.otago.ac.nz\nApptainer&gt; matlab -nodesktop\n\n&lt; M A T L A B (R) &gt;\nCopyright 1984-2024 The MathWorks, Inc.\nR2024a (24.1.0.2537033) 64-bit (glnxa64)\nFebruary 21, 2024\n\nTo get started, type doc.\nFor product information, visit www.mathworks.com.\n\n&gt;&gt; gpuDevice\n</code></pre> <pre><code>ans =\n\nCUDADevice with properties:\n\n                    Name: 'NVIDIA H100 NVL'\n                   Index: 1\n       ComputeCapability: '9.0'\n          SupportsDouble: 1\n   GraphicsDriverVersion: '555.42.06'\n             DriverModel: 'N/A'\n          ToolkitVersion: 12.2000\n      MaxThreadsPerBlock: 1024\n        MaxShmemPerBlock: 49152 (49.15 KB)\n      MaxThreadBlockSize: [1024 1024 64]\n             MaxGridSize: [2.1475e+09 65535 65535]\n               SIMDWidth: 32\n             TotalMemory: 99989127168 (99.99 GB)\n         AvailableMemory: 99438559232 (99.44 GB)\n             CachePolicy: 'balanced'\n     MultiprocessorCount: 132\n            ClockRateKHz: 1785000\n             ComputeMode: 'Default'\n    GPUOverlapsTransfers: 1\n  KernelExecutionTimeout: 0\n        CanMapHostMemory: 1\n         DeviceSupported: 1\n         DeviceAvailable: 1\n          DeviceSelected: 1\n</code></pre> <pre><code>&gt;&gt; gpuDeviceTable\n</code></pre> <pre><code>ans =\n\n1x5 table\n\n  Index          Name           ComputeCapability    DeviceAvailable    DeviceSelected\n  _____    _________________    _________________    _______________    ______________\n\n    1      \"NVIDIA H100 NVL\"          \"9.0\"               true              true\n</code></pre> <pre><code>&gt;&gt; rendererinfo()\n</code></pre> <pre><code>ans = \n\nstruct with fields:\n\n  GraphicsRenderer: 'OpenGL Software'\n            Vendor: 'Mesa/X.org'\n           Version: '4.5 (Compatibility Profile) Mesa 22.3.6'\n    RendererDevice: 'llvmpipe (LLVM 15.0.6, 256 bits)'\n           Details: [1x1 struct]\n</code></pre> <p>Submit a slurm batch job:</p> <p>Terminal</p> <pre><code>[userxyz@aoraki-login]$ cat /projects/userxyz/matlab.sh\n#!/bin/bash\n\n#SBATCH --job-name=\"matlab-xyz\"                         # job name\n#SBATCH --partition=aoraki                              # partition to which job should be submitted aoraki_gpu...\n##SBATCH --nodelist=aoraki15                            # optional node \n##SBATCH --gres=gpu:1                                   # optional gpu if running a gpu job on the gpu partition\n#SBATCH --nodes=1                                       # node count\n#SBATCH --ntasks=2                                      # total number of tasks across all nodes\n#SBATCH --cpus-per-task=1                         # cpu-cores per task\n#SBATCH --mem=20G                                      # total memory per node\n#SBATCH --time=7-00:00                                  # wall time DD-HH:MM\n##SBATCH --auks=yes                                     # optional if using HCS\n##SBATCH --output=/projects/.../userxyz/%x/%x_%j_%a.out # optional output folder\n#SBATCH --mail-user userxyz@otago.ac.nz                 # optional email\n#SBATCH --mail-type BEGIN\n#SBATCH --mail-type END\n#SBATCH --mail-type FAIL\n\necho \"Script start\"\n\n## Export matlab licence\nexport MLM_LICENSE_FILE=27001@slo-licence-svr.registry.otago.ac.nz\n\n## GPU job\n## apptainer exec --nv --bind \"$(pwd)\":/opt/workspace,/weka/rtis/userxyz/tmp:/tmp --pwd /opt/workspace /opt/apptainer_img/matlab-r2024a-GL.sif matlab -nodisplay -nodesktop &lt; MATLAB_job.m\n\n## CPU job\napptainer exec --bind \"$(pwd)\":/opt/workspace,/weka/rtis/userxyz/tmp:/tmp --pwd /opt/workspace /opt/apptainer_img/matlab-r2024a-GL.sif matlab -nodisplay -nodesktop &lt; MATLAB_job.m\n\necho \"Script end\"\n</code></pre> <p>Submit a slurm array batch job:</p> <p>Terminal</p> <pre><code>[userxyz@aoraki-login]$ cat /projects/userxyz/matlab-array.sh\n#!/bin/bash\n\n#SBATCH --job-name=\"matlab-array                        # job name\n#SBATCH --partition=aoraki                              # partition to which job should be submitted aoraki_gpu...\n##SBATCH --nodelist=aoraki15                            # optional node \n##SBATCH --gres=gpu:1                                   # optional gpu if running a gpu job on the gpu partition\n#SBATCH --nodes=1                                       # node count\n#SBATCH --ntasks=1                                      # total number of tasks across all nodes\n#SBATCH --cpus-per-task=1                               # cpu-cores per task\n#SBATCH --mem=20G                                       # total memory per node\n#SBATCH --array=1-8                                     # run 8 array jobs\n#SBATCH --time=0-01:00                                  # wall time DD-HH:MM\n##SBATCH --auks=yes                                     # optional if using HCS\n##SBATCH --output=/projects/.../userxyz/%x/%x_%j_%a.out # optional output folder\n#SBATCH --mail-user userxyz@otago.ac.nz                 # optional email\n#SBATCH --mail-type BEGIN\n#SBATCH --mail-type END\n#SBATCH --mail-type FAIL\n\necho \"Script start\"\n\n## Export matlab licence\nexport MLM_LICENSE_FILE=27001@slo-licence-svr.registry.otago.ac.nz\n\n## GPU job\n## apptainer exec --nv --bind \"$(pwd)\":/opt/workspace --pwd /opt/workspace /opt/apptainer_img/matlab-r2024a-GL.sif matlab -nodisplay -nodesktop -r \"process_input(${SLURM_ARRAY_TASK_ID}); exit;\"\n\n## CPU job\napptainer exec --bind \"$(pwd)\":/opt/workspace --pwd /opt/workspace /opt/apptainer_img/matlab-r2024a-GL.sif matlab -nodisplay -nodesktop -r \"process_input(${SLURM_ARRAY_TASK_ID}); exit;\"\n\necho \"Script end\"\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/ollama/","title":"Ollama","text":"<p>Ollama is a free and open source inference runtime for large language model (LLM) applications.</p> <p>GPU acceleration is required for inference so it needs to be run on a GPU partition.</p>"},{"location":"getting_started/software/software_environments/containers/ollama/#setup-and-basic-operation","title":"Setup and basic operation","text":"<p>The container that has the Ollama software in it is called the Ollama Shell Environment. When you run it it loads up the Ollama inference server in the background and when that has loaded it then drops you into an Ubuntu 22.04 Bash shell where you can start to give commands via the command line:</p> <p>Terminal</p> <pre><code>[harsi12p@aoraki27 ~]$ ollama-env.sh\nNOTICE: Starting Ollama server in the background.\nNOTICE: Waiting for the server to come online (1/10)\n\n## Ollama Container Shell Environment ##\n\nAny missing packages or libraries? Send requests to:\n  Mail:     rtis.solutions@otago.ac.nz\n  Subject:  Additions to Ollama shell environment (container_ollama_shellenv)\n\nUse the following environmental variables for this container instance:\n  * OLLAMA_HOST     : 127.0.0.1:11444\n  * OLLAMA_BASE_URL : http://127.0.0.1:11444\n  * OLLAMA_MODELS   : /home/harsi12p/.ollama/models\n  * HF_HOME         : ~/.cache/huggingface\n  * OPENAI_URL_BASE : http://127.0.0.1:11444/v1\n\nInstall any extra Python packages with:\n  python install --user &lt;&lt;PACKAGE_NAME&gt;&gt;\n\nPress [CTRL] + [D] to exit.\n\nOllEnv harsi12p@aoraki27:~$\n</code></pre> <p>This has been done using a convience script called <code>ollama-env.sh</code>. Useful files such as this can be extracted from the container by running the following in an empty directory:</p> <p>Terminal</p> <pre><code>apptainer run /opt/apptainer_img/ollama_shellenv.sif --copy-execute-files\n</code></pre> <p>I recommend that you put <code>ollama-env.sh</code> in a directory that has been added to your <code>PATH</code> so you can call it regardless of where you are in the directory tree. If you don't want to do this you can run the container:</p> <p>Terminal</p> <pre><code>apptainer run --nv /opt/apptainer_img/ollama_shellenv.sif\n</code></pre> <p>The container picks an unused TCP port (not the dafault) and then starts the server on that. This is to provide isolation between different container instances. The container sets the <code>OLLAMA_HOST</code> environmental variable that tells the <code>ollama</code> binary and Ollama python library where to send it requests. </p>"},{"location":"getting_started/software/software_environments/containers/ollama/#home-directory-quota-constraints","title":"Home directory quota constraints","text":"<p>A quota system is in place on Aoraki that limits the data in a users home directory to 15GB. This can be easy exceeded with LLM models.  The possible solutions involve moving your data onto another storage medium (Ohau or HCS) and either setting environmental variables or copying data and then creating symlinks to the new data location.  Other hidden directories can also contains large amount of hidden files: <code>~/.cache</code> and <code>~/.local</code>. You can set the environmental variable <code>OLLAMA_MODELS</code> to a directory that is not in <code>/home</code> and Ollama will put any downloaded model files in there.</p>"},{"location":"getting_started/software/software_environments/containers/ollama/#operation","title":"Operation","text":"<p>Command line</p> <p>Commands can be given at the command line. For example:</p> <p>Terminal</p> <pre><code>OllEnv harsi12p@aoraki27:~$ ollama run llama3\n&gt;&gt;&gt; What is a cat? Give a response as a single sentence.\nA cat is a small, typically furry, carnivorous mammal of the family Felidae that purrs, scratches, and curls up in adorable ways to delight its human companions.\n&gt;&gt;&gt; Send a message (/? for help) \n</code></pre> <p>It may take anywhere from 10 seconds to two minutes to initially load the code and weights into VRAM. Pressing [CTRL] + [D] exits Ollama. You can use standard Unix pipes and redirection as well:</p> <p>Terminal</p> <pre><code>echo \"What is a cat? Give a response as a single sentence.\" | ollama run llama3 &gt; what-is-a-cat.txt\n</code></pre> <p>ipython</p> <p>iPython provides syntax highlighting and completion in the terminal. For example:</p> <p>Terminal</p> <pre><code>OllEnv harsi12p@aoraki27:~$ ipython\nPython 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]\nType 'copyright', 'credits' or 'license' for more information\nIPython 8.26.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: import ollama\nIn [2]: res = ollama.generate(\"llama3\", \"What is a cat? Give a response as a single sentence.\")\nIn [3]: print(res[\"response\"])\nA cat is a small, typically furry, carnivorous mammal that belongs to the family Felidae and is characterized by its agility, playful behavior, and distinctive vocalizations.\n</code></pre> <p>Pressing [CTRL] + [D] exits iPython. You have have to have run <code>ollama run llama3</code> or <code>ollama pull llama3</code> before you run any Python code using that LLM model.</p> <p>jupyter-notebook</p> <p>Terminal</p> <pre><code>OllEnv harsi12p@aoraki27:~$ jupyter-notebook\n</code></pre> <p>Jupyter notebook pops up Firefox that is also included within the container. Because of this you have to be able to display X11 programs (through WSL on Windows and XQuartz on Mac) on your local machine. This requires a bit more setup to function correctly.</p> <p>Batch mode The container can also run in batch mode. This is done by giving a container a single parameter. What happens is that the container starts the Ollama server but instead of dropping you into an interractive bash shell it runs your command and then exits. For example:</p> <p>Terminal</p> <pre><code>apptainer run --nv /opt/apptainer_img/ollama_shellenv.sif 'echo \"What is a cat? Give a response as a single sentence.\" | ollama run llama3 &gt; what-is-a-cat-batched.txt'\n</code></pre> <p>This is useful if you have large inference jobs that you want to run and you want to use SLURM (for a significant speed and resource increase).</p>"},{"location":"getting_started/software/software_environments/containers/ollama/#resources","title":"Resources","text":"<p>LLM inference jobs are very heavy on VRAM (video card RAM) and also on CUDA cores. Under testing it was found that Ollama would only run one model in VRAM at a time.  This would make using multiple models at the same time excruciating slow (i.e. having a standard inference and embedding model running at the same time).  Because of this the number of models has been set to three. You can change this before you run the container by setting the following environmental variables:</p> <p>Terminal</p> <pre><code>export OLLAMA_KEEP_ALIVE=5m\n# How long to keep the model in VRAM before it is unloaded.\n\nexport OLLAMA_MAX_LOADED_MODELS=3\n# How many models to have in VRAM at one time.\n\nexport OLLAMA_NUM_PARALLEL=1\n# How many inference jobs to do in parallel.\n</code></pre> <p>You can check resource usage by using <code>nvidia-smi</code> and <code>ollama ps</code>:</p> <p>Terminal</p> <pre><code>OllEnv harsi12p@aoraki27:~$ nvidia-smi\nFri Aug 30 18:26:42 2024\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 555.42.06              Driver Version: 555.42.06      CUDA Version: 12.5     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:21:00.0 Off |                    0 |\n| N/A   27C    P0             32W /  250W |       1MiB /  40960MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA A100-PCIE-40GB          Off |   00000000:81:00.0 Off |                    0 |\n| N/A   26C    P0             36W /  250W |    5477MiB /  40960MiB |      0%      Default |\n|                                         |                        |             Disabled |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|    1   N/A  N/A   3034108      C   ...unners/cuda_v11/ollama_llama_server       5468MiB |\n+-----------------------------------------------------------------------------------------+\n</code></pre> <p>And for Ollama process information:</p> <p>Terminal</p> <pre><code>OllEnv harsi12p@aoraki27:~$ ollama ps\nNAME            ID              SIZE    PROCESSOR       UNTIL\nllama3:latest   365c0bd3c000    5.4 GB  100% GPU        4 minutes from now\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/openprose/","title":"Openprose","text":"<p>.. _openpose-apptainer:</p>"},{"location":"getting_started/software/software_environments/containers/openprose/#openpose","title":"OpenPose","text":"<p>OpenPose is a real-time multi-person keypoint detection library developed by the CMU Perceptual Computing Lab. It supports body, face, hand, and foot pose estimation using deep learning.</p> <p>It must be run on a partition/node with GPU compute/CUDA support and is optimized for NVIDIA GPUs, particularly A100 and H100. OpenPose processes image directories or video files and outputs pose estimations in JSON and/or image formats.</p> <p>The OpenPose container is available on the cluster as a shared Apptainer image, installed at: <code>/opt/apptainer_img/openpose.sif</code></p>"},{"location":"getting_started/software/software_environments/containers/openprose/#using-the-module","title":"Using the Module","text":"<p>You can use the <code>apptainer/openpose</code> module to load OpenPose with convenient aliases for execution:</p> <p>Termianl</p> <pre><code>module avail openpose\nmodule load apptainer/openpose/1.7\n# Required if using aliases in a batch script\nshopt -s expand_aliases\n\nopenpose --image_dir &lt;input_folder&gt; \\\n         --write_json &lt;output_json_folder&gt; \\\n         --display 0 \\\n         --render_pose 0\n</code></pre> <p>Example test run:</p> <p>Terminal</p> <pre><code>openpose --image_dir /projects/rtis/higje06p/openpose/openpose/examples/media/ \\\n         --write_json /projects/rtis/higje06p/openpose/output/ \\\n         --display 0 \\\n         --render_pose 0\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/openprose/#running-a-gpu-job-on-the-cluster","title":"Running a GPU Job on the Cluster","text":"<p>To run OpenPose on a GPU node interactively via SLURM:</p> <p>Terminal</p> <pre><code>srun --partition=aoraki_gpu_A100_40GB \\\n     --gres=gpu:1 \\\n     --cpus-per-task=8 \\\n     --mem=32G \\\n     --time=01:00:00 \\\n     --pty bash\n\nmodule load apptainer/openpose/1.7\nopenpose --help\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/openprose/#ondemand-desktop-gui-use","title":"OnDemand Desktop GUI Use","text":"<p>You can also run OpenPose interactively on a GPU node via Open OnDemand:</p> <ol> <li>Navigate to: https://ondemand.otago.ac.nz</li> <li>Go to Interactive Apps &gt; Otago HPC Desktop (experimental)</li> <li> <p>Select the following options:</p> </li> <li> <p>GPU: Enabled</p> </li> <li>Advanced Options: De-select \"Shared GPU\"</li> <li>Number of GPUs: 1</li> <li>Partition: <code>aoraki_gpu_A100_40GB</code></li> <li>Cores: 4</li> <li>Memory: 16 GB</li> <li> <p>Walltime: 8 hours (adjust as needed)</p> </li> <li> <p>Launch the session and open a terminal in the remote desktop.</p> </li> </ol> <p>Example non-GUI container run:</p> <p>Terminal</p> <pre><code>apptainer exec --nv \\\n  --bind /home/$USER:/home/$USER \\\n  /opt/apptainer_img/openpose.sif \\\n  bash -c \"mkdir -p /home/$USER/openpose_output/json &amp;&amp; \\\n  cd /apps/openpose &amp;&amp; \\\n  ./build/examples/openpose/openpose.bin \\\n    --video examples/media/video.avi \\\n    --model_folder /apps/openpose/models/ \\\n    --write_json /home/$USER/openpose_output/json/ \\\n    --render_pose 0 --display 0\"\n</code></pre> <p>To run with GUI display enabled:</p> <p>Terminal</p> <pre><code>apptainer exec --nv \\\n  --env DISPLAY=$DISPLAY \\\n  --bind /tmp/.X11-unix:/tmp/.X11-unix \\\n  --bind /home/$USER:/home/$USER \\\n  /opt/apptainer_img/openpose-1.7-h100.sif \\\n  bash -c \"cd /apps/openpose &amp;&amp; \\\n  ./build/examples/openpose/openpose.bin \\\n    --video examples/media/video.avi \\\n    --model_folder /apps/openpose/models/ \\\n    --display 2 \\\n    --render_pose 1\"\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/openprose/#working-with-apptainer-directly","title":"Working with Apptainer Directly","text":"<p>Alternatively, run OpenPose manually using <code>apptainer exec</code>. Example:</p> <p>.. code-block:: bash</p> <pre><code>apptainer exec --nv $APPTAINER_IMG/openpose.sif [command]\n</code></pre> <p>You must use <code>--nv</code> to enable NVIDIA GPU support.</p> <p>Make sure input/output paths are within bound directories (e.g., <code>$HOME</code>, <code>/projects</code>) or manually specify them with <code>--bind</code>.</p>"},{"location":"getting_started/software/software_environments/containers/openprose/#common-openpose-cli-flags","title":"Common OpenPose CLI Flags","text":"<ul> <li><code>--image_dir &lt;dir&gt;</code>: Directory of input images</li> <li><code>--video &lt;file&gt;</code>: Input video file</li> <li><code>--write_json &lt;dir&gt;</code>: Output directory for JSON pose data</li> <li><code>--write_images &lt;dir&gt;</code>: Output directory for rendered images</li> <li><code>--write_video &lt;file&gt;</code>: Output rendered video</li> <li><code>--model_folder &lt;dir&gt;</code>: Model path (in-container path: <code>/apps/openpose/models/</code>)</li> <li><code>--display 0</code>: Disable window display</li> <li><code>--render_pose 0</code>: Disable rendering (useful for headless performance runs)</li> </ul> <p>To list all available CLI options:</p> <p>.. code-block:: bash</p> <pre><code>openpose --help\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/openprose/#further-information","title":"Further Information","text":"<p>Refer to the official OpenPose GitHub repository for detailed documentation:</p> <p>https://github.com/CMU-Perceptual-Computing-Lab/openpose</p>"},{"location":"getting_started/software/software_environments/containers/samtools/","title":"Samtools","text":"<p>Samtools 1.20 is natively installed for all users</p> <p>Terminal</p> <pre><code>samtools --version \n</code></pre>"},{"location":"getting_started/software/software_environments/containers/spaceranger/","title":"Space Ranger","text":"<p>Space Ranger is a set of analysis pipelines that process 10x Genomics Visium data with brightfield or fluorescence microscope images, allowing users to map the whole transcriptome in a variety of tissues. Space Ranger v3.0 now supports Visium HD. </p> <p>Space Ranger is available on the cluster in: `/opt/spaceranger/spaceranger-3.0.1/spaceranger``</p> <p>Space Ranger tutorial: https://www.10xgenomics.com/support/software/space-ranger/latest/tutorials/count-ffpe-tutorial</p> <p>To run spaceranger on the slurm cluster node create similar slurm script:</p> <p>Terminal</p> <pre><code>[account@aoraki-login spaceranger]$ cat spaceranger-slurm.sh \n\n#!/bin/bash\n\n#SBATCH --job-name=\"spaceranger-16c-64g\"     # job name\n#SBATCH --account=account                   # account       \n#SBATCH --partition=aoraki_bigcpu        # partition to which job should be submitted\n#SBATCH --nodes=1                        # node count\n#SBATCH --ntasks=1                       # total number of tasks across all nodes\n#SBATCH --cpus-per-task=16               # cpu-cores per task\n#SBATCH --mem=64G                        # total memory per node\n#SBATCH --time=0-10:00           # wall time DD-HH:MM\n#SBATCH --output=/scratch/tempdir/drazentest/%x/%x_%j_%a.out\n#SBATCH --mail-user account@otago.ac.nz\n#SBATCH --mail-type BEGIN\n#SBATCH --mail-type END\n#SBATCH --mail-type FAIL\n\necho \"Script start\"\n\n## Load bcl2fastq2\nmodule purge\nmodule load bcl2fastq2\n\n/opt/spaceranger/spaceranger-3.0.1/spaceranger count --id=\"Visium_FFPE_Mouse_Brain\" \\\n      --transcriptome=refdata-gex-mm10-2020-A \\\n      --probe-set=Visium_Mouse_Transcriptome_Probe_Set_v1.0_mm10-2020-A.csv \\\n      --fastqs=datasets/Visium_FFPE_Mouse_Brain_fastqs \\\n      --image=datasets/Visium_FFPE_Mouse_Brain_image.jpg \\\n      --slide=V11J26-127 \\\n      --area=B1 \\\n      --reorient-images=true \\\n      --localcores=16 \\\n      --localmem=64 \\\n      --create-bam=true\n\necho \"Script end\"\n</code></pre> <p>As of May 30, 2024, SLURM/cgroup rules now enforce strict resource limits based on the job specifications in the SLURM script. For example,  if your SLURM script requests 16 cores and 64 GB of RAM, the job will be restricted to these limits. Should the job exceed the  64 GB RAM allocation, it will terminate with an \"Out of memory\" error. For different or larger datasets, please adjust the resource requests in your  script accordingly to avoid such issues.</p> <p>Terminal</p> <pre><code>#SBATCH --cpus-per-task=16\n#SBATCH --mem=64G \n...\n\n--localcores=16 \\\n--localmem=64 \\\n</code></pre> <p>Submit slurm job:</p> <p>Terminal</p> <pre><code>[account@aoraki-login spaceranger]$ sbatch spaceranger-slurm.sh\n\n[account@aoraki-login slurm-mkdir]$ squeue --format=\"%.18i %.18P %.14u %.14a %.40j %.2t %.10M %.15l %.11D %.6C %.10m %.16b %.18N %.15R %15Q\"|grep account\n511981   aoraki_bigcpu   account   account   spaceranger-16c-64g  R    0:05   1-10:00:00     1     16    64G       N/A      aoraki15      aoraki15 200040\n</code></pre> <p>After the SLURM job is done, please check the job's efficiency and adjust the script if needed.</p> <p>Terminal</p> <pre><code>[account@aoraki-login spaceranger]$ seff 511981\n</code></pre> <pre><code>Job ID: 511981\nCluster: aoraki\nUser/Group: /lx_account\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 16\nCPU Utilized: 01:16:33\nCPU Efficiency: 31.83% of 04:00:32 core-walltime\nJob Wall-clock time: 00:15:02\nMemory Utilized: 43.62 GB\nMemory Efficiency: 68.15% of 64.00 GB\n</code></pre> <p>Generating FASTQs with bcl2fastq (Illumina Software)</p> <p>https://www.10xgenomics.com/support/software/cell-ranger/latest/analysis/inputs/cr-direct-demultiplexing-bcl2fastq</p> <p>bcl2fastq is available as a spack module.</p> <p>Terminal</p> <pre><code>[account@aoraki-login ~]$ module load bcl2fastq2\n[account@aoraki-login ~]$ bcl2fastq\n</code></pre> <pre><code>BCL to FASTQ file converter\nbcl2fastq v2.20.0.422\n</code></pre> <p>Please refer to the Space Ranger web page for more information regarding running spaceranger. https://www.10xgenomics.com/support/software/space-ranger/latest</p>"},{"location":"getting_started/software/software_environments/containers/tbprofiler/","title":"TBProfiler","text":"<p>TBProfiler is made available as an Apptainer container.</p> <p>You can use the <code>apptainer/TBProfiler</code> module which will add convenient aliases to <code>tb-profiler</code>, which will run within the container.</p> <p>To use any of the aliases in a non-interactive/SLURM batch script, add the following in your script before using the alias:</p> <p>Terminal</p> <pre><code>shopt -s expand_aliases\nmodule load apptainer/TBProfiler\ntb-profiler --version\n</code></pre>"},{"location":"getting_started/software/software_environments/containers/whisper/","title":"Whisper (speech-to-text)","text":"<p>It is highly recommended to run this on a GPU compute partition supporting CUDA12.</p>"},{"location":"getting_started/software/software_environments/containers/whisper/#gui","title":"GUI","text":"<p>A basic Gradio web UI can be accessed via the Open OnDemand Applications.</p> <p>Steps on doing basic speech-to-text transcriptions is further documented [here](https://rtis.cspages.otago.ac.nz/research-computing/hosted/stt.html#whisper.</p>"},{"location":"getting_started/software/software_environments/containers/whisper/#commandline-usage","title":"commandline usage","text":"<p>TODO</p>"},{"location":"getting_started/software/software_environments/containers/xdsguI/","title":"xdsguI","text":""},{"location":"getting_started/software/software_environments/containers/xdsguI/#xdsgui","title":"XDSGUI","text":"<p>XDSGUI is a graphical user interface for XDS, SHELX and ARCIMBOLDO for processing and phasing results for X-ray, neutron and electron diffraction data.</p>"},{"location":"getting_started/software/software_environments/containers/xdsguI/#via-ondemand","title":"via OnDemand","text":"<p>XDSGUI is available as an Open OnDemand app.</p>"},{"location":"getting_started/software/software_environments/containers/xdsguI/#via-sbgrid","title":"via SBGrid","text":"<p>See SBGrid</p>"},{"location":"getting_started/software/software_environments/modules/4ti2/","title":"4ti2","text":"<p>4ti2 is a software suite for algebraic, geometric, and combinatorial problems on linear spaces. It is available on the cluster via a module.</p>"},{"location":"getting_started/software/software_environments/modules/4ti2/#loading-the-module","title":"Loading the Module","text":"<p>To load 4ti2 version 1.6.11 and its required dependencies (GLPK and GMP):</p> <p>Terminal</p> <pre><code> module load 4ti2/1.6.11\n</code></pre> <p>This automatically loads the required modules:</p> <ul> <li><code>glpk/5.0-6xtss3c</code></li> <li><code>gmp/6.2.1</code></li> </ul> <p>Once loaded, 4ti2 commands like <code>zsolve</code>, <code>markov</code>, <code>groebner</code>, and others become available in your shell.</p>"},{"location":"getting_started/software/software_environments/modules/4ti2/#quick-example-solving-an-integer-system","title":"Quick Example: Solving an Integer System","text":"<p>Create a file named <code>matrix.mat</code> with the following content:</p> <pre><code>   3 3\n   1 2 3\n   0 1 4\n   3 3 1\n</code></pre> <p>Then run:</p> <p>Terminal</p> <p>```bash</p> <p>zsolve matrix.mat    ```</p> <p>This computes the integer kernel of the matrix.</p>"},{"location":"getting_started/software/software_environments/modules/4ti2/#common-commands-in-4ti2","title":"Common Commands in 4ti2","text":"<p>Some of the frequently used command-line tools in 4ti2 include:</p> <ul> <li><code>zsolve</code> \u2013 Solve systems of linear equations over integers</li> <li><code>groebner</code> \u2013 Compute toric Gr\u00f6bner bases</li> <li><code>markov</code> \u2013 Compute Markov bases</li> <li><code>hilbert</code> \u2013 Compute Hilbert bases</li> <li><code>homogenize</code> \u2013 Homogenize a matrix</li> <li><code>dual</code> \u2013 Compute the dual of a matrix</li> <li><code>graver</code> \u2013 Compute Graver bases</li> </ul> <p>You can get command-line help with:</p> <p>Terminal</p> <pre><code>&lt;command&gt; --help\n</code></pre> <p>For example:</p> <p>Terminal</p> <p>```bash</p> <p>groebner --help    ```</p>"},{"location":"getting_started/software/software_environments/modules/4ti2/#slurm-job-script-example","title":"SLURM Job Script Example","text":"<p>Here\u2019s a sample SLURM batch script for using 4ti2 non-interactively:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=4ti2-test\n#SBATCH --time=00:05:00\n#SBATCH --mem=1G\n#SBATCH --cpus-per-task=1\n#SBATCH --output=4ti2-%j.out\nmodule load 4ti2/1.6.11\necho \"1 1\" &gt; test.mat\necho \"1\" &gt;&gt; test.mat\nzsolve test.mat\n</code></pre>"},{"location":"getting_started/software/software_environments/modules/4ti2/#documentation-and-references","title":"Documentation and References","text":"<p>Official website: https://4ti2.github.io/</p> <p>The software is useful for research in:</p> <ul> <li>Integer programming</li> <li>Algebraic statistics</li> <li>Toric ideals</li> <li>Gr\u00f6bner and Markov bases</li> </ul>"},{"location":"getting_started/software/software_environments/modules/macaulay2/","title":"Macaulay2","text":"<p>Macaulay2 is a software system for research in algebraic geometry and commutative algebra. It is designed to be easy to use and provides a powerful programming environment. It is particularly well-suited for computations in algebraic geometry, commutative algebra, and related areas.</p> <p>Macaulay2 is provided via an Apptainer container and can run in either CPU or GPU mode. Use the module <code>macaulay2/1.24.11</code> to access the environment.</p>"},{"location":"getting_started/software/software_environments/modules/macaulay2/#cpu-mode-default","title":"CPU Mode (Default)","text":"<p>Macaulay2 can run in a regular CPU environment by default. No additional configuration is needed.</p> <p>Loading the Module:</p> <p>Terminal</p> <pre><code>module load macaulay2\nM2\n</code></pre> <p>Helper commands:</p> <ul> <li><code>M2-help</code> \u2013 Show help information</li> <li><code>M2-script</code> \u2013 Run a script non-interactively</li> <li><code>M2-interactive</code> \u2013 Start in quiet mode (no banner)</li> </ul> <p>Inside Macaulay2:</p> <p>Terminal</p> <pre><code>R = QQ[x, y]\nI = ideal(x^3 + y^2 - 1)\ngens gb I\n</code></pre> <p>Interactive CPU Access with srun:</p> <p>To start an interactive session on a general compute node (CPU-only):</p> <p>Terminal</p> <pre><code>srun --partition=aoraki --mem=4G --cpus-per-task=2 --time=00:30:00 --pty bash\n</code></pre> <p>Once on the node:</p> <p>Terminal</p> <pre><code>module load macaulay2\nM2\n</code></pre>"},{"location":"getting_started/software/software_environments/modules/macaulay2/#gpu-mode-requires-gpu-node","title":"GPU Mode (Requires GPU Node)","text":"<p>To use GPU acceleration, you must be on a GPU-capable node and request a GPU using SLURM.</p> <p>Set <code>USE_GPU=1</code> before loading the module to enable GPU support:</p> <p>Interactive GPU Access with srun:</p> <p>To run interactively on the <code>aoraki_gpu</code> partition:</p> <p>Terminal</p> <pre><code>srun --partition=aoraki_gpu --gres=gpu:1 --mem=8G --cpus-per-task=2 --time=00:30:00 --pty bash\n</code></pre> <p>Then:</p> <p>Terminal</p> <pre><code>export USE_GPU=1\nmodule load macaulay2\nM2\n</code></pre> <p>GPU SLURM Batch Job Example:</p> <p>Create a file named <code>macaulay2_gpu.slurm</code> with the following content:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=m2-gpu\n#SBATCH --partition=aoraki_gpu\n#SBATCH --gres=gpu:1\n#SBATCH --time=00:10:00\n#SBATCH --mem=4G\n#SBATCH --cpus-per-task=2\n#SBATCH --output=m2-gpu-%j.out\n\nexport USE_GPU=1\nmodule load macaulay2\n\nM2 &lt;&lt;'EOF'\nR = QQ[a,b]\nI = ideal(a^4 + b^4 - 1)\ngens gb I\nEOF\n</code></pre> <p>Submit the job:</p> <p>Terminal</p> <pre><code>sbatch macaulay2_gpu.slurm\n</code></pre>"},{"location":"getting_started/software/software_environments/modules/macaulay2/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p><code>M2: command not found</code>:   Ensure the module is loaded.</p> </li> <li> <p>GPU not detected:   Make sure you set <code>export USE_GPU=1</code> before loading the module, and requested a GPU in your SLURM job.</p> </li> <li> <p>To verify GPU access inside the container:</p> </li> </ul> <p>Terminal</p> <pre><code>apptainer exec --nv /opt/macaulay2/1.24.11/macaulay2.sif nvidia-smi\n</code></pre>"},{"location":"getting_started/software/software_environments/modules/mathematica/","title":"Mathematica","text":""},{"location":"getting_started/software/software_environments/modules/mathematica/#overview","title":"Overview","text":"<p>Mathematica is available on the HPC cluster via the <code>Environment Modules</code> system. The software is installed centrally in:</p> <p>Terminal</p> <pre><code>/opt/mathematica\n</code></pre>"},{"location":"getting_started/software/software_environments/modules/mathematica/#loading-the-module","title":"Loading the Module","text":"<p>Before using Mathematica, load the environment module:</p> <p>Terminal</p> <pre><code>module load mathematica\n</code></pre> <p>You can verify it is working by checking the version:</p> <p>Terminal</p> <pre><code>math -version\n</code></pre>"},{"location":"getting_started/software/software_environments/modules/mathematica/#using-mathematica-with-slurm","title":"Using Mathematica with SLURM","text":"<p>You can use Mathematica either interactively or via batch jobs.</p>"},{"location":"getting_started/software/software_environments/modules/mathematica/#interactive-job-example-srun","title":"Interactive Job Example (srun)","text":"<p>To launch an interactive Mathematica session on a compute node:</p> <p>Terminal</p> <pre><code>srun --pty --ntasks=1 --cpus-per-task=1 --mem=2G --time=00:30:00 bash\nmodule load mathematica\nmath\n</code></pre> <p>This starts an interactive kernel on an allocated node.</p>"},{"location":"getting_started/software/software_environments/modules/mathematica/#batch-job-example-sbatch","title":"Batch Job Example (sbatch)","text":"<p>To run a Mathematica script (<code>script.m</code>) in batch mode:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=math_batch\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --mem=4G\n#SBATCH --time=01:00:00\n#SBATCH --output=math_output.log\n\nmodule load mathematica/14.2.1\n\nmath -script script.m\n</code></pre> <p>Replace <code>script.m</code> with your actual Mathematica file.</p>"},{"location":"getting_started/software/software_environments/modules/mathematica/#using-wolframscript","title":"Using WolframScript","text":"<p>WolframScript allows inline evaluation or execution of <code>.wls</code> scripts:</p> <p>Run code directly:</p> <p>Terminal</p> <pre><code>wolframscript -code 'FactorInteger[123456]'\n</code></pre> <p>Or execute a script:</p> <p>Terminal</p> <pre><code>wolframscript -file myanalysis.wls\n</code></pre> <p>Submitting as a batch job:</p> <p>Terminal</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=wolframscript_job\n#SBATCH --ntasks=1\n#SBATCH --mem=2G\n#SBATCH --time=00:10:00\n\nmodule load mathematica\n\nwolframscript -file myanalysis.wls\n</code></pre> <p>For official Wolfram documentation:</p> <p>https://reference.wolfram.com/language/</p>"},{"location":"getting_started/software/software_environments/modules/modules/","title":"Modules (LMOD)","text":"<p>Environment Modules is a tool that lets users easily switch between different versions of applications by managing the shell environment.</p> <p>Basic usage</p> <ul> <li><code>module avail</code> will list all available modules </li> <li><code>module spider &lt;string&gt;</code> will seach for all packages containing </li> <li> <p><code>module load &lt;name&gt;</code> will load the  module. e.g.: <code>module load apptainer/gubbins</code>.</p> </li> <li> <p>If you don't specify a version, the highest numeric version number is automatically selected. e.g. <code>module load r</code> will load the latest available R version</p> </li> <li> <p>Alternatively you can <code>module load</code> an excplicit version, .e.g. <code>module load r/4.4.3</code></p> </li> <li> <p><code>module list</code> will show which modules/versions are loaded</p> </li> </ul>"},{"location":"getting_started/software/software_environments/self_managed/apptainer/","title":"Apptainer (Singularity)","text":"<p>Apptainer (formerly: Singularity) is a secure, HPC-friendly alternative to Docker. </p> <p>Apptainer has its own container image format, but is generally compatible with Docker images.</p>"},{"location":"getting_started/software/software_environments/self_managed/apptainer/#basic-usage","title":"Basic usage","text":"<p>The Apptainer user guide may be found at https://apptainer.org/docs/user/main/ , which explains, amongst other things, how to pull down Docker images from public repositories (such as Dockerhub) and make them work with Apptainer. Email rtis.support@otago.ac.nz if you need any help with this. </p> <p>On the Research Cluster and other RTIS-managed shared servers, pre-existing shared Apptainer images (.sif) are generally located at <code>$APPTAINER_IMG</code>. </p> <p>For convenience, shared Apptainer images may have been wrapped in a modulefile that will create the necessary aliases to the relevant in-container binaries, so after loading the module these binaries can then be invoked as per usual. e.g.:</p> <p>Terminal</p> <pre><code>module avail foo\nmodule load apptainer/foo/0.1\nfoo_bin -v\n</code></pre> <p>where <code>foo_bin</code> will actually be an alias to <code>apptainer run $APPTAINER_IMG/foo_0.1.sif foo_bin</code>.  Run <code>alias</code> to display a list of all defined aliases in your shell</p> <p>To use these aliases in a non-interactive script or via SLURM, add the following in your script before using the alias:</p> <p>Terminal</p> <pre><code>shopt -s expand_aliases\n</code></pre> <p>Available .sif images can also be run as an executable;  i.e.: to start a container with the default run command: <code>$APPTAINER_IMG/&lt;image.sif&gt;</code> (which is identical to: <code>apptainer run $APPTAINER_IMG/&lt;image.sif&gt;</code>) or alternatively add a custom command to run: <code>$APPTAINER_IMG/&lt;image.sif&gt; &lt;command in the container&gt;</code>  or to start an interactive shell in the container: <code>apptainer shell $APPTAINER_IMG/&lt;image.sif&gt;</code> </p>"},{"location":"getting_started/software/software_environments/self_managed/apptainer/#bind-mounts","title":"Bind mounts","text":"<p>Your <code>$HOME</code> directory, <code>/scratch</code>, and <code>/projects</code> will be available within the Apptainer container by default. Other directories/files on the host filesystem will be inaccessible from within the container, unless explicitely mounted into the container (similar to Docker's volumes). </p> <p>If you need access to other arbitrary filesystem paths, specify these with the <code>--bind</code>/<code>-B</code> option  e.g. To start the <code>foo.sif</code> container with the host directory <code>/some/host_path/test</code> mounted on <code>/tmp/test</code> within the container:</p> <p>Terminal</p> <pre><code>apptainer run --bind /some/host_path/test:/tmp/test $APPTAINER_IMG/foo.sif\n</code></pre> <p>Refer to https://apptainer.org/docs/user/main/bind_paths_and_mounts.html#user-defined-bind-paths for additional information and examples.</p>"},{"location":"getting_started/software/software_environments/self_managed/apptainer/#gui-applications","title":"GUI applications","text":"<p>GUI applications in an Apptainer container can be remotely started from within a graphical environment (e.g. OnDemand, :doc:<code>X2Go &lt;/common/x2go&gt;</code>, :doc:<code>FastX &lt;/common/fastx&gt;</code>, X11-forwarded SSH session).</p>"},{"location":"getting_started/software/software_environments/self_managed/apptainer/#gui-applications-with-3d-acceleration-opengl","title":"GUI applications with 3D acceleration (OpenGL)","text":"<p>This requires </p> <ul> <li>an execution host with GPU(s) set up with NVIDIA drivers</li> <li>a specially crafted Apptainer container image containing the necessary libraries and settings for OpenGL and VirtualGL</li> <li>the Apptainer image ran with the <code>--nv</code> flag</li> <li>a Virtual-GL capable VNC client (such as the OnDemand noVNC browser client on the Research Cluster, or a native client supporting VirtualGL (TurboVNC, TigerVNC))</li> </ul>"},{"location":"getting_started/software/software_environments/self_managed/apptainer/#cuda-gpu-compute-support","title":"CUDA (GPU compute) support","text":"<p>If the node/server has NVIDIA GPU cores available, starting the Apptainer container with the <code>--nv</code> flag will setup the container\u2019s environment to use the NVIDIA GPU and the basic CUDA libraries to run a CUDA enabled application.</p>"},{"location":"getting_started/software/software_environments/self_managed/conda/","title":"Conda/Mamba","text":""},{"location":"getting_started/software/software_environments/self_managed/conda/#conda","title":"Conda","text":"<p>For the most up-to-date and detailed documentation please refer to the official conda documentation at https://docs.conda.io/en/latest/</p> <p>If you are looking to only manage python packages, consider the use of venv which can be simpler.</p> <p>Mamba</p> <p>Mamba is a drop in replacement for conda which has a faster package and dependency resolver. Mamba is available if you self-install using the miniforge instructions below.</p>"},{"location":"getting_started/software/software_environments/self_managed/conda/#loading-conda","title":"Loading Conda","text":"<p>Conda is available through the module system</p> <p>Terminal</p> <pre><code>module load miniconda3\nsource $(conda info --base)/etc/profile.d/conda.sh\n</code></pre> <p>This will load the latest version of conda installed on the system.</p> <p>Note</p> <p>After loading conda, <code>source $(conda info --base)/etc/profile.d/conda.sh</code> is needed to load the conda functions into your environment and needs to be done everytime the miniconda module is loaded. This is done instead of using <code>conda init</code>. The use of <code>conda init</code> is not recommended as it hard codes a specific version of conda into your bashrc which can cause issues if you are not managing the installation of conda yourself.</p> <p>This also mirrors the requirement for when using conda in a slurm script as your bashrc is not parsed as part of a SLURM job.</p>"},{"location":"getting_started/software/software_environments/self_managed/conda/#self-installing","title":"Self installing","text":"<p>If you would prefer to install and manage your own version of conda you can do so, we recommend using Miniforge to manage conda environments and packages. Miniforge is a community-led, minimal conda/mamba installer that uses conda-forge as the default channel.</p> <p>To install Miniforge under your user account, you can use the following commands:</p> <p>Terminal</p> <pre><code>wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\nbash Miniforge3-Linux-x86_64.sh -b -u\n</code></pre> <p>Once installed (the default location is <code>/home/&lt;user&gt;/miniforge3</code>), load conda using:</p> <p>Terminal</p> <pre><code>source ~/miniforge3/etc/profile.d/conda.sh\n</code></pre> <p>Note</p> <p>Installing miniforge and loading as above will also give you access to <code>mamba</code> which is a drop in faster replacement to <code>conda</code>.</p>"},{"location":"getting_started/software/software_environments/self_managed/conda/#extra-configuration","title":"Extra Configuration","text":""},{"location":"getting_started/software/software_environments/self_managed/conda/#cache-location","title":"Cache location","text":"<p>By default conda will download the code for packages into a cache in your home directory. It is a good idea to change this to instead be your project directory (if you have one)</p> <p>Terminal</p> <pre><code>conda config --add pkgs_dirs /path/to/project/conda_pkgs\n</code></pre>"},{"location":"getting_started/software/software_environments/self_managed/conda/#bioconda","title":"Bioconda","text":"<p>Bioconda (https://bioconda.github.io) is a popular repository for bioinformatic software. To be able to make use of the bioconda repository you must configure conda to know about it. The following commands are from https://bioconda.github.io/#usage and will configure conda to search and download from the bioconda repositiory when installing into enivronments.</p> <p>Terminal</p> <pre><code>conda config --add channels bioconda\nconda config --add channels conda-forge\nconda config --set channel_priority strict\n</code></pre> <p>These commands modify your <code>~/.condarc</code> file.</p>"},{"location":"getting_started/software/software_environments/self_managed/conda/#conda-environments","title":"Conda environments","text":"<p>Conda environments let you manage software (and it's dependencies). Ultimately, conda install software into specific directories and then alters your <code>PATH</code> for you to make them accessible. To utilise an environment it must first exist, and then be activated. </p> <p>There are two types of environments: named and prefix. </p> <ul> <li> <p>Named environments are installed within your home directory (subdirectories within <code>~/.conda/envs</code>) and will let you activiate by <code>conda activate &lt;environment_name&gt;</code>.</p> </li> <li> <p>Prefix environments are installed into the location you specify at creation. If this location is accessible by others, they too can use the environment. For reproducibility it is useful to create an environment in a project directory and use that for operating on data there. When you change to a different project, you can activate the corresponding environment. This lets you manage your software at the project level.</p> </li> </ul> <p>Note</p> <p>As named environments are stored in your home directory these can take up a large portion of your home directory storage quota. They are also not accessible to others.</p> <p>The use of prefix environments is encouraged as you can specify the location to store these (ideally in your project directory)</p> <p>Warning</p> <p>We strongly recommend against using <code>conda init</code>. It inserts a snippet in your <code>~/.bashrc</code> file that will freeze the version of conda used, bypassing the environment module system.</p> <p>Creating and activating a sub-environment</p> <p>Although once you have activated the base conda environment, you can in principle start to install packages immediately, your use of conda will generally be better organised if you do not install packages directly into the base environment, but instead use a named sub-environment.  You can have multiple sub-environments under a single base environment, and activate the one that is required at any one time.  Unless you install packages directly into the base environment, your sub-environments will work independently.</p> <p>Managing Conda Environments to Conserve Home Directory Storage</p> <p>To save home directory storage space, it is recommended to create Conda environments in a shared project directory.  This approach allows you to manage your Conda environments within your project directory and if needed share them with collaborators. If you do not yet have a shared project directory, please contact RTIS Solutions to request one.</p> <p>To create a named environment (for example, called \"myenv\"), ensure that the base environment is activated (the command prompt should start with \"(base) \"), and type:</p> <p>Creating Conda Environments</p> PrefixNamed <pre><code>conda create -p /path/to/create/environment/\n</code></pre> <pre><code>conda create -n myenv\n</code></pre> <p>It will show the proposed installation location, and once you answer the prompt to proceed, will do the installation.  If you have followed these instruction, this location should be <code>/home/users/&lt;your_username&gt;/miniconda3/envs/myenv</code>.  You can alternatively give it a different location using the option <code>-p &lt;path&gt;</code> instead of <code>-n &lt;name&gt;</code>.</p> <p>Warning</p> <p>Do not create conda environments in subdirectories of <code>/mnt/auto-hcs/</code> - conda will either fail or have it will have issues.</p> <p>Once you have created your sub-environment, you can activate it using <code>conda activate &lt;name&gt;</code> for example:</p> <p>Terminal</p> PrefixNamed <pre><code>conda activate /path/to/conda/environment\n</code></pre> <pre><code>conda activate myenv\n</code></pre> <p>The command prompt will then change (e.g. to start with \"(myenv) \") to reflect this.  Typing conda deactivate once will return you to the base environment; typing it a second time will deactivate conda completely (as above).  | To List your conda environments type the following:</p> <p>Terminal</p> <pre><code>conda env list\n</code></pre>"},{"location":"getting_started/software/software_environments/self_managed/conda/#installing-conda-packages","title":"Installing conda packages","text":"<p>Once you have activated an environment, you can install packages with the conda install command, for example:</p> <p>Terminal</p> <pre><code>conda install gcc\n</code></pre> <p>You can also force particular versions to be installed.  See the conda cheat sheet for details.</p> <p>To list the packages installed in the currently activated environment, you can type conda list.</p>"},{"location":"getting_started/software/software_environments/self_managed/conda/#cleaning-up","title":"Cleaning up","text":""},{"location":"getting_started/software/software_environments/self_managed/conda/#cleaning-cache","title":"Cleaning Cache","text":"<p>Once you've made your environments it can be a good idea to clean up your cache </p> <p>Terminal</p> <pre><code># remove index cache, lock files, unused cache packages, tarballs, and logfiles\nconda clean --all\n</code></pre>"},{"location":"getting_started/software/software_environments/self_managed/conda/#migrating-an-existing-conda-environment","title":"Migrating an Existing Conda Environment","text":"<p>To move an existing Conda environment to a new location:</p> <ol> <li> <p>Export your current environment to a YAML file:</p> <p>Terminal</p> <pre><code>conda env export --name existing_env &gt; environment.yml\n</code></pre> </li> <li> <p>Create a new environment from the exported YAML file at your chosen location:</p> <p>Terminal</p> <pre><code>conda env create --prefix /path/to/project_directory/env/conda_envs/myenv --file environment.yml\n</code></pre> </li> <li> <p>Activate the newly created environment:</p> <p>Terminal</p> <pre><code>conda activate /path/to/project_directory/env/conda_envs/myenv\n</code></pre> </li> </ol>"},{"location":"getting_started/software/software_environments/self_managed/conda/#creating-an-alias-for-easy-activation","title":"Creating an Alias for Easy Activation","text":"<p>To simplify environment activation, consider adding an alias to your shell configuration file (e.g., <code>.bashrc</code> or <code>.bash_profile</code>):</p> <p>Terminal</p> <pre><code>alias activate_myenv=\"conda activate /path/to/project_directory/env\"\n</code></pre> <p>Activate your environment using the alias:</p> <p>Terminal</p> <pre><code>activate_myenv\n</code></pre> <p>This method is Python-version agnostic and provides a convenient way to manage Conda environments in shared or collaborative project directories.</p>"},{"location":"getting_started/software/software_environments/self_managed/conda/#removing-environments","title":"Removing environments","text":"<p>If you no longer need an environment the easiest way to remove it is:</p> <p>Terminal</p> PrefixNamed <pre><code>conda env remove -p /path/to/env\n</code></pre> <pre><code>conda env remove -n env_name\n</code></pre>"},{"location":"getting_started/software/software_environments/self_managed/conda/#running-packages-from-your-conda-environment","title":"Running packages from your conda environment","text":"<p>In order to run packages from a conda environment that you installed previously, you will first need to activate the environment in the session that you are using.  This means repeating some of the commands typed above.  Of course, you will not need to repeat the steps to create the environment or install the software, but the following may be needed again:</p> <p>Terminal</p> <pre><code>conda deactivate\n\nconda activate myenv\n</code></pre>"},{"location":"getting_started/software/software_environments/self_managed/conda/#installing-pip-packages","title":"Installing pip packages","text":"<p>Many python packages that are available via PyPI are also available as conda packages in conda-forge, and it is generally best to use these via \"conda install\" as above.</p> <p>Nonetheless, you can also install pip packages (as opposed to conda packages) into your conda environment.  However, first you should type:</p> <p>Terminal</p> <pre><code>conda install pip\n</code></pre> <p>before typing the desired commands such as</p> <p>Terminal</p> <pre><code>pip install numpy\n</code></pre> <p>If you do not install pip into your sub-environment, then either:</p> <p>your shell will fail to find the pip executable, or your shell will find pip in your base environment, which will lead to pip packages being installed into the base environment, resulting in potential interference between your conda environments Explicitly installing pip into your sub-environment will guard against this.    </p>"},{"location":"getting_started/software/software_environments/self_managed/conda/#using-conda-with-slurm","title":"Using conda with SLURM","text":"<p>In order to use conda environments within your slurm script you need to source the conda profile script so that the conda paths get set.</p> <p>Terminal</p> ModulesSelf installed miniforge <pre><code>module load miniconda3\nsource $(conda info --base)/etc/profile.d/conda.sh\nexport PYTHONNOUSERSITE=1 # don't add python user site library to path\n\nconda activate /path/to/env/\n</code></pre> <pre><code>source ~/miniforge3/etc/profile.d/conda.sh\nexport PYTHONNOUSERSITE=1 # don't add python user site library to path\n\nconda activate /path/to/env/\n</code></pre>"},{"location":"getting_started/software/software_environments/self_managed/conda/#adding-custom-conda-environments-to-jupyter","title":"Adding custom conda environments to Jupyter","text":"<p>On the commandline, first create a conda environment and install the packages/software you wish into it.  Then add the <code>ipykernel</code> and register it with Juptyer.</p> <p>Terminal</p> <pre><code>conda create --path /path/to/env\n\nconda activate /path/to/env\n\nconda install &lt;packages/software of interest&gt;\n\nconda install ipykernel\n\npython -m ipykernel install --user --name=myCondaEnvironment\n</code></pre> <p>Then in Jupyter the custom environment can be loaded by Kernel -&gt; Change Kernel</p>"},{"location":"getting_started/software/software_environments/self_managed/renv/","title":"Renv (R package environments)","text":"<p>TODO</p>"},{"location":"getting_started/software/software_environments/self_managed/shells/","title":"Shells (bash, zsh, fish, tcsh)","text":"<p>A shell is a command-line interface that interprets user commands and interacts with the operating system, allowing users to execute commands,  navigate directories, and manage files. Common examples of shells include Bash, Zsh, and Fish. Different shells have different features, built-in commands and different syntax for scripting.</p> <p>Bash (GNU Bourne-Again SHell) is the most common default shell in GNU/Linux distributions, and the default shell on the Cluster.</p> <p>For non-interactive scripts (especially scripts for Slurm and workflows that may be shared with other people), we recommend sticking with  the default bash, though some users may prefer to set an alternative shell for their interactive sessions. <code>cat /etc/shells</code> will list the different shells that are available cluster-wide.</p> <p>Due to the way account information is sourced from the central directory, there is no trivial way to change your account's default login shell.  The easiest workaround is to start your preferred shell from the bash login shell.</p>"},{"location":"getting_started/software/software_environments/self_managed/shells/#zsh","title":"zsh","text":"<p>To automatically launch <code>zsh</code> when you start a terminal session, add the following to your <code>~/.bashrc</code>:</p> <p>Terminal</p> <pre><code>export SHELL=/usr/bin/zsh\nexec /usr/bin/zsh\n</code></pre>"},{"location":"getting_started/software/software_environments/self_managed/shells/#fish","title":"fish","text":"<p>To automatically launch :code:<code>fish</code> when you start a terminal session, add the following to your :code:<code>~/.bashrc</code>:</p> <p>Terminal</p> <pre><code>export SHELL=/usr/bin/fish\nexec /usr/bin/fish\n</code></pre>"},{"location":"getting_started/software/software_environments/self_managed/spack/","title":"Spack","text":"<p>Spack is a package manager that simplifies installing and running customised scientific software stacks. With Spack, you can build a package with multiple versions,  configurations, platforms, and compilers, and all of these builds can coexist in parallel.</p> <p>A shared library of common software has been preinstalled on the Research Cluster, available to use via a Shared read-only Spack instance or via environment modules. </p> <p>Users wanting to install their own Spack packages should install and manage their own local Spack instance (See User-local Spack installation).</p>"},{"location":"getting_started/software/software_environments/self_managed/spack/#shared-read-only-spack","title":"Shared read-only Spack","text":"<p>Note</p> <p>All pre-installed Spack packages are also available by default via environment module without needing to use Spack.</p> <p>To initialise use of the read-only shared Spack instance, run:</p> <p>Terminal</p> <pre><code>source /opt/spack/spack/share/spack/setup-env.sh\n</code></pre> <p>(or equivalent <code>setup-env</code> script for non-default shells)</p> <p>That will make the <code>spack</code> function available, and gives you read-only access to the shared library of pre-installed packages.</p> <p>You could add this to your <code>~/.bashrc</code> file (or its equivalent when not using the default bash shell and <code>~/.bash_profile</code>) to automatically source the initialisation script the next time you open a terminal session:</p> <p>Terminal</p> <pre><code>echo 'source /opt/spack/spack/share/spack/setup-env.sh' &gt;&gt; ~/.bashrc\n</code></pre>"},{"location":"getting_started/software/software_environments/self_managed/spack/#user-local-spack-installation","title":"User-local Spack installation","text":"<p>In order to install Spack packages youself, you need to set up your own Spack instance.</p> <ul> <li>Open a terminal and with your $HOME as the current working directory, follow the Installation instructions on the Spack website and source the appropriate initialisation script:</li> </ul> <p>Termial</p> <pre><code>cd &amp;&amp; git clone -c feature.manyFiles=true --depth=2 https://github.com/spack/spack.git\nsource ~/spack/share/spack/setup-env.sh\n</code></pre> <ul> <li>The sourcing of <code>setup-env</code> may be added to your <code>~/.bashrc</code> file (or its equivalent when not using the default bash shell      and <code>~/.bash_profile</code>) to automatically source the initialisation script the next time you open a terminal session:</li> </ul> <p>Terminal</p> <pre><code>echo 'source ~/spack/share/spack/setup-env.sh' &gt;&gt; ~/.bashrc\n</code></pre> <ul> <li>We highly recommend chaining your user-local Spack to our shared Spack, so you can make use of and build upon the shared packages already available.      In order to do so, you can run the following to create the upstreams configuration file <code>~/.spack/upstreams.yaml</code>:</li> </ul> <p>Terminal</p> <pre><code>mkdir -p ~/.spack &amp;&amp; cat &lt;&lt;EOF &gt; ~/.spack/upstreams.yaml\nupstreams:\n  system-spack:\n    install_tree: /opt/spack/spack/opt/spack\nEOF\n</code></pre> <ul> <li> <p>Following the above instructions, Spack packages will be installed under <code>~/spack/</code> in your home directory, which can get  sizable and will count towards your home directory storage quota. If you have a <code>/projects</code> folder set up, Spack can alternatively  be installed there or set up as an alternative prefix.</p> </li> <li> <p>If you installed Spack via the recommended <code>git clone</code>, you can keep your local Spack instance up to date by running :code:<code>cd $SPACK_ROOT &amp;&amp; git pull</code></p> </li> </ul>"},{"location":"getting_started/software/software_environments/self_managed/spack/#usage-on-the-research-cluster","title":"Usage on the Research Cluster","text":"<p>Please refer to the Basic usage documentation on the Spack website for a comprehensive overview and  examples of how to query, install, load and use packages and Spack environments.</p>"},{"location":"getting_started/software/software_environments/self_managed/venv/","title":"Python virtual environments","text":""},{"location":"getting_started/software/software_environments/self_managed/venv/#venv","title":"Venv","text":"<p>The purpose of venv is to create isolated Python environments for individual projects. This allows each project to have its own dependencies and package versions without interfering with other projects or the system-wide Python installation. It ensures cleaner organization, avoids version conflicts, and enables easy reproduction of the environment using tools like requirements.txt. This is especially useful when collaborating with others or deploying code across different systems.</p> <ol> <li> <p>Create a Project Directory</p> <p>Terminal</p> <pre><code>mkdir my_project\ncd my_project\n</code></pre> </li> <li> <p>Create a Virtual Environment</p> <p>Terminal</p> <pre><code>python -m venv venv\n</code></pre> <p>This creates a venv/ folder containing the isolated Python environment.</p> <p>You can name it anything (e.g., .venv is common too).</p> </li> <li> <p>Activate the Virtual Environment</p> <p>Terminal</p> <pre><code>source venv/bin/activate\n</code></pre> </li> <li> <p>Install Dependencies</p> <p>Use pip to install libraries:</p> <p>Terminal</p> <pre><code>pip install requests flask\n</code></pre> </li> <li> <p>Freeze Dependencies     Save your dependencies:</p> <p>Terminal</p> <pre><code>pip freeze &gt; requirements.txt\n</code></pre> </li> <li> <p>Create a .gitignore (if using git)</p> <p>Prevent <code>venv/</code> from being tracked:</p> <p>Terminal</p> <pre><code>echo venv/ __pycache__/ *.pyc &gt;&gt; .gitignore\n</code></pre> </li> <li> <p>Run Your Python App     You can now run your scripts:</p> <p>Terminal</p> <pre><code>python main.py\n</code></pre> </li> <li> <p>Reproducing the Environment (for others)     To recreate the environment:</p> <p>Terminal</p> <pre><code>python -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n</code></pre> </li> </ol>"},{"location":"getting_started/software/software_environments/self_managed/venv/#comparison-of-venv-vs-conda","title":"Comparison of venv vs conda","text":"Feature <code>venv</code> (Standard Python) <code>conda</code> (Anaconda/Miniconda) Purpose Create isolated Python environments Manage environments and packages (Python + non-Python) Included With Standard in Python 3.3+ Requires installing Anaconda or Miniconda Language Support Python only Supports multiple languages (Python, R, Julia, etc.) Package Manager Uses <code>pip</code> Uses <code>conda</code> (can also use <code>pip</code> inside conda) Speed of Package Resolution Fast for pure Python; slower for complex dependencies Faster for scientific packages due to prebuilt binaries System Packages Installs from PyPI Installs from conda channels (e.g., conda-forge) Non-Python Dependencies Manual (e.g., via apt/brew) Built-in (e.g., OpenCV, HDF5, BLAS) Binary/Compiled Packages Not handled natively; relies on wheels from PyPI Conda packages are often precompiled Environment Reproducibility Via <code>requirements.txt</code> Via <code>environment.yml</code> Cross-Platform Consistency Less consistent (due to pip building from source) More consistent across OSes Footprint Lightweight (just Python + pip) Heavier (especially full Anaconda distribution)"},{"location":"getting_started/workflows/nextflow/","title":"Nextflow","text":"<p>Nextflow is a workflow framework that eases the writing and sharing of data-intensive computational pipelines.</p>"},{"location":"getting_started/workflows/nextflow/#usage-on-the-research-cluster","title":"Usage on the Research Cluster","text":"<p>Nextflow and its dependencies can be made available with <code>module load nextflow</code>.</p> <p>There are a few important steps to take to make sure the pipeline can run on the cluster and integrate with the scheduler;</p> <ul> <li>Create a global Nextflow configuration file at <code>$HOME/.nextflow/config</code> that tells nextflow to use the 'slurm' scheduler and which SLURM partition to use for 'gpu'-labeled tasks. By default Nextflow will merge this global config file with any other local nextflow.config files and parameters.  (See https://www.nextflow.io/docs/latest/executor.html#slurm for additional SLURM options.)</li> </ul> <pre><code>    process {\n      executor = 'slurm'\n      time = 6.h\n      withLabel: 'gpu' {\n        queue = 'aoraki_gpu'\n      }\n    }\n</code></pre> <p>In your workflow's working directory, running <code>nextflow config -profile singularity</code> will show what the merged configuration looks like.</p> <ul> <li>Make sure to set the 'profile' parameter to <code>singularity</code>, or in some cases <code>apptainer</code> when running a workflow. Workflows most likely have the standard profile set to use Docker, which is not available in HPC environments.</li> </ul> <p>Terminal</p> <pre><code>nextflow run &lt;workflow&gt; &lt;workflow parameters&gt; -profile singularity\n</code></pre>"},{"location":"getting_started/workflows/nextflow/#nf-core","title":"nf-core","text":"<p>https://nf-co.re/ is a curated set of over 100 Nextflow analysis pipelines.</p>"},{"location":"getting_started/workflows/nextflow/#epi2me","title":"EPI2ME","text":"<p>EPI2ME is Oxford Nanopore's data analysis platform providing a user-friendly graphical interface to running ONT's bioinformatics Nextflow pipelines, as well as other generic Nextflow pipelines.</p> <p>More info here.</p>"},{"location":"getting_started/workflows/project_setup/bioinformatics_software/","title":"Bioinformatics software","text":""},{"location":"getting_started/workflows/project_setup/bioinformatics_software/#general-bioinformatics-tools","title":"General Bioinformatics Tools","text":"<p>TODO convert to markdown</p> <p>The following categories of bioinformatics tools are available through conda:</p> <ul> <li>Read aligners (e.g., bwa, bowtie2)</li> <li>Variant callers (e.g., freebayes, gatk, bcftools) </li> <li>File format tools (e.g., samtools, vcftools)</li> <li>GWAS tools (e.g., plink, gemma)</li> <li>Visualization (e.g., igv, multiqc)</li> <li>RNA-seq / transcriptomics (e.g., kallisto, salmon)</li> <li>Assemblers (e.g., spades, megahit)</li> </ul> <p>Finding Bioinformatics Tools ^^^^^^^^^^^^^^^^^^^^^^^^^^^^</p> <p>There are several ways to find bioinformatics tools in conda:</p> <ol> <li> <p>Search online (recommended for discovery)</p> <ul> <li>Use the Anaconda package search or browse specific channels:</li> <li>Bioconda: https://anaconda.org/bioconda</li> <li>Conda-Forge: https://anaconda.org/conda-forge</li> <li>You can search for tools like:</li> <li>plink</li> <li>bcftools</li> <li>samtools</li> </ul> </li> <li> <p>Command-line search      From your terminal:</p> <p>.. code-block:: bash</p> <pre><code> # Search all channels (if configured)\n conda search &lt;package-name&gt;\n\n # Example:\n conda search plink\n\n # If using Mamba (faster alternative to conda)\n mamba search plink\n</code></pre> <p>To restrict search to a specific channel:</p> <p>.. code-block:: bash</p> <pre><code> conda search -c bioconda plink\n</code></pre> </li> <li> <p>Get full list (advanced)      You can list everything in a channel, but it's very large:</p> <p>.. code-block:: bash</p> <pre><code> # List all bioconda packages\n conda search --channel bioconda \"*\" | less\n</code></pre> <p>Tip: pipe it through grep to find specific tools:</p> <p>.. code-block:: bash</p> <pre><code> conda search -c bioconda \"*\" | grep vcftools\n</code></pre> </li> </ol>"},{"location":"getting_started/workflows/project_setup/project_setup/","title":"Project setup","text":"<p>TODO flesh out page</p>"},{"location":"getting_started/workflows/project_setup/project_setup/#project-structure","title":"Project structure","text":""},{"location":"getting_started/workflows/project_setup/project_setup/#data","title":"Data","text":"<p>TODO Where will your data live? </p> <p>For how to move data into this structure see globus</p>"},{"location":"getting_started/workflows/project_setup/project_setup/#software","title":"Software","text":"<p>TODO</p>"},{"location":"getting_started/workflows/project_setup/rstudio/","title":"Rstudio","text":"<p>TODO</p>"},{"location":"getting_started/workflows/project_setup/rstudio/#installing-libraries","title":"Installing Libraries","text":""},{"location":"getting_started/workflows/project_setup/rstudio/#renv","title":"renv","text":""},{"location":"storage/file_permissions/","title":"File permissions","text":"<p>TODO</p> <p>The storage on Aoraki uses 2 systems of file permissioning. </p> Location Unix permissions ACL Permissions /home/&lt;username&gt; /projects /weka"},{"location":"storage/file_permissions/#unix-permissions","title":"Unix Permissions","text":"<p>Unix file permissions control who can read, write, or execute a file or directory. They apply to three categories:</p> <ul> <li>Owner \u2013 the user who owns the file</li> <li>Group \u2013 users in the file's group</li> <li>Others \u2013 all other users</li> </ul> <p>\ud83d\udcdc Permission Types</p> Symbol Meaning r Read w Write x Execute - No permission <p>For example, given <code>-rwxr-xr--</code> the permissions would be:</p> <ul> <li>Owner <code>rwx</code>   Read, write, execute</li> <li>Group <code>r-x</code>   Read, execute only</li> <li>Others    <code>r--</code>   Read only</li> </ul> <p>To view or change the Unix permissions on a file</p> <p>Terminal</p> <pre><code># View permissions\nls -l filename\n</code></pre>"},{"location":"storage/file_permissions/#modifying-permissions-with-chmod","title":"Modifying permissions with <code>chmod</code>","text":"<p>TODO</p>"},{"location":"storage/file_permissions/#access-control-lists-acl","title":"Access Control Lists (ACL)","text":"<p>ACLs extend the standard Unix file permission model (owner/group/others) by allowing fine-grained access control for additional users and groups on a per-file or per-directory basis.</p> <p>Where the standard permissions (chmod) set access for:</p> <ul> <li>Owner</li> <li>Group</li> <li>Others</li> </ul> <p>ACLs allow for:</p> <ul> <li>Specific users (e.g., user:bob)</li> <li>Specific groups (e.g., group:research)</li> <li>Default rules for directories (e.g., default:user:bob)</li> </ul> <p>NFSv4 ACL Breakdown Each line has the form:</p> <p>bash Copy Edit A:(type):[who]:permissions Where:</p> <p>A = allow (you might also see D = deny)</p> <p>OWNER@, GROUP@, EVERYONE@ = NFSv4 built-in identities</p> <p>u:username@domain = specific user</p> <p>g:group@domain = specific group</p> Code Name What it allows r Read Data Read file contents or list directory contents w Write Data Modify file contents or create files in a directory a Append Data Append to a file or create subdirectories in a directory D Delete Child Delete files within a directory x Execute Execute file or traverse directory t Read Attributes View basic file metadata (size, timestamps) T Write Attributes Modify basic file metadata (e.g. change timestamps) n Read Named Attributes Access extended attributes N Write Named Attributes Modify extended attributes c Read ACL View the ACL of the file C Write ACL Modify the ACL of the file y Synchronize Ensure file changes are written to stable storage (fsync) <p>Example</p> <p>Terminal</p> <pre><code>nfs4_getfacl /projects/\n</code></pre> <pre><code># file: /projects/\nA::OWNER@:rwaDxtTnNcCy\nA::GROUP@:rxtncy\nA::EVERYONE@:rxtncy\n</code></pre> <p>NFSv4 ACL Entries Explained</p> Entry Who it applies to Permissions Meaning A::OWNER@:rwaDxtTnNcCy File owner rwaDxtTnNcCy Full access A::GROUP@:rxtncy File group rxtncy Read + Execute + Metadata access A::EVERYONE@:rxtncy Everyone else rxtncy Read + Execute + Metadata access"},{"location":"storage/storage_options/","title":"Storage Overview","text":"<p>RTIS provides high-performance storage solutions for researchers at the University of Otago. These storage solutions are available on the Research Cluster to all researchers at the University of Otago upon application approval.</p> <pre><code>graph TD;\n    root(\"/\")\n    root --&gt; home\n    home --&gt; username\n\n    root --&gt; projects\n    projects --&gt; div1[div]\n    div1 --&gt; school1[school]\n    school1 --&gt; dept1[dept]\n    dept1 --&gt; group1[group]\n\n    root --&gt; weka\n    weka --&gt; div2[div]\n    div2 --&gt; school2[school]\n    school2 --&gt; dept2[dept]\n    dept2 --&gt; group2[group]\n\n    root --&gt; mnt\n    mnt --&gt; auto-hcs\n    auto-hcs --&gt; share</code></pre> Home directory RTIS (Ohau) Storage WEKA High capacity storage Ideal Use Storage of scripts and configuration files Research data you are working on Workflows that require very high speed data reading/writing Long term storage of important research data Mount point <code>/home/&lt;username&gt;</code> <code>/projects/&lt;division&gt;/&lt;school&gt;/&lt;dept&gt;/&lt;group&gt;/</code> <code>/weka/&lt;division&gt;/&lt;school&gt;/&lt;dept&gt;/&lt;group&gt;/</code> <code>/mnt/auto-hcs/&lt;share name&gt;</code> Backed up Default quota 40 GB Set by request on group creation 0 GB (needs to be requested) (Managed by ITS)"},{"location":"storage/storage_options/#research-storage-ohau-storage-pool","title":"Research Storage (Ohau storage pool)","text":"<p>Warning</p> <p>Note that the Research storage is not backed up and it is the responsibility of the user to ensure their important data is safe. See data transfer for options to move data you want to retain. If you need assistance with backing up your data, please email rtis.support@otago.ac.nz.</p>"},{"location":"storage/storage_options/#home-directory","title":"Home directory","text":"<p>All users of the Otago Research Cluster have a home directory that is mounted  at <code>/home/&lt;username&gt;</code>. The home storage is intended for storing configuration files, scripts, and other smaller datasets that are used for computations.</p> <p>The hard quota for home directories is 40GB. When you reach this limit you will not be able to write anymore data to your home directory. A warning will be sent when the you have 30GB of data stored in your home directory.</p>"},{"location":"storage/storage_options/#projects-directory","title":"Projects directory","text":"<p>Projects storage is organised per department and group <code>/projects/&lt;division&gt;/&lt;department&gt;/&lt;Research_Group&gt;</code>.  The projects storage is high-performance and is ideal for temporarily storing data that is in use for individuals and sharing within groups using the research infrastructure.  Note that this storage is not backed up and is the responsibility of the user to ensure their important data is backed up. We recommend having a copy of your data on HCS, and transferring a copy to <code>/projects/</code> for working on, then removing this working copy once finished and transferring results back to HCS. </p> <p>To apply for a projects directory, please fill out the storage-signup-form form.</p>"},{"location":"storage/storage_options/#when-to-use-projects-storage","title":"When to use /projects/ storage","text":"<ul> <li>Intermediate Data Storage    During complex computations, intermediate data or temporary results are often generated.</li> <li>Checkpointing    In long-running computations, checkpointing is used to save the state of a job at regular intervals.</li> <li>Data Staging    Before running a job, input data can be staged (preloaded) into <code>/projects/</code> from HCS (or other sources) to ensure that the computation starts immediately without waiting for data transfers from slower storage systems.</li> <li>Temporary Data Processing    For tasks that generate large amounts of temporary data, such as sorting, indexing, or image processing.</li> </ul> <p>Warning</p> <p>Note that the Research storage is not backed up and it is the responsibility of the user to ensure their important data is safe. See data transfer for options to move data you want to retain. If you need assistance with backing up your data, please email rtis.support@otago.ac.nz.</p>"},{"location":"storage/storage_options/#backing-up-your-data","title":"Backing up your data","text":"<p>The Research Storage (anything within /home, /projects, /weka) is not backed up. It is the responsibility of the user to ensure their data is safe. RTIS recommends that users back up their data to the HCS. </p> <p>Setting up Globus to automatically transfer data between the two storage solutions is a great way to ensure your data is backed up.</p> <p>If you need assistance with backing up your data, please email rtis.support@otago.ac.nz.</p>"},{"location":"storage/data_locations/hcs/","title":"Hcs","text":""},{"location":"storage/data_locations/hcs/#otago-hcs-high-capacity-storage","title":"Otago HCS (High Capacity Storage)","text":"<p>HCS is the main data storage pool on the Otago campus. HCS is able to be mounted on desktop and lab computers. It is possible to mount HCS shares on the Cluster Login node to transfer data across but the connection is sub-optimal for high-speed computing and cannot (easily) be mounted across nodes in the cluster. We recommend that you use the HCS for your primary storage needs and transfer your data to your working area in <code>/projects/</code> when you want to work on it, and transfer results back to HCS for long term storage. </p> <p>Note</p> <p>If you do not have an HCS share available for your department/group, please fill out the HCS Access Form .</p>"},{"location":"storage/data_locations/hcs/#mounting-hcs-on-the-login-node","title":"Mounting HCS on the Login node","text":"<p>For small one job data transfers, you can mount your HCS share on the login node. This is not recommended for large data transfers or for data that will be used in a job as the connection is not optimal for high-speed computing.  It can also be mounted on an HPC desktop for smaller data sets and transfers.</p> <p>To mount your HCS share on the login node, you will need to have a valid Kerberos ticket. If you do not have a valid Kerberos ticket, you can generate one by running the <code>kinit</code> command and entering your University password.</p> <p>Otago HCS (High Capacity Storage) shares can be accessed on the cluster provided you have security access and obtain a Kerberos ticket (password authentication).     To access your HCS share:     1. Check if you have a kerberos ticket <code>klist</code>. If not, obtain Kerberos ticket <code>kinit</code> and entering your University password.     3. Navigate to your HCS share <code>/mnt/auto-hcs/&lt;your_share_name&gt;</code> (the last portion of the HCS address)         eg. if my share is <code>\\\\storage.hcs-p01.otago.ac.nz\\its-rtis</code> then navigate to <code>/mnt/auto-hcs/its-rtis</code></p> <p>Occasionally, it may take up to a minute for this access to become available after running <code>kinit</code>.</p> <p>Otago MDS storage (Windows Managed Desktop Share) can be access on the cluster if you have a Kerberos ticket.     To access your MDS share:     1. Check if you have a Kerberos ticket <code>klist</code>. If not, obtain Kerberos ticket <code>kinit</code> and entering your University password.     2. Navigate to your HCS share <code>/mnt/auto-mds/&lt;your_first_inital_of_your_username&gt;/&lt;your_username&gt;</code> (the last portion of the HCS address)         eg. if my MDS share is <code>\\\\registry.otago.ac.nz\\mdr\\Profiles-V2\\h\\higje06p</code> then navigate to <code>/mnt/auto-mds/h/higje06p</code></p>"},{"location":"storage/data_locations/hcs/#mounting-hcs-on-the-cluster-nodes-auks","title":"Mounting HCS on the cluster nodes - Auks","text":"<p>The Auks Slurm plugin enables users to save their Kerberos ticket from the login node onto the Auks server. This saved ticket can then be used on any Slurm compute node to access HCS shares. The Kerberos ticket is automatically renewed by Auks and remains valid for up to 7 days. After this period, the ticket must be manually renewed. Once a user saves their Kerberos ticket, the renewal process happens automatically.</p> <p>Whenever a user logs in to the Aoraki login node using their password, a new krbtgt (Kerberos Ticket-Granting Ticket) is issued. However, if the user logs in via SSH keys or accesses nodes through the OnDemand shell, they must manually obtain a new valid ticket by running the <code>kinit</code> command. When you type your password after running the <code>kinit</code> command, the terminal intentionally suppresses any visible feedback. Confirm your password by pressing Enter key. </p> STAFFSTUDENTS <p>code</p> <pre><code>kinit userx01p@REGISTRY.OTAGO.AC.NZ\nPassword for userx01p@REGISTRY.OTAGO.AC.NZ:\n</code></pre> <p>code</p> <pre><code>kinit studx012@STUDENT.OTAGO.AC.NZ\nPassword for studx012@STUDENT.OTAGO.AC.NZ:\n</code></pre> <p>A krbtgt (Kerberos Ticket-Granting Ticket) is valid for 10 hours, as indicated by the \"Expires\" date and time. During this period, the ticket can be renewed using the <code>kinit -R</code> command. This renewal extends the \"Expires\" time but does not change the \"Renew Until\" date and time. The Auks system handles this renewal process automatically for the user.</p> <p>As long as a user has a valid krbtgt ticket, they can communicate with the Auks server seamlessly.</p> <p>Check if you have a valid krbtgt ticket:  </p> <p>Terminal</p> <pre><code>[studx012@aoraki-login ~]$ klist\n</code></pre> <pre><code>Ticket cache: KCM:40005987:63840\nDefault principal: studx012@STUDENT.OTAGO.AC.NZ\n\nValid starting Expires Service principal\n11/26/2024 08:56:14 11/26/2024 18:56:14 krbtgt/STUDENT.OTAGO.AC.NZ@STUDENT.OTAGO.AC.NZ\n    renew until 12/03/2024 08:56:14\n</code></pre> <pre><code>[studx012@aoraki-login ~]$ kinit -R\n[studx012@aoraki-login ~]$ klist\n</code></pre> <pre><code>Ticket cache: KCM:40005987:63840\nDefault principal: studx012@STUDENT.OTAGO.AC.NZ\n\nValid starting Expires Service principal\n11/26/2024 11:22:40 11/26/2024 21:22:40 krbtgt/STUDENT.OTAGO.AC.NZ@STUDENT.OTAGO.AC.NZ\n    renew until 12/03/2024 08:56:14\n</code></pre> <p>Ping server to verify connection:</p> <p>Terminal</p> <pre><code>[studx012@aoraki-login ~]$ auks -p\n</code></pre> <pre><code>Auks API request succeed\n</code></pre> <p>Save current krbtgt on the ausk server:</p> <p>Terminal</p> <pre><code>[studx012@aoraki-login ~]$ auks -a\n</code></pre> <pre><code>Auks API request succeed\n</code></pre> <p>Now you can use <code>--auks=yes</code> option with srun or <code>#SBATCH --auks=yes</code> in you slurm batch script:</p> <p>Terminal</p> <pre><code>srun --auks=yes --partition=aoraki ls -lah /mnt/auto-hcs/hcs_share_name\n</code></pre> <p>HCS shares are automatically mounted on each node when requested. The user's krbtgt, retrieved from the Auks server, is used to generate Kerberos tickets for communication with the HCS storage servers. However, obtaining these tickets can take some time. If the HCS share is not yet mounted on the node, the job or script may fail.</p> <p>To mitigate this, it is recommended to include a command such as sleep 20 in batch scripts before accessing the automounted share. </p>"},{"location":"storage/data_locations/hcs/#generating-auto-renewal-kerberos-tickets","title":"Generating Auto Renewal Kerberos Tickets","text":"<p>Info</p> <p>If you want to keep using your data over hours or days you will need to renew your Kerberos ticket.  Kerberos tickets are only valid for 1 hour and then need to be renewed either by generating one via the command line or  automatically by generating a keytab file and setting a cronjob to renew the ticket at a specified interval.</p> <p>To see if you have a valid Kerberos ticket you can type <code>klist</code> </p> <ol> <li>You do not have a valid ticket if you get a message like: klist: Credentials cache 'KCM:51776880' not found</li> <li>You do have a ticket if you get a message similar to:</li> </ol> STAFFSTUDENTS <p>Terminal</p> <pre><code>klist\n</code></pre> <pre><code>Ticket cache: KCM:51776880 Default principal: higje06p@REGISTRY.OTAGO.AC.NZ\n\nValid starting     Expires            Service principal\n27/09/23 15:01:57  28/09/23 01:01:57  krbtgt/REGISTRY.OTAGO.AC.NZ@REGISTRY.OTAGO.AC.NZ\n    renew until 04/10/23 15:01:57\n</code></pre> <p>To generate an auto renewing ticket (so as to not expire):</p> <ul> <li>Create a keytab file - replace username with your otago username:*</li> </ul> <p>Terminal</p> <pre><code>ktutil\n\nktutil: addent -password -p &lt;username&gt;@REGISTRY.OTAGO.AC.NZ -k 1 -e aes256-cts (enter your password)\nKtutil: wkt /home/&lt;username&gt;/&lt;username&gt;.keytab\nKtutil: quit\n</code></pre> <ul> <li>To test: <code>kinit  *username*@REGISTRY.OTAGO.AC.NZ -k -t 'username'.keytab</code></li> <li><code>klist</code> (to check if ticket is valid)</li> <li>Then create a cronjob to renew your ticket <code>crontab -e</code> then enter the following line to renew your ticket every hour: <code>0 * * * * kinit  *username*@REGISTRY.OTAGO.AC.NZ -k -t /home/*username*/*username*.keytab</code></li> <li>Ensure the permissions on your .keytab file are <code>0600</code> to keep it secure!</li> </ul> <p>Terminal</p> <pre><code>klist\n</code></pre> <pre><code>Ticket cache: KCM:51776880 Default principal: higje06p@STUDENT.OTAGO.AC.NZ\n\nValid starting     Expires            Service principal\n27/09/23 15:01:57  28/09/23 01:01:57  krbtgt/STUDENT.OTAGO.AC.NZ@STUDENT.OTAGO.AC.NZ\n    renew until 04/10/23 15:01:57\n</code></pre> <p>To generate an auto renewing ticket (so as to not expire):</p> <ul> <li>Create a keytab file - replace username with your otago username:*</li> </ul> <p>Terminal</p> <pre><code>ktutil\n\nktutil: addent -password -p &lt;username&gt;@STUDENT.OTAGO.AC.NZ -k 1 -e aes256-cts (enter your password)\nKtutil: wkt /home/&lt;username&gt;/&lt;username&gt;.keytab\nKtutil: quit\n</code></pre> <ul> <li>To test: <code>kinit  &lt;username&gt;@STUDENT.OTAGO.AC.NZ -k -t 'username'.keytab</code></li> <li><code>klist</code> (to check if ticket is valid)</li> <li>Then create a cronjob to renew your ticket <code>crontab -e</code> then enter the following line to renew your ticket every hour: <code>0 * * * * kinit  &lt;username&gt;@STUDENT.OTAGO.AC.NZ -k -t /home/&lt;username&gt;/&lt;username&gt;.keytab</code></li> <li>Ensure the permissions on your .keytab file are <code>0600</code> to keep it secure!</li> </ul>"},{"location":"storage/data_locations/homes/","title":"Homes","text":"<p>TODO</p>"},{"location":"storage/data_locations/projects/","title":"Projects","text":"<p>TODO</p>"},{"location":"storage/data_locations/weka/","title":"Weka","text":""},{"location":"storage/data_locations/weka/#faster-scratch-storage-weka-storage-pool","title":"Faster Scratch Storage (WEKA Storage Pool)","text":"<p>The WEKA Storage cluster is mounted on the Aoraki compute cluster at <code>/weka/&lt;Research Group&gt;</code> or <code>/weka/&lt;user&gt;</code>. The <code>WEKA platform &lt;https://www.weka.io/&gt;</code>_ is a high-performance storage solution ideal for temporary data used in high-performance computing jobs.</p>"},{"location":"storage/data_locations/weka/#when-to-use-weka-storage","title":"When to use /weka/ storage","text":"<ul> <li>High I/O Performance    Fast scratch storage typically offers higher input/output (I/O) performance compared to regular storage solutions. This can significantly speed up tasks that involve large amounts of data read/write operations, such as simulations, data analysis, and scientific computations.</li> </ul> <p>The WEKA cluster, with its all-flash array and 100Gb networking, is ideal for high-speed data handling and ensures optimal performance for your high-performance computing tasks.</p>"}]}